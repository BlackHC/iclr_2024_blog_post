---
layout: distill
title: "Bridging the Data Processing Inequality and Function-Space Variational Inference"
description: >-
  This blog post explores the interplay between the <i>Data Processing Inequality (DPI)</i> and <i>Function-Space Variational Inference (FSVI)</i> within Bayesian deep learning and information theory. After examining the DPI, a cornerstone concept in information theory, and its pivotal role in governing the transformation and flow of information through stochastic processes, we employ its unique connection to FSVI to highlight the FSVI's focus on Bayesian predictive posteriors over parameter space. Throughout the post, theoretical concepts are intertwined with intuitive explanations and mathematical rigor, offering a holistic understanding of these complex topics. The post culminates by synthesizing insights into the significance of predictive priors in model training and regularization, shedding light on their practical implications in areas like continual learning and knowledge distillation. This comprehensive examination not only enriches theoretical understanding but also highlights practical applications in machine learning, making it a valuable read for researchers and practitioners.
date: 2024-05-07
future: true
htmlwidgets: true

# Anonymize when submitting
authors:
  - name: Anonymous

# authors:
#   - name: Albert Einstein
#     url: "https://en.wikipedia.org/wiki/Albert_Einstein"
#     affiliations:
#       name: IAS, Princeton
#   - name: Boris Podolsky
#     url: "https://en.wikipedia.org/wiki/Boris_Podolsky"
#     affiliations:
#       name: IAS, Princeton
#   - name: Nathan Rosen
#     url: "https://en.wikipedia.org/wiki/Nathan_Rosen"
#     affiliations:
#       name: IAS, Princeton

# must be the exact same name as your blogpost
bibliography: 2024-05-07-dpi-fsvi.bib  

# Add a table of contents to your post.
#   - make sure that TOC names match the actual section names
#     for hyperlinks within the post to work correctly. 
#   - please use this format rather than manually creating a markdown table of contents.
toc:
  - name: Equations
  - name: Images and Figures
    subsections:
    - name: Interactive Figures
  - name: Citations
  - name: Footnotes
  - name: Code Blocks
  - name: Diagrams
  - name: Tweets
  - name: Layouts
  - name: Other Typography?

# Below is an example of injecting additional post-specific styles.
# This is used in the 'Layouts' section of this post.
# If you use this post as a template, delete this _styles block.
_styles: >
  .fake-img {
    background: #bbb;
    border: 1px solid rgba(0, 0, 0, 0.1);
    box-shadow: 0 0px 4px rgba(0, 0, 0, 0.1);
    margin-bottom: 12px;
  }
  .fake-img p {
    font-family: monospace;
    color: white;
    text-align: left;
    margin: 12px 0;
    text-align: center;
    font-size: 16px;
  }
  .box-note, .box-warning, .box-error, .box-important {
    padding: 15px 15px 15px 10px;
    margin: 20px 20px 20px 5px;
    border: 1px solid #eee;
    border-left-width: 5px;
    border-radius: 5px 3px 3px 5px;
  }
  d-article .box-note {
    background-color: #eee;
    border-left-color: #2980b9;
  }
  d-article .box-warning {
    background-color: #fdf5d4;
    border-left-color: #f1c40f;
  }
  d-article .box-error {
    background-color: #f4dddb;
    border-left-color: #c0392b;
  }
  d-article .box-important {
    background-color: #d4f4dd;
    border-left-color: #2bc039;
  }
  d-article article p {
    text-align: justify;
    text-justify: inter-word;
    -ms-hyphens: auto;
    -moz-hyphens: auto;
    -webkit-hyphens: auto;
    hyphens: auto;
  }
  d-article aside {
    border: 1px solid #aaa;
    border-radius: 4px;
    padding: .5em .5em 0;
    font-size: 90%;
  }
  d-article aside p:first-child {
      margin-top: 0;
  }
  d-article details {
    border: 1px solid #aaa;
    border-radius: 4px;
    padding: .5em .5em 0;
  }
  d-article summary {
    font-weight: bold;
    margin: -.5em -.5em 0;
    padding: .5em;
    display: list-item;
  }
  d-article details[open] {
    padding: .5em;
  }
  d-article details[open] summary {
    border-bottom: 1px solid #aaa;
    margin-bottom: .5em;
  }
categories:
- Data Processing Inequality
- Information Theory
- Data Processing Inequality
- Information Theory
- Function-Space Variational Inference
- Parameter Equivalence Classes
- Entropy Regularization
- Label Entropy Regularization
---

In information theory, the **data processing inequality (DPI)** is a powerful concept. Informally, it tells us that processing data cannot increase the amount of contained information. In this *two-part* blog post, we will explore the DPI and its applications to **function-space variational inference (FSVI)**.

<aside class="l-body box-warning" markdown="1">
The data processing inequality examines how information cannot increase due to processing. In information theory, it is usually stated based on a Markov chain of random variables $$X \rightarrow Y \rightarrow Z$$ and their mutual information. We will also look at different data processing inequalities that relate different distributions instead of different random variables.
</aside>

In the first part of this blog post, we will provide intuitive explanations and present mathematical proofs of the DPI. Then, in the second part, we will explore the application of the data processing inequality to function-space variational inference and its relationship to variational inference in general.

The goal of this post is to look at the data processing inequality from different angles to better understand it. We will also consider the equality case (which is arguably the best way to understand inequalities).

{% raw %}
<div style="display: none;">
$$\require{mathtools}
\DeclareMathOperator{\opExpectation}{\mathbb{E}}
\newcommand{\E}[2]{\opExpectation_{#1} \left [ #2 \right ]}
\newcommand{\simpleE}[1]{\opExpectation_{#1}}
\newcommand\MidSymbol[1][]{%
\:#1\:}
\newcommand{\given}{\MidSymbol[\vert]}
\DeclareMathOperator{\opmus}{\mu^*}
\newcommand{\IMof}[1]{\opmus[#1]}
\DeclareMathOperator{\opInformationContent}{H}
\newcommand{\ICof}[1]{\opInformationContent[#1]}
\newcommand{\xICof}[1]{\opInformationContent(#1)}
\DeclareMathOperator{\opEntropy}{H}
\newcommand{\Hof}[1]{\opEntropy[#1]}
\newcommand{\xHof}[1]{\opEntropy(#1)}
\DeclareMathOperator{\opMI}{I}
\newcommand{\MIof}[1]{\opMI[#1]}
\DeclareMathOperator{\opTC}{TC}
\newcommand{\TCof}[1]{\opTC[#1]}
\newcommand{\CrossEntropy}[2]{\opEntropy(#1 \MidSymbol[\Vert] #2)}
\DeclareMathOperator{\opKale}{D_\mathrm{KL}}
\newcommand{\Kale}[2]{\opKale(#1 \MidSymbol[\Vert] #2)}
\DeclareMathOperator{\opJSD}{D_\mathrm{JSD}}
\newcommand{\JSD}[2]{\opJSD(#1 \MidSymbol[\Vert] #2)}
\DeclareMathOperator{\opp}{p}
\newcommand{\pof}[1]{\opp(#1)}
\newcommand{\hpof}[1]{\hat{\opp}(#1)}
\newcommand{\pcof}[2]{\opp_{#1}(#2)}
\newcommand{\hpcof}[2]{\hat\opp_{#1}(#2)}
\DeclareMathOperator{\opq}{q}
\newcommand{\qof}[1]{\opq(#1)}
\newcommand{\hqof}[1]{\hat{\opq}(#1)}
\newcommand{\qcof}[2]{\opq_{#1}(#2)}
\newcommand{\varHof}[2]{\opEntropy_{#1}[#2]}
\newcommand{\xvarHof}[2]{\opEntropy_{#1}(#2)}
\newcommand{\varMIof}[2]{\opMI_{#1}[#2]}
\newcommand{\w}{\boldsymbol{\theta}}
\newcommand{\W}{\boldsymbol{\Theta}}
\DeclareMathOperator{\opf}{f}
\newcommand{\fof}[1]{\opf(#1)}
\newcommand{\Dany}{\mathcal{D}}
\newcommand{\y}{y}
\newcommand{\Y}{Y}
\newcommand{\L}{\boldsymbol{L}}
\newcommand{\x}{\boldsymbol{x}}
\newcommand{\X}{\boldsymbol{X}}
\newcommand{\pdata}[1]{\hpcof{\text{data}}{#1}}
\newcommand{\normaldist}[1]{\mathcal{N}(#1)}
$$
</div>
{% endraw %}

## Background
<details markdown="1">
<summary>Information-Theoretic Notation</summary>

Information theory deals with the communication of information. In this blog post, we use information-theoretic notation to express various quantities related to probability distributions and their relationships. Here are some key concepts we will use:

The **information content** of an event $$x$$ is denoted as $$\Hof{x}$$ and is defined as $$-\log \pof{x}$$. It represents the amount of information needed to describe the occurrence of $$x$$ given an underlying probability distribution.
In machine learning, this information content is often used as a minimization objective, represented as the negative log-likelihood or cross-entropy when averaged over a dataset.

The **entropy** $$\Hof{X}$$ of a random variable $$X$$ is the expectation of its information content:

$$
\Hof{X} \triangleq \E{\pof{x}}{\Hof{x}} = \E{\pof{x}}{-\log \pof{x}}.
$$

The entropy measures the average amount of information needed to describe the random variable $$X$$. It provides a measure of uncertainty or randomness associated with $$X$$.

We will also use the **Kullback-Leibler divergence** (ðŸ¥¬) $$\Kale{\pof{X}}{\qof{X}}$$ and the **cross-entropy** $$\CrossEntropy{\pof{X}}{\qof{X}}$$:

$$
\begin{align}
\CrossEntropy{\pof{X}}{\qof{X}} & = \E{\pof{x}}{-\log \qof{x}}\\
\Kale{\pof{X}}{\qof{X}} & = \CrossEntropy{\pof{X}}{\qof{X}} - \Hof{X}
\end{align}
$$

The cross-entropy quantifies the average number of bits needed to encode samples drawn from the true distribution $$\pof{X}$$ using a different distribution $$\qof{X}$$. The Kullback-Leibler divergence is a measure of the difference between two probability distributions and captures the additional bits needed to encode samples from $$\pof{X}$$ compared to encoding them using the true distribution $$\qof{X}$$.

Now that we have refreshed our notation, let's delve into the data processing inequality.
</details>

## Data Processing Inequality

The **data processing inequality (DPI)** is a fundamental inequality in information theory that states the mutual information between two random variables cannot increase through processing. The original DPI is typically stated for a Markov chain of random variables $$X \rightarrow Y \rightarrow Z$$ and relates the mutual information terms as follows:

$$
\MIof{X;Y} \ge \MIof{X;Z}.
$$

We can view $$\rightarrow$$ as a processing or transition step, that maps $$X$$ to $$Y$$ and $$Y$$ to $$Z$$, whereas the mapping can be deterministic or stochastic. 
The inequality tells us that processing the random variable $$X$$ to obtain $$Y$$ and further processing $$Y$$ to obtain $$Z$$ cannot increase the mutual information between $$X$$ and $$Z$$ compared to the mutual information between $$X$$ and $$Y$$.

### Example 1

The following three scenarios illustrate the data processing inequality using different mappings.

#### Image Processing Pipeline

Consider an image processing pipeline with the following steps. Let:

* $$X$$ be the original image data;
* $$Y$$ be a compressed version of the image; and
* $$Z$$ be $$Y$$ after adding blur and pixelation.

In this case, $$X$$ has more mutual information with $$Y$$ than with $$Z$$. The compression reduces information, but the image is still recognizable. However, after the additional processing of blurring and pixelating, the mutual information between $$X$$ and $$Z$$ is further reduced. This gives an intuitive example of how additional processing on data reduces the mutual information with the original data. Each processing step results in some loss of information.

#### Supervised Learning
Consider a supervised learning pipeline with the following steps. Let

* $$X$$ be the input features;
* $$Y$$ be the intermediate representations learned by the model; and
* $$Z$$ be the model predictions.

Here, $$X \rightarrow Y \rightarrow Z$$ forms a Markov chain. The data processing inequality tells us that the mutual information between the inputs $$X$$ and predictions $$Z$$ cannot exceed the mutual information between the inputs $$X$$ and intermediate representations $$Y$$:

$$\MIof{X; Y} \geq \MIof{X; Z}.$$

This makes intuitive sense---the intermediate representations $$Y$$ are obtained by processing the raw inputs $$X$$, so they cannot contain more information about $$X$$ than $$X$$ itself. The predictions $$Z$$ are obtained by further processing $$Y$$, so additional information may be lost, reducing the mutual information with the original inputs $$X$$.

As a more concrete example, consider an image classification model. Let:

* $$X$$ be the input images;
* $$Y$$ be the activations of the convolutional layers; and
* $$Z$$ be predicted image labels.

The convolutional layers will extract features from the input images, but cannot extract more information than present in the original images. The predicted labels are obtained by further processing these convolutional features, so may lose some fine-grained information about the original inputs.

#### Autoencoders
An autoencoder compresses the input $$X$$ into a latent code $$Y$$ and then tries to reconstruct the original input from the code, producing $$\hat{X}$$. Let:

* $$X$$ be the input;
* $$Y$$ be the latent code; and
* $$\hat{X}$$ be the reconstruction;

The data processing inequality tells us again:

$$\MIof{X; Y} \geq \MIof{X; \hat{X}}.$$

The latent code $$Y$$ is obtained by compressing $$X$$, so cannot contain more information. The reconstruction $$\hat{X}$$ tries to recover $$X$$ from $$Y$$, but some information may be lost, reducing the mutual information with $$X$$.

Intuitively, autoencoders try to preserve as much mutual information between inputs $$X$$ and reconstructions $$\hat{X}$$ as possible by learning latent representations $$Y$$ that compress inputs without losing too much information. The data processing inequality quantifies this information bottleneck.

### Proof of the Data Processing Inequality

The proof is simple and connects the DPI to another important inequality.

First we note that the Markov Chain implies that the following factorization of the joint distribution:

$$
\pof{x, y, z} = \pof{x} \pof{y \given x} \pof{z \given y}.
$$

Using this factorization, we can express the mutual information terms:

$$
\begin{align}
\MIof{X;Y} &= \Hof{X} - \Hof{X \given Y} \\
&\ge \Hof{X} - \Hof{X \given Z} \\
&= \MIof{X;Z}.
\end{align}
$$

This relies on $$\Hof{X \given Y} \le \Hof{X \given Z}$$. Why is this true?

We have the following chain of inequalities:

$$
\Hof{X \given Y} = \underbrace{\MIof{X ; Z \given Y}}_{\overset{(1)}{=}0} + \Hof{X \given Y, Z} \overset{(2)}{\le} \Hof{X \given Z}.
$$

**(1)** follows from the Markov chain property: when $$X \rightarrow Y \rightarrow Z$$, $$X$$ does not depend on $$Z$$ at all when conditioned on $$Y$$; and **(2)** follows from the fact that conditioning reduces entropy, i.e. $$\Hof{A \given B} \le \Hof{A}.$$

The equality gap $$\Hof{X \given Y, Z} - \Hof{X \given Z}$$ corresponds to the mutual information $$\MIof{X ; Y \given Z}$$. This mutual information measures the extra information about $$X$$ contained in $$Y$$ that is not already conveyed by $$Z$$. It is zero if and only if $$X \rightarrow Z \rightarrow Y$$ forms a Markov chain, indicating that $$Z$$ is a sufficient statistic for $$X$$.

<details markdown="1">
<summary>Proof of **(2)** "Conditioning Reduces Entropy":</summary>
We can easily show that conditioning reduces entropy by using the non-negative property of the mutual information:

$$
\begin{aligned}
0 &\le \Kale{\pof{X,Y}}{\pof{X}\pof{Y}}  \\
&= \MIof{X;Y} \\
&= \Hof{X} - \Hof{X \given Y} \\
\implies \Hof{X \given Y} &\le \Hof{X}.
\end{aligned}
$$
</details>

The fact that conditioning reduces entropy, $$\Hof{X} \ge \Hof{X \given Y}$$ is a very important property by itself and can be seen as another form of the data processing inequality. $$\Hof{X \given Y}$$ tells us how much uncertainty is left after knowing $$Y$$. For example, if the mapping from $$X$$ to $$Y$$ is injective ("1:1"), then $$Y$$ contains everything about $$X$$. Vice-versa, worst-case, if $$Y$$ is independent of $$X$$, $$\Hof{X} = \Hof{X \given Y}$$. Intuitively, this says that processing $$X$$ can only ("at best") reduce the uncertainty about $$X$$ and not increase it.

Let's move on and consider the ðŸ¥¬ data processing inequality.

## ðŸ¥¬ Data Processing Inequality

A similar DPI can be expressed for different distributions $$\pof{x}$$ and $$\qof{x}$$ and the KL divergence (ðŸ¥¬) between them.
It states that if we evolve two distributions using the same transition function, they cannot become less similar. The KL divergence (ðŸ¥¬) is sometimes also referred to as "relative entropy", so we could also  call this the "*relative data processing inequality*".

This can be formalized for distributions $$\pof{x}$$ and $$\qof{x}$$ and a stochastic transition function $$X \overset{\fof{y \given x}}{\longrightarrow} Y$$:

$$
\Kale{\pof{X}}{\qof{X}} \ge \Kale{\pof{Y}}{\qof{Y}},
$$

where $$\pof{y \given x} = \fof{y \given x} = \qof{y \given x}$$. The marginals after the transition are $$\pof{y} = \E{\pof{x}}{\fof{y \given x}}$$ and $$\qof{y} = \E{\qof{x}}{\fof{y \given x}}$$, so more explicitly:

$$
\Kale{\pof{X}}{\qof{X}} \ge \Kale{\E{\pof{x}}{\fof{Y \given x}}}{\E{\qof{x}}{\fof{Y \given x}}}.
$$

Thomas and Cover describe this in their book [Elements of Information Theory](https://www.wiley.com/en-us/Elements+of+Information+Theory%2C+2nd+Edition-p-9780471241959) as "relative entropy never increases" and relate it to the second law of thermodynamics.

### Example 2: Comparing Image Distributions

As another example, let:

* $$\pof{x}$$ be the true distribution of images in a dataset;
* $$\qof{x}$$ be a generative model that tries to mimic $$\pof{x}$$; and
* $$\fof{y \given x}$$ be a function that thresholds images $$x$$ into bilevel black and white images $$y$$.

Then $$\pof{y}$$ and $$\qof{y}$$ will be more difficult to distinguish after the thresholding operation than $$\pof{x}$$ and $$\qof{x}$$. Converting to black and white images has lost information that could help distinguish the real and generated distributions.

This provides some intuition for why the ðŸ¥¬ divergence between distributions decreases under a shared stochastic mapping, as formalized by the ðŸ¥¬ data processing inequality. Processing through $$\fof{y \given x}$$ makes the distributions harder to tell apart.

### Counter-Example 3: Bayesian Inference

It might be inviting to think that this data processing inequality also applies to Bayesian inference (updating the model parameters based on new evidence). Then one could argue that if two agents start with different prior beliefs but update based on the same evidence, their posterior beliefs will become more similar. However, this intuition is flawed: the data processing inequality does not apply to Bayesian inference.
Let's walk through why.

Let:

* $$\pof{\w}$$ be an agent's prior belief;
* $$\qof{\w}$$ be another agent's different prior;
* $$\pof{\w\given x}$$ is the posterior after observing data $$x$$; and
* $$\qof{\w\given x}$$ is the other agent's posterior.

The priors $$\pof{\w}$$ and $$\qof{\w}$$ may have large divergence, representing very different initial beliefs. However, when conditioning on the same data $$x$$, the KL divergence between $$\pof{\w \given x}$$ and $$\qof{\w \given x}$$ could increase or decrease---the data processing inequality does not give us any guarantee.

This is because $$\pof{\w}$$ and $$\qof{\w}$$ are not evolving under the same stochastic mapping. Rather, each prior is mapped to its respective posterior via Bayes' rule, which operates differently on $$\opp$$ and $$\opq$$.

The correct intuition is that observing the same data $$x$$ does not necessarily bring the posterior beliefs closer together---their divergence depends on the interplay between the specific priors and likelihoods. The data processing inequality does not directly apply to this Bayesian updating scenario.

$$
\Kale{\qof{\W}}{\pof{\W}} \color{red}{\not\ge} \Kale{\qof{\W \given \mathcal{D}}}{\pof{\W \given \mathcal{D}}},
$$

This counterexample highlights the importance of precisely understanding the assumptions underlying conceptual principles like the DPI. While the DPI provides insight about information dynamics in many cases, it does not universally apply, as exemplified here by Bayesian updating under different priors.

<aside class="l-body box-note">
Identifying counterexamples sharpens our comprehension of the true meanings and limitations of information-theoretic inequalities.
</aside>

### Proofs of the ðŸ¥¬ Data Processing Inequality

We will prove this inequality in two different ways. First, we will develop a "brute-force" proof, and then we will look at a more elegant proof that follows Thomas and Cover. Importantly, we will also consider the equality case in detail.

#### Brute-force Proof

If $$\opp$$ does not have support in $$\opq$$, the inequality is trivially true because then $$\Kale{\pof{Y}}{\qof{Y}}=\infty$$.

Thus, now assume that $$\opp$$ has support in $$\opq$$. Then, we can brute-force from the cross-entropy:

$$
\begin{aligned}
\CrossEntropy{\pof{Y}}{\qof{Y}}&=\CrossEntropy{\pof{Y}}{\E{\qof{x}}{\pof{Y \given x}}}\\
&=\CrossEntropy{\pof{Y}}{\E{\qof{x}}{\frac{\pof{x \given Y}\pof{Y}}{\pof{x}}}}\\
&=\CrossEntropy{\pof{Y}}{\E{\pof{x \given Y}}{\frac{\qof{x}}{\pof{x}}}}+\CrossEntropy{\pof{Y}}{\pof{Y}}\\
&\overset{(1)}{=}\CrossEntropy{\pof{Y}}{\E{\pof{x \given Y}}{\frac{\qof{x}}{\pof{x}}}}+\xHof{\pof{Y}}\\
&\overset{(2)}{\le}\CrossEntropy{\pof{X, Y}}{\frac{\qof{X}}{\pof{X}}}+\xHof{\pof{Y}}\\
&\overset{(3)}{=}\CrossEntropy{\pof{X}}{\frac{\qof{X}}{\pof{X}}}+\xHof{\pof{Y}}\\
&\overset{(4)}{=}\Kale{\pof{X}}{\qof{X}}+\xHof{\pof{Y}}\\
\iff \Kale{\pof{Y}}{\qof{Y}}&\le\Kale{\pof{X}}{\qof{X}},
\end{aligned}
$$

where we have used **(1)** the cross-entropy of a distribution with itself is just the entropy, **(2)** the cross-entropy is convex and we can apply Jensen's inequality, **(3)** the RHS side of the cross-entropy does not depend on $$Y$$ and we can trivially marginalize it out, and **(4)** the definition of the Kullback-Leibler divergence as (unnormalized) cross-entropy of a fraction.

This makes it difficult to extract the case for equality, however.

#### Equality Case

<aside class="l-body box-note" markdown="1">
We have equality in Jensen's inequality $$\E{\pof{x}}{g(x)} \le g(\E{\pof{x}}{x})$$ when $$g$$ is affine (commutes with addition) where $$\pof{x}$$ has support. A special case is when $$g(x)$$ is constant almost everywhere in the support of $$\pof{x}$$.
</aside>

For **(2)**, this is sadly slightly more complex than it might seem on first glance. 
Let's unwrap the term:

$$
\CrossEntropy{\pof{Y}}{\E{\pof{x \given Y}}{\frac{\qof{x}}{\pof{x}}}} = \E{\pof{y}}{-\log \E{\pof{x \given y}}{\frac{\qof{x}}{\pof{x}}}}.
$$

We take an expectation over $$\pof{y}$$, so we need to look at almost all $$\pof{x \given y} \not= 0$$ for different almost all $$\pof{y}$$ separately to consider equality. $$-\log x$$ is strictly convex, so we need $$F = \frac{\qof{X}}{\pof{X}}$$ to be constant almost everywhere in the support of $$\pof{x \given y}$$ for any fixed $$y$$. Then we have equality in Jensen's inequality.

In the following, I will limit myself to the discrete case to avoid having to deal with measure theory<d-footnote>I currently don't have a good 'toolbox' to express simple ideas cleanly in measure theory. I'm working on it.</d-footnote>.
To obtain equality, for all $$y$$ with $$\pof{y} \not= 0$$ (i.e. we have support) and for all $$x_1, x_2$$ with $$\pof{x_1 \given y}, \pof{x_2 \given y} \not= 0$$, we need $$\frac{\qof{x_1}}{\pof{x_1}} = \frac{\qof{x_2}}{\pof{x_2}}$$. 
Equivalently (for the reader, why is then $$\pof{x_1} \not= 0?$$):

$$
\begin{aligned}
\frac{\qof{x_1}}{\pof{x_1}} &= \frac{\qof{x_2}}{\pof{x_2}} \\
\iff \qof{x_1} &= \frac{\qof{x_2}}{\pof{x_2}} \, \pof{x_1} \\
\end{aligned}
$$

This means that $$\qof{x} = C_y \pof{x}$$ piecewise for all $$x$$ for which $$\pof{x \given y} \not= 0$$ for some fixed $$y$$ with $$\pof{y} \not= 0$$. That is if we keep $$y$$ fixed, all the $$x$$ for which $$\pof{x \given y} \not= 0$$ have the same constant factor $$C_y$$. Then for all $$y$$ with $$\pof{y} \not= 0$$, we have equality and overall equality in **(2)**.

If for any $$x$$ there are multiple $$y$$, e.g. $$y_1, y_2$$ for which $$\pof{x \given y} \not= 0$$, then we have $$C_{y_1} = C_{y_2}$$.

As an example, at the simplest, if this is the case for all $$y$$, then $$C_y = 1$$ constant.

#### Simple & Elegant Proof

Thomas and Cover provide a beautifully simple proof:

<aside class="l-body box-note" markdown="1">
Using the chain rule of the ðŸ¥¬ divergence:

$$
\Kale{\pof{X, Y}}{\qof{X, Y}} = \Kale{\pof{X}}{\qof{X}} + \Kale{\pof{Y \given X}}{\qof{Y \given X}},
$$

and its symmetry, we have:

$$
\begin{aligned}
&\Kale{\pof{X}}{\qof{X}} + \underbrace{\Kale{\pof{Y\given X}}{\qof{Y \given X}}}_{=\Kale{\fof{Y\given X}}{\fof{Y \given X}}=0}\\
&\quad =\Kale{\pof{X, Y}}{\qof{X, Y}}\\
&\quad =\Kale{\pof{Y}}{\qof{Y}}+\underbrace{\Kale{\pof{X \given Y}}{\qof{X \given Y}}}_{\ge 0}\\
&\quad \ge \Kale{\pof{Y}}{\qof{Y}}.
\end{aligned}
$$

We have equality exactly when $$\pof{x \given y} = \qof{x \given y}$$ for (almost) all $$x, y.$$
</aside>

What does this mean? Whereas $$\fof{y \given x}$$ is the 'forward' transition function, $$\pof{x \given y}$$ and $$\qof{x \given y}$$ are the 'backward' transition functions. We only have equality when the backward transition functions are equal (almost everywhere). 

The statement on equality is not very informative yet though, so we have to put in a bit more work. Again, this is written for the discrete case.

This time we explicitly use Bayes' rule to connect the forward and backward transition functions.
First, we have to fix $$y$$ such that $$\pof{y} \not= 0$$ (i.e. $$y$$ is in the support of $$\pof{y}$$) and then $$\qof{y} \not=0$$.
We have:

$$
\begin{aligned}
\pof{x \given y} &= \qof{x \given y} \\
\overset{\text{ass. }\pof{y} \not= 0}{\iff} \frac{\fof{y \given x}\pof{x}}{\pof{y}} &= \frac{\fof{y \given x}\qof{x}}{\qof{y}} \\
\overset{\text{ass. }\fof{y \given x}\not= 0}{\iff} \frac{\pof{x}}{\pof{y}} &= \frac{\qof{x}}{\qof{y}} \\
\iff \pof{x} &= \frac{\pof{y}}{\qof{y}} \, \qof{x}.
\end{aligned}
$$

For a given $$y$$ with $$\pof{y} \not=0$$, for the equality case, we see that for all $$x$$ with $$\fof{y \given x} \not= 0$$, $$\pof{x}$$ and $$\qof{x}$$ have to be coupled via piecewise constant factors. 

As another example, if $$\fof{y \given x} \not=0$$ (has full support) for all possible $$x$$, for the equality case we have $$\pof{x} = \qof{x}$$.

Compared to the previous equality case we went a bit deeper and rewrote the conditions to consider the ratios between $$x$$ and $$y$$. Note we could have shown the same thing in the "brute-force" proof, too. 

Altogether, we have see that both $$x$$ and $$y$$ are modulated by the same constant factor between $$\pof{\cdot}$$ and $$\qof{\cdot}$$. Essentially, this tells us that we could split our support into unconnected sub-domains and examine each individually for the equality case.

<aside class="l-body box-note" markdown="1">
One technicality is the question of what $$\pof{y \given x}$$ is when $$\pof{x} = 0$$. We could define it to be anything we want, but really it is undefined. 

Previously, we said that we want $$\pof{y \given x} = \fof{y \given x}$$ and $$\qof{y \given x} = \fof{y \given x}$$, but, given the above, we only need these equalities to hold where $$\pof{x} \not= 0$$ and $$\qof{x} \not= 0$$, respectively.

On the other hand, $$\pof{x} = 0$$ does imply $$\pof{x \given y} = 0$$.
</aside>

### Overall Statement 
We have the following overall statement:

<aside class="l-body box-warning" markdown="1">
Given $$\pof{x}$$ and $$\qof{x}$$ and shared transition function $$\fof{y \given x}$$ for the model $$X \rightarrow Y$$, the relative ðŸ¥¬ data processing inequality is:

$$
\Kale{\pof{X}}{\qof{X}} \ge \Kale{\pof{Y}}{\qof{Y}},
$$

When $$\pof{x} \ll \qof{x}$$, we have equality when $$\pof{X \given Y} = \qof{X \given Y}$$.
</aside>
($$\pof{x} \ll \qof{x}$$ means that $$\qof{x} > 0$$ implies $$\pof{x} > 0$$, so the ðŸ¥¬ divergence is not $$\infty$$.)

More precisely, for $$\pof{x} \ll \qof{x}$$, we have equality when:

$$
\forall y, \pof{y} \not= 0 \exists C_y \in \mathbb{R}_{> 0} \forall x, \fof{y \given x}\not=0\colon \pof{x} = C_y \, \qof{x}.
$$

## Revisiting Data Processing Inequalities

Now, we can use this to derive a few additional results and also to close the circle to the original data processing inequality.

### Jensen-Shannon Divergence

The KL (ðŸ¥¬) divergence is not a metric. The triangle inequality does not hold, and it is not symmetric.

We can symmetrize it to obtain the [Jensen-Shannon divergence (JSD)](https://en.wikipedia.org/wiki/Jensen%E2%80%93Shannon_divergence). The JSD is defined as the mean of the two ðŸ¥¬ divergences of the two distributions from their average. It essentially makes the ðŸ¥¬ divergence symmetric:

$$
\begin{aligned}
\fof{x} &= \frac{\pof{x} + \qof{x}}{2}\\
\JSD{\pof{x}}{\qof{x}} &= \frac{1}{2} \Kale{\pof{x}}{\fof{x}} + \frac{1}{2} \Kale{\qof{x}}{\fof{x}}.
\end{aligned}
$$

Similar approaches can be used to "symmetrize" other concepts; for example matrices: $$\frac{1}{2} A + \frac{1}{2} A^T$$ is also symmetric by construction for any matrix $$A$$.

The square root of the Jensen-Shannon divergence is symmetric and satisfies the triangle inequality---it is a metric: the *Jensen-Shannon distance*.

### The JSD Data Processing Inequality

We can also obtain a data processing inequality for the Jensen-Shannon divergence and the Jensen-Shannon distance:

<aside class="l-body box-warning" markdown="1">
Given $$\pof{x}$$ and $$\qof{x}$$ and shared transition function $$\fof{y \given x}$$ for the model $$X \rightarrow Y$$, the Jensen-Shannon divergence data processing inequality is:

$$
\JSD{\pof{X}}{\qof{X}} \ge \JSD{\pof{Y}}{\qof{Y}}.
$$

We have equality exactly when $$\pof{x \given y} = \qof{x \given y}$$ almost everywhere.
</aside>

The proof uses the ðŸ¥¬ data processing inequality:

$$
\begin{aligned}
\JSD{\pof{X}}{\qof{X}} &= \frac{1}{2} \Kale{\pof{X}}{\fof{X}} + \frac{1}{2} \Kale{\qof{X}}{\fof{X}}\\
&\ge \frac{1}{2} \Kale{\pof{Y}}{\fof{Y}} + \frac{1}{2} \Kale{\qof{Y}}{\fof{Y}}\\
&= \JSD{\pof{Y}}{\qof{Y}}.
\end{aligned}
$$

We verify $$\fof{y} = \frac{\pof{y} + \qof{y}}{2}$$ is the average of $$\pof{y}$$ and $$\qof{y}$$:

$$
\begin{aligned}
\fof{y} &= \E{\fof{x}}{\fof{y \given x}}\\
&= \E{\frac{\pof{x}+\qof{x}}{2}}{\fof{y \given x}}\\
&= \frac{1}{2} \E{\pof{x}}{\fof{y \given x}} + \frac{1}{2} \E{\qof{x}}{\fof{y \given x}}\\
&= \frac{1}{2} \pof{y} + \frac{1}{2} \qof{y}.
\end{aligned}
$$

Finally, $$\pof{x}, \qof{x} \ll \fof{x}$$, and the equality condition of the ðŸ¥¬ data processing inequality gives us:

$$
\begin{aligned}
&\Kale{\pof{X \given Y}}{\fof{X \given Y}} = 0 &\\
&\Kale{\qof{X \given Y}}{\fof{X \given Y}} = 0 &\\
\implies &\pof{x \given y} = \fof{x \given y} \land \qof{x \given y} = \fof{x \given y}& \forall x,y \\
\implies &\pof{x \given y} = \qof{x \given y}& \forall x,y.
\end{aligned}
$$

### Mutual Information

The JSD can also be expressed as a mutual information. For 
$$
\begin{aligned}
Z &\sim \mathrm{Bernoulli}(\frac{1}{2}) = \fof{Z} \\
X \given Z = 0 &\sim \pof{x}\\
X \given Z = 1 &\sim \qof{x},
\end{aligned}
$$

we have:

$$
\JSD{\pof{X}}{\qof{X}} = \MIof{X;Z}.
$$

This follows from rewriting the mutual information as a ðŸ¥¬ divergence:

$$
\begin{aligned}
\MIof{X;Z} &= \Kale{\fof{X \given Z}}{\fof{X}}\\
&= \E{\fof{z}} {\Kale{\fof{X \given Z = z}}{\fof{X}}}\\
&= \frac{1}{2} \Kale{\pof{x}}{\fof{x}} + \frac{1}{2} \Kale{\qof{x}}{\fof{x}}\\
&= \JSD{\pof{X}}{\qof{X}}.
\end{aligned}
$$

We can generalize this to the Markov chain $$Z \rightarrow X \rightarrow Y$$ with $$\fof{z, x, y} = \fof{z} \fof{x \given z} \fof{y \given x}$$ for any distribution $$\fof{z}$$:

$$
\begin{aligned}
\MIof{X;Z} &= \Kale{\fof{X \given Z}}{\fof{X}}\\
&= \E{\fof{z}} {\Kale{\fof{X \given z}}{\fof{X}}}\\
&\overset{(1)}{\ge} \E{\fof{z}} {\Kale{\fof{Y \given z}}{\fof{Y}}}\\
&= \Kale{\fof{Y \given Z}}{\fof{Y}}\\
&= \MIof{Y;Z},
\end{aligned}
$$

where $$(1)$$ follows from the ðŸ¥¬ data processing inequality.

This is just the data processing inequality we presented initially.

The equality gap (*Jensen gap*) is $$\Kale{\fof{X \given Y, Z}}{\fof{X \given Y}}$$. We have equality when:

$$
\begin{aligned}
\Kale{\fof{X \given Y, Z}}{\fof{X \given Y}} &= 0\\
\implies \MIof{X;Z \given Y} &= 0.
\end{aligned}
$$

This is exactly when $$X$$ is independent of $$Z$$ given $$Y$$. (Then $$Y$$ is a sufficient statistic.)

**Summary.** We have shown that we can derive the data processing inequality using the ðŸ¥¬ data processing inequality.

## Conclusion

In this blog post, we explored the data processing inequality and its applications in information theory. Specifically, we discussed both the original data processing inequality and the ðŸ¥¬ data processing inequality. We provided derivations and explanations for these inequalities, emphasizing their significance and limitations.

While the data processing inequality provides valuable insights into information dynamics in various scenarios, it is crucial to consider the specific assumptions and conditions under which it holds. The examples and counterexample hopefully demonstrate the nuances of applying these inequalities in different contexts.

By understanding the foundations and limitations of the data processing inequality, we can leverage its insights effectively in information theory and related fields.

---

In the [first part](/2023/08/sdpi_fsvi/) of this two-part series on *Function-Space Variational Inference (**FSVI**)*, we looked at the *Data Processing Inequality (**DPI**)*. 
In this second part, we finally look at the relationship between FSVI, a method focusing on the Bayesian predictive posterior rather than the parameter space, and the DPI.c
We will recover several interesting results from the literature in a simple form, and hopefully build intuitions for the relationship between parameter and functional priors. 

Most importantly, we consider how FSVI can measure a **predictive** divergence between the approximate and true posterior which is independent of parameter symmetries. 
With parameter symmetries, I refer to different parameters that yield the same predictions, which is very common in over-parameterized neural networks:
think of parameter symmetries like different paths leading to the same destination; they might look different but end up at the same predictions<d-footnote>Thanks to ChatGPT for this analogy! ðŸ¤—</d-footnote>.

As a nice example and application, we relate FSVI to training with label entropy regularization: a potentially more meaningful prior than the ones usually used in Bayesian neural networks<d-footnote>In many papers, an isotropic Gaussian is used, even though there are not many reasons given to do so except for its simplicity. Indeed, there are better alternatives, see <https://arxiv.org/abs/2102.06571> and <https://arxiv.org/abs/2105.06868>.</d-footnote>.

Variational inference is a powerful technique for approximating complex Bayesian posteriors with simpler distributions. In its usual form, it optimizes an approximate, *variational* distribution to match the *Bayesian **parameter** posterior* as closely as possible. This way, it transforms the problem of Bayesian inference into an optimization problem.

Especially for deep neural networks, obtaining a good approximation of the parameter space can be difficult. One reason is the sheer size of the parameter space. Additionally, the parameterization of a neural network often contains many symmetries (different parameter configurations can lead to the same predictions of the model) that are not taken into account either. 

Function-space variational inference (FSVI) side-steps some of these restrictions by only requiring that the variational distribution matches the *Bayesian **predictive** posterior*. 

# Data Processing Inequality

The DPI states that processing data stochastically can only reduce information. More formally:

<aside class="l-body box-note" markdown="1">
Consider two distributions $$\qof{\W}$$ and $$\pof{\W}$$ over a random variable $$\W$$. Let $$Y = \fof{\W}$$ be a stochastic mapping from $$\W$$ to $$Y$$. Then the DPI states:

$$\Kale{\qof{\W}}{\pof{\W}} \ge \Kale{\qof{Y}}{\pof{Y}}.$$

</aside>

That is, the KL divergence (*ðŸ¥¬ divergence*) between $$\qof{Y}$$ and $$\pof{Y}$$ cannot be larger than between the original $$\qof{\W}$$ and $$\pof{\W}$$. Intuitively, the stochastic mapping $$\opf$$ induces a bottleneck that reduces how well we can distinguish between $$\opp$$ and $$\opq$$. Finally we have equality when $$\Kale{\qof{\W \given Y}}{\pof{\W \given Y}} = 0$$. See the [first part of this two-part series](https://blog.blackhc.net/2023/08/sdpi_fsvi/) for more details.

The paper "*Understanding Variational Inference in Function-Space*" by Burt et al. (2021)<d-cite key="burt2021understanding"></d-cite> succinctly summarizes the DPI as follows:

<blockquote>
The data processing inequality states that if two random variables are transformed in this way, they cannot become easier to tell apart.
</blockquote>

## Problem Setting & Notation

In the following, I assume a classification task with cross-entropy loss. This post uses the following notation:

- $$\y$$ is the label,
- $$\x$$ is the input,
- $$\qof{\y \given \x}$$ is the predictive distribution we want to learn,
- $$\pdata{\y \given \x}$$ is the data distribution, and
- $$C$$ is the number of classes.

The probabilistic model is as usual:

$$\pof{\y, \w \given \x} = \pof{\y \given \x, \w} \, \pof{\w}.$$

Note that I drop conditioning on $$\x$$ for simplicity, and also note, that I use upper-case letters for random variables, I'm taking an expectation over in some form, e.g.\ the ðŸ¥¬ divergence, and lower-case letters when I'm referring to specific observations or values that could be substituted (with the exception of $$\Dany$$).


## Chain Rule of the ðŸ¥¬ Divergence & DPI

An important property of the ðŸ¥¬ divergence is the chain rule:

$$
\Kale{\qof{\Y_n,...,\Y_1}}{\pof{\Y_n,...,\Y_1}} = \sum_{i=1}^n \Kale{\qof{\Y_i \given 
\Y_{i-1}, ..., \Y_1}}{\pof{\Y_i \given \Y_{i-1}, ..., \Y_1}}.
$$

Above application of the chain rule yields a *chain inequality* for the DPI as well:

$$
\begin{align}
\Kale{\qof{\W}}{\pof{\W}} &\ge \Kale{\qof{\Y_n,...,\Y_1}}{\pof{\Y_n,...,\Y_1}}\\
&\ge \Kale{\qof{\Y_{n-1},...,\Y_1}}{\pof{\Y_{n-1},...,\Y_1}}\\
&\ge \Kale{\qof{\Y_1}}{\pof{\Y_1}}.
\end{align}
$$

# Function-Space Variational Inference

The DPI has an intriguing connection to FSVI. Let's say we want to approximate a Bayesian posterior $$\pof{\w \given \Dany}$$ with a variational distribution $$\qof{\w}$$. In standard VI, we would minimize $$\Kale{\qof{\W}}{\pof{\W \given \Dany}}$$ to match the variational distribution to the Bayesian posterior. Specifically:

$$
\begin{align}
\Kale{\qof{\W}}{\pof{\W \given \Dany}} &= \underbrace{\E{\qof{\w}}{-\log \pof{\Dany \given \w}} + \Kale{\qof{\W}}{\pof{\W}}}_{\text{Evidence}\ \text{Bound}} + \log \pof{\Dany} \ge 0 \\
\iff \underbrace{-\log \pof{\Dany}}_{=\xHof{\pof{\Dany}}} &\le \E{\qof{\w}}{-\log \pof{\Dany \given \w}} + \Kale{\qof{\W}}{\pof{\W}}.
\end{align}
$$

This derives an information-theoretic evidence upper bound on the information content $$-\log \pof{\Dany}$$ of the data $$\Dany$$ under the variational distribution $$\qof{\w}$$.
In other more probability-theory inspired literature, the negative of this bound is called the *evidence lower bound (ELBO)*, which we maximize.

In FSVI (with a caveat I detail below), we apply the DPI to the prior ðŸ¥¬ divergence term and obtain a functional version of the evidence bound:

$$
\begin{align}
\Kale{\qof{\W}}{\pof{\W}} \ge \Kale{\qof{\Y... \given \x...}}{\pof{\Y... \given \x...}},
\end{align}
$$

where $$\Y... \given \x...$$ are (finite or infinite) sets of samples. That is, we do not only optimize marginal distributions but also joint distributions.

<aside class="l-body box-note" markdown="1">
For more on the specific differences between marginal and joint objectives, the workshop paper <https://arxiv.org/abs/2205.08766> provides conceptual thoughts in the same light, which builds on the following important works:

* "The neural testbed: evaluating joint predictions" by Osband et al, 2022<d-cite key="osband2022neural"></d-cite>
* "Evaluating high-order predictive distributions in deep learning" by Osband et al, 2022<d-cite key="osband2022evaluating"></d-cite>
* "Beyond marginal uncertainty: how accurately can Bayesian regression models estimate posterior predictive correlations?" by Wang et al, 2021 <d-cite key="wang2021beyond"></d-cite>
</aside>

The resulting objective

$$
\begin{align}
\E{\qof{\w}}{-\log \pof{\Dany \given \w}} + \Kale{\qof{\Y... \given \x...}}{\pof{\Y... \given \x...}}
\end{align}
$$

is equal to the (negative) *functional ELBO (fELBO)* in "*Functional variational Bayesian neural networks*" by Sun et al. (2019)<d-cite key="sun2019functional"></d-cite>---with caveats that we discuss below.

## Choosing $$\x...$$

One important detail is the question of how to choose the $$\x...$$:

Ideally, we want to choose them such that the DPI inequality is as tight as possible.

Given the chain inequality, it is obvious that the larger the set $$\x...$$, the tighter the inequality will be. 
Hence, if we could choose an infinite set of points well, we might be able to get the tightest possible inequality. 
However, this might not be tractable and in practice, it is often not.

Some works take a supremum over finite subsets of a certain size, essentially building a core-set as an approximation (Rudner et al., 2022a<d-cite key="rudner2022tractable"></d-cite>/b<d-cite key="rudner2022continual"></d-cite>); 
others take an expectation over the finite sets of input samples (Sun et al., 2019)<d-cite key="sun2019functional"></d-cite>, which is not necessarily yielding the tightest inequality but provides an unbiased estimate; while again other works focus on finite datasets for which the all points can be taken into account (Klarner et al., 2023<d-cite key="klarner2023drug"></d-cite>).

We will discuss the tightness of the inequality and the implications in the data limit below.

Focusing on the most important aspect of FSVI, we observe:

<aside class="l-body box-important" markdown="1">
Instead of a parameter prior (term) $$\Kale{\qof{\W}}{\pof{\W}}$$, we have a functional or *predictive* prior (term) $$\Kale{\qof{\Y... \given \x...}}{\pof{\Y... \given \x...}}$$.
</aside>

## Continual Learning 

When we directly optimize the ðŸ¥¬ divergence on the finite input dataset, for example, we align $$\opq$$ with the prior of $$\opp$$ where it matters most: on the predictions of the observed data.

This is of particular interest in continual learning, where the prior for the next task is chosen to be the posterior from the previous task. In this case, the functional ELBO can be used to approximate the posterior of the previous model while incorporating new data.

For two great papers that are very readable and provide further insights, see "*Continual learning via sequential function-space variational inference*"<d-cite key="rudner2022continual"></d-cite> and "*Tractable function-space variational inference in Bayesian neural networks*"<d-cite key="rudner2022tractable"></d-cite>, both by Rudner et al. (2022).

## Comparison to FSVI in the literature

<aside class="l-body box-error" markdown="1">
An important detail is that in above FSVI papers, the DPI is applied to the *logits* not the *probabilities*. That is we have a distribution over logits, which adds an extra layer of uncertainty. Unlike the class probabilities, which we can also obtain from a deterministic neural network, thanks to the semantic interpretation of the outputs as distribution $$\pof{\y \given \x, \w}$$, a deterministic neural network only provides us with deterministic logits and not a distribution over them. Only a (more) Bayesian approach can provide this.

In general, we have the following inequality chain that relates the ðŸ¥¬ divergence between the logits $$\L$$, which is a vector unlike $$\Y$$, and the ðŸ¥¬ divergence between the output probabilities for $$\Y$$ to the ðŸ¥¬ divergence between the parameters $$\W$$:

$$
\begin{align}
\Kale{\qof{\W}}{\pof{\W}} &\ge \Kale{\qof{\L...\given \x...}}{\pof{\L...\given \x...}} \\
&\ge \Kale{\qof{\Y...\given \x...}}{\pof{\Y...\given \x...}}.
\end{align}
$$

</aside>


In practice, both works by Rudner et al.\ (2022), for example, linearize the logits<d-footnote>The final activations of the neural network before applying the softmax function (in multi-class classification), for example. Not to be confused with the pre-logits (e.g. embeddings).</d-footnote> (similar to a Laplace approximation) and use the DPI to shown (in their notation):

$$
\mathbb{D}_{\mathrm{KL}}\left(q_{f(\cdot ; \boldsymbol{\Theta})} \| p_{f(\cdot ; \boldsymbol{\Theta})}\right) \leq \mathbb{D}_{\mathrm{KL}}\left(q_{\Theta} \| p_{\Theta}\right)
$$

which in my notation is equivalent to the above:

$$
\Kale{\qof{\L...\given \x...}}{\pof{\L...\given \x...}} \le \Kale{\qof{\W}}{\pof{\W}}.
$$

They maximize the fELBO objective:

$$
\mathcal{F}\left(q_{\boldsymbol{\Theta}}\right)=\mathbb{E}_{q_{f\left(\mathbf{x}_{\mathcal{D}} ; \boldsymbol{\Theta}\right)}}\left[\log p_{\mathbf{y} \mid f(\mathbf{X} ; \boldsymbol{\Theta})}\left(\mathbf{y}_{\mathcal{D}} \mid f\left(\mathbf{X}_{\mathcal{D}} ; \boldsymbol{\theta}\right)\right)\right]-\sup _{\mathbf{X} \in \mathcal{X}_{\mathbb{N}}} \mathbb{D}_{\mathrm{KL}}\left(q_{f(\mathbf{X} ; \boldsymbol{\Theta})} \| p_{f(\mathbf{X} ; \boldsymbol{\Theta})}\right),
$$

which is equivalent to minimizing the information-theoretic objective:

$$
\E{\qof{\w}}{-\log \pof{\Dany \given \w}} + \Kale{\qof{\L... \given \x...}}{\pof{\L... \given \x...}},
$$
if we choose the $$\x...$$ to tighten the DPI inequality as much as possible (i.e.\ choosing the supremum).

Using the inequality chain from above, we can sandwich their objective between a regular (negative) ELBO and the (negative) functional ELBO, we have derived above:

$$
\begin{aligned}
&\E{\qof{\w}}{-\log \pof{\Dany \given \w}} + \Kale{\qof{\W}}{\pof{\W}} \\
&\quad \E{\qof{\w}}{-\log \pof{\Dany \given \w}} + \Kale{\qof{\L... \given \x...}}{\pof{\L... \given \x...}} \\
&\quad \ge \E{\qof{\w}}{-\log \pof{\Dany \given \w}} + \Kale{\qof{\Y... \given \x...}}{\pof{\Y... \given \x...}}.
\end{aligned}
$$

In practice, using the probabilities instead of logits when performing linearization is often cumbersome due to the non-linearity of the softmax functions, which requires Monte-Carlo sampling of the logits to obtain an approximation of the final probabilities. I speculate that sampling the logits can be more benign given that we often use ReLUs in the underlying neural networks. (Don't quote me too strongly on this, though.)

Conceptually, this explains the derivation of their ELBO objective and also relates them to the 'purer' and simpler functional evidence bound derived above, but this raises the question of how these different inequalities are different and what the gap between them tells us. We will address this question in the next section.

# The Equality Case and Equivalence Classes

When do we have $$\Kale{\qof{\W}}{\pof{\W}} = \Kale{\qof{\Y... \given \x...}}{\pof{\Y... \given \x...}}$$? And what does it tell us?
As we have seen in the first part, we have equality in the DPI if and only $$\Kale{\qof{\W \given \Y..., \x...}}{\pof{\W \given \Y..., \x...}}=0$$.

Given that we are trying to approximate the Bayesian posterior $$\pof{\w \given \Y..., \x...}$$ using $$\qof{\w}$$, this equality condition tells us that we would have to find the exact posterior for equality. 
Hence, it is unlikely that we will have equality in practice. Thus, this raises the question of what this predictive prior term $$\Kale{\qof{\Y... \given \x...}}{\pof{\Y... \given \x...}}$$ provides us with.

Another way to think about the gap between the two ðŸ¥¬ divergences is that one is parameter-based and the other one is not:
Deep neural networks have many parameter symmetries, and it is often possible to permute the weights of a neural network without changing the outputs---for example, in a convolutional neural network, we could swap channels.
 
The functional ðŸ¥¬ divergences won't be affected by this as they are parameter-free and do not take into account the parameters of the model but only the predictions. 
The regular parameter-based ðŸ¥¬ divergence, however, would be affected by this---depending on the prior $$\pof{\w}$$.

If the prior assigns different probability to otherwise equivalent parameters, this obviously changes the parameter posterior, while the outputs are invariant to these changes if the overall assigned probability remains the same.

<aside class="l-body box-important" markdown="1">
A better name for the functional ðŸ¥¬ divergence and function-space variational inference might be *predictive* ðŸ¥¬ divergence and *predictive variational inference*. 

To me, functions are an abstract thing, and determining how to assign a prior or even a distribution to them sounds difficult and complex, adding an air of mystique for me, whereas predictions are something we can easily understand and relate to.
</aside>

## Equivalence Classes

Unless there are other considerations, it makes sense to use priors that assign the same density to parameters that are equivalent. 
Hence, for a given function $$\fof{\x ; \w}$$, which determines the likelihood $$\pof{\y \given \x, \w} \triangleq \pof{y \given \fof{\x ; \w}}$$, we can define an equivalence relation such that $$\w \sim \w'$$ if and only if $$\fof{\x; \w} = \fof{\x; \w'}$$ *for all* $$\x$$. 
This equivalence relation partitions the parameter space into equivalence classes:

$$[\w] \triangleq \{\w' : \fof{x ; \w} = \fof{x ; \w} \quad \forall x \}.$$

Any prior $$\pof{\w}$$ induces a prior $$\hpof{[\w]}$$ over the equivalence classes:

$$\hpof{[\w]} \triangleq \sum_{\w' \in [\w]} \pof{\w'}.$$

---or $$\int_{[\w]} \pof{\w'} \, d \w'$$ for continuous $$\w$$---with the corresponding model:

$$
\begin{aligned}
\hpof{\y, [\w] \given \x} &\triangleq \hpof{\y \given [\w], \x} \, \hpof{[\w]} \\
&= \pof{\y \given \x, \w} \, \hpof{[\w]}.
\end{aligned}
$$

<aside class="l-body box-error" markdown="1">
Crucially, all of this depends on which $$\x$$ we consider as input domain. Different domains for $$\x$$ will induce different equivalence classes. 

For example, for function families on $$\mathbb{R} \to \mathbb{R}$$, if we restrict the domain to $$\x \in (0, 1)$$, the equivalence classes will be different from the domain $$\x \in \mathbb{R}$$. For $$\x \in (0, 1)$$, we do not differentiate between functions that behave differently outside this domain. For any $$k \ge 1$$,

$$
\fof{x} = \begin{cases} 
x^2 & \text{if } |x| \leq k \\
2|x| - 2k + k^2  & \text{otherwise}
\end{cases}
$$

has the same predictions within the domain $$\x \in (0, 1)$$ while being different outside of it for different $$k$$.
</aside>


## Consistency

Importantly, this definition is consistent with Bayesian inference:

<aside class="l-body box-note" markdown="1">$$[\w]$$ commutes with Bayesian inference:

$$
\hpof{[\w] \given \Dany} = \sum_{\w' \in [\w]} \pof{\w' \given \Dany}.
$$

</aside>

This is easy to show with the definition and application of Bayes' rule:

$$
\begin{aligned}
\hpof{[\w] \given \Dany} &= \hpof{\Dany \given [\w]} \, \hpof{[\w]} / \hpof{\Dany} \\
&= \pof{\Dany \given \w} \sum_{\w' \in [\w]} \pof{\w'} / \hpof{\Dany} \\
&=  \sum_{\w' \in [\w]} \pof{\Dany \given \w'} \, \pof{\w'} / \hpof{\Dany} \\
&=  \sum_{\w' \in [\w]} \pof{\w' \given \Dany} \, \pof{\Dany} / \hpof{\Dany} \\
&= \sum_{\w' \in [\w]} \pof{\w' \given \Dany}.
\end{aligned}
$$

The last step follows from $$\hpof{\Dany}=\pof{\Dany}$$:

$$
\begin{aligned}
\hpof{\Dany} &= \sum_{[\w]} \hpof{\Dany, [\w]} \\
&= \sum_{[\w]} \sum_{\w' \in [\w]} \pof{\Dany, \w'} \\
&= \sum_{\w'} \pof{\Dany, \w} \\
&= \pof{\Dany}. 
\end{aligned}
$$

This also tells us that, for any $$\x$$ and $$\y$$, $$\pof{\y... \given \x...} = \hpof{\y... \given \x...}$$.

Given this consistency, we don't have to differentiate between $$\hat\opp$$ and $$\opp$$ and can use $$\opp$$ interchangeably.
The same holds for $$\opq$$.

<aside class="l-body box-important" markdown="1">
This is not a special property of the equivalence class mapping but a general property of applying functions to random variables: $$[\cdot]$$ maps the random variable $$\W$$ to the transformed random variable $$[\W]$$, and this mapping commutes with Bayesian inference.

Above proof is valid for any function $$\fof{\cdot}$$ applied to a random variable.
</aside>

## Equality & Symmetries

We can view $$[\w]$$ as a projection from $$\w$$ to its equivalence class $$[\w]$$. The DPI then gives us:

$$
\Kale{\qof{\W}}{\pof{\W}} \ge \Kale{\qof{[\W]}}{\pof{[\W]}}.
$$

What does the gap between the two terms tell us? 

<aside class="l-body box-error" markdown="1">
Intuitively speaking, $$\Kale{\qof{[\W]}}{\pof{[\W]}}$$ captures the meaningful divergence between approximate and true posterior, while $$\Kale{\qof{\W}}{\pof{\W}}$$ includes any divergence due to parameter symmetries that has no effect on the predictions, now or later.
</aside>

Let's look at a few examples to get a better understanding of this.

1. Trivial Constant Case

Let $$\fof{\x ; \w} = 0$$ independent of any $$f$$. Then $$[\w] = [\w']$$ for any $$\w$$, $$\w'$$.

For any approximate distribution $$\qof{\w}$$, the induced $$\Kale{\qof{[\W]}}{\pof{[\W]}}=0$$, while $$\Kale{\qof{\W}}{\pof{\W}}$$ also includes superfluous divergence.

2. Unused Parameter

Let $$\y \given (\w_1, \w_2) = \w_1$$ deterministic but independent of $$\w_2$$. Then $$[(\w_1, \w_2)] = [(\w_1, {\w'}_2)]$$ for any $${\w'}_2$$ and $$[(\w_1,*)]\not=[({\w'}_1, *)]$$ for any $$\w_1 \not= \w'_1$$.

Then $$\Kale{\qof{[\W]}}{\pof{[\W]}}=\Kale{\qof{\W_1}}{\pof{\W_1}}$$ and captures purely the meaningful divergence between approximate and true posterior, while $$\Kale{\qof{\W}}{\pof{\W}}$$ includes any divergence due to parameter symmetries across all $$\w_2$$ that has no effect on the predictions.

3. Periodic Parameter Space

Finally, let's assume that the predictions are periodic in some way. That is, e.g.\ $$\y = \sin \w$$. Obviously, $$[\w] = [\w + 2\pi]$$ then. 

Further, let $$\pof{\w} = \operatorname{U}(\w; [0,2\pi \, N))$$ for some $$N$$ that determines the number of periods. Then, if we introduce another random variable $$K$$, that captures which period we are in, we can (again) use the chain rule to write:

$$
\begin{aligned}
\Kale{\qof{\W}}{\pof{\W}} &= \Kale{\qof{\W \given \W \in [K\,2\pi, (K+1)\,2\pi]}}{\pof{\W \given \W \in [K\,2\pi, (K+1)\,2\pi]}} \\
&\quad + \Kale{\qof{\W \in [K\,2\pi, (K+1)\,2\pi]}}{\pof{\W \in [K\,2\pi, (K+1)\,2\pi]}} \\
&= \Kale{\qof{[\W]}}{\pof{[\W]}} \\
&\quad + \Kale{\qof{\W \in [K\,2\pi, (K+1)\,2\pi]}}{\pof{\W \in [K\,2\pi, (K+1)\,2\pi]}}. 
\end{aligned}
$$

This follows from the setup of this specific example.

Finally, $$\Kale{\qof{\W \in [K\,2\pi, (K+1)\,2\pi]}}{\pof{\W \in [K\,2\pi, (K+1)\,2\pi]}} \le \log N$$.

So, if $$\opq$$ only had support in a single period for example, the difference between $$\Kale{\qof{\W}}{\pof{\W}}$$ and $$\Kale{\qof{[\W]}}{\pof{[\W]}}$$ would be $$\log N$$, capturing the redundancy.

## Predictive Prior

How does the predictive prior term fit into this? The DPI again yields the answer:

<aside class="l-body box-important" markdown="1">
As the predictions are independent of the parameters, we can map from equivalence classes $$[\w]$$ to predictions, and the DPI yields:

$$
\begin{align}
\Kale{\qof{\W}}{\pof{\W}} &\ge \Kale{\qof{[\W]}}{\pof{[\W]}} \\
&\ge \Kale{\qof{\Y...\given\x...}}{\pof{\Y...\given\x...}}.
\end{align}
$$

</aside>

This tells us that the predictive prior term can at best measure the ðŸ¥¬ divergence between the equivalence classes of the parameters---and not between the parameters itself. Luckily, this is the more meaningful divergence anyway.

For the equality cases, we note that:

1. we need a 1:1 mapping between parameters and equivalence classes for the first bound to be tight, and
2. we need $$\Kale{\qof{\Y_n,...,\Y_1\given\x_n,...,\x_1}}{\pof{\Y_n,...,\Y_1\given\x_n,...,\x_1}} \to 0$$ for $$n \to \infty$$ for the second bound to be tight.

For **2.**, we know from the chain rule that $$\Kale{\qof{\Y_n,...\Y_1\given\x_n,...,\x_1}}{\pof{\Y_n,...\Y_1\given\x_n,...,\x_1}}$$ is monotonically increasing in $$n$$, and as it is bounded by $$\Kale{\qof{[\W]}}{\pof{[\W]}}$$ from above, it must converge.

To give intuition and without attempting to prove this formally, we can appeal to [*Bernstein von Mises* theorem](https://en.wikipedia.org/wiki/Bernstein%E2%80%93von_Mises_theorem), which states that the posterior distribution of the parameters converges to a Gaussian distribution with mean and variance given by the maximum likelihood estimate (MLE) as the number of samples goes to infinity *as long as the model parameters are identifiable, that is the true parameters we want to learn are unique*:

In the space of equivalence classes given our definitions, the \[MLE\] will be unique by definition and thus the model identifiable, and as the MLE is prior-independent, both $$\opq$$ and $$\opp$$ will converge to the MLE. In other words, both $$\opq$$ and $$\opp$$ will converge to the same equivalence class, and $$\Kale{\qof{[\W]\given \Y..., \x...}}{\pof{[\W] \given \Y..., \x...}} \to 0$$ for $$n \to \infty$$.
Thus, we have:

$$
\begin{align}
\Kale{\qof{[\W]}}{\pof{[\W]}} = \sup_{n\in \mathbb{N}} \Kale{\qof{\Y_n,...,\Y_1\given\x_n,...,\x_1}}{\pof{\Y_n,...,\Y_1\given\x_n,...,\x_1}}.
\end{align}
$$

# Parameter Priors vs Predictive Priors

What is the advantage of this all?

In Bayesian deep learning, we often use parameter priors that are not meaningful in the sense that they do not take parameter symmetries into account. For example, a unit Gaussian prior over the parameters of a neural network does not induce different predictions for different parameters necessarily. While this prior can be sensible from a parameter compression perspective (e.g.\ see Hinton and van Camp (1993)<d-cite key="hinton1993keeping"></d-cite>), this does not have to be the only consideration.

With function priors, that is predictive priors, we can induce more meaningful priors because we can focus on the predictions and ignore the parameters. This can connect Bayesian approaches to data augmentation and other regularization techniques.

Given that these priors are difficult to express explicitly though, using the DPI to obtain a functional ELBO can be an easier way to express and approximate them.

## Label Entropy Regularization

Applied to the functional ELBO, we can gain a new perspective on label entropy regularization.

The functional evidence bound can be lower-bounded using the chain rule by:

$$
\begin{align}
\E{\qof{\w}}{-\log \pof{\Dany \given \w}} + \Kale{\qof{\Y... \given \x...}}{\pof{\Y... \given \x...}} \\
\ge \E{\qof{\w}}{-\log \pof{\Dany \given \w}} + \E{\pdata{\x}}{\Kale{\qof{\Y \given \x}}{\pof{\Y \given \x}}},
\end{align}
$$

where we can expand the term under the second expectation to:

$$
\Kale{\qof{\Y \given \x}}{\pof{\Y \given \x}}=\CrossEntropy{\qof{\Y \given \x}}{\pof{\Y \given \x}} - \xHof{\qof{\Y \given \x}}.
$$

*Assuming that our prior yields a uniform distribution over the labels*, we can drop the cross entropy term because it is constant and obtain:

$$
\E{\qof{\w}}{-\log \pof{\Dany \given \w}} - \E{\pdata{\x}}{\xHof{\qof{\Y \given \x}}}.
$$

This is the same as an MLE minimization objective with an additional entropy regularization term $$-\xHof{\qof{\Y \given \x}}$$ for different $$\x$$ that prevents the model from overfitting to the labels and collapsing to the one-hot encoding of the labels.

Thus, in the simplest approximation, the DPI and functional variational inference give us a new perspective on label entropy regularization.

## Knowledge Distillation

Obviously, assuming non-uniform prior predictions, $$\E{\pdata{\x}}{\Kale{\qof{\Y \given \x}}{\pof{\Y \given \x}}}$$ can be related to knowledge distillation in deep neural networks as introduced by Hinton et al. (2015)<d-cite key="hinton2015distilling"></d-cite>.

The main difference is that knowledge distillation is using the reverse KL divergence instead of the forward KL divergence.

Only that we are not distilling the knowledge from a teacher model but from the prior that we downweigh while also training our model on the data itself.

# Conclusion

In summary, the data processing inequality provides an elegant perspective on why optimizing the variational posterior in function-space can be an effective inference strategy. 
This basic tool from information theory has helped us with a very simple deduction of a modern Bayesian deep learning method. 

While recovering the results in their full generality is beyond the scope of this post, we were able to derive several interesting results from the literature in simplified form (or at least provide strong intuitions and motivation for them). 

Further, we gained intuition for the relationship between parameter and predictive priors and how they relate to the DPI. Predictive priors can be seen as more meaningful priors for Bayesian neural networks because of their ability to ignore parameter symmetries.

Most importantly, we have looked at equivalence classes based on the predictions and pointed out how the *predictive* prior divergences approximate this functional ðŸ¥¬ divergence between equivalence classes and not the usual parameter-based ðŸ¥¬ divergences. 

Lastly, we have seen that the DPI can be used to derive a new perspective on label entropy regularization, which is a commonly used regularization technique in deep learning.
