---
layout: distill
title: How to compute Hessian-vector products?
description: The products between the Hessian of a function and a vector, so-called Hessian-vector product (HVPs) is a quantity that appears in optimization and machine learning. However, the computation of HVPs is often considered prohibitive, preventing practitioners from using algorithms that rely on these quantities. Standard automatic differentiation theory predicts that computing an HVP has a cost of the same order of magnitude as computing a gradient. The goal of this blog post is to provide a practical counterpart to this theoretical result, showing that modern automatic differentiation frameworks, Jax and Pytorch, allow for efficient computation of these HVPs in standard deep learning cost functions.
date: 2024-05-07
future: true
htmlwidgets: true

# Anonymize when submitting
# authors:
#   - name: Anonymous

authors:
  - name: Anonymous

# must be the exact same name as your blogpost
bibliography: 2024-05-07-bench-hvp.bib  

# Add a table of contents to your post.
#   - make sure that TOC names match the actual section names
#     for hyperlinks within the post to work correctly. 
#   - please use this format rather than manually creating a markdown table of contents.
toc:
  - name: What are HVPs and where are they useful?
  - subsections:
    - name: Newton-CG method
    - name: Bilevel optimization with approximate implicit differentiation
    - name: The HVPs for the study of the loss landscape
  - name: Naive computation of HVPs
  - name: A quick detour by automatic differentiation
  - subsections:
    - name: Computational graph
    - name: Forward mode
    - name: Reverse mode
  - name: HVPs with automatic differentiation
    subsections:
      - name: Forward-over-reverse
      - name: Reverse-over-reverse
      - name: Reverse-over-forward
      - name: Forward-over-forward
  - name: Benchmark with deep learning architectures
    subsections:
      - name: Time complexity
      - name: Memory complexity
      - name: What about PyTorch?

# Below is an example of injecting additional post-specific styles.
# This is used in the 'Layouts' section of this post.
# If you use this post as a template, delete this _styles block.
_styles: >
  .fake-img {
    background: #bbb;
    border: 1px solid rgba(0, 0, 0, 0.1);
    box-shadow: 0 0px 4px rgba(0, 0, 0, 0.1);
    margin-bottom: 12px;
  }
  .fake-img p {
    font-family: monospace;
    color: white;
    text-align: left;
    margin: 12px 0;
    text-align: center;
    font-size: 16px;
  }
  .framed {
    border: 1px var(--global-text-color) dashed !important;
    padding: 20px;
  }
  .marge {
    margin-left: 20px;
  }
---

Hessian-vector products (HVPs) are at the heart of many recent bilevel optimization algorithms <d-cite key=Arbel2022amigo></d-cite>
 
The geometry of a function is relevant information that influences the behavior of optimization algorithms or the generalization properties of deep neural networks. This information can be accessed through the second-order information of this function. To this end, computing Hessian-vector products (HVPs) can provide an interesting summary of local geometry. However, computing such quantity is often considered prohibitive by practitioners, discouraging them from using algorithms that rely on HVPs.

In this blog post, we recall why computing HVPs is useful in optimization and machine learning applications. Then, we present how to leverage automatic differentiation to compute HVPs efficiently. Finally, we benchmark the different methods to compute HVPs on several deep neural network architectures in terms of time and memory and compare the results to the complexity of the computation of a gradient.

## What are HVPs and where are they useful?

Consider a twice differentiable function $$f:\mathbb{R}^d\to\mathbb{R}$$. For a vector $$x\in\mathbb{R}^d$$, the Hessian matrix of $$f$$ at $$x$$ is the matrix of its second order partial derivatives

$$
  \nabla^2 f(x) = \left[\frac{\partial^2f}{\partial x_i\partial x_j}(x)\right]_{1\leq i,j\leq d}\in\mathbb{R}^{d\times d}\enspace.
$$

For a vector $$v\in\mathbb{R}^d$$, the Hessian-vector product (HVP) is simply the vector $$\nabla^2 f(x) v\in\mathbb{R}^d$$.

A first interpretation of the HVPs is that $$\nabla^2 f(x) v$$ is the directional derivative of the gradient of $$f$$ in the direction $$v$$ at $$x$$. Formally

$$
  \nabla^2f(x)v = \lim_{\epsilon\to 0} \frac1\epsilon[\nabla f(x+\epsilon v) - \nabla f(x)]\enspace.
$$

Let us review some use cases of HVPs in optimization and machine learning.

### Newton-CG method

A common use case of HVPs is the Newton-CG method. Assume one is interested in solving

$$
  \min_{x\in\mathbb{R}^d} f(x)
$$

for a twice differentiable function $$f:\mathbb{R}^d\to\mathbb{R}$$. This problem can be solved using Newton's method. This algorithm consists in making the variable $$x$$ move in the direction $$-[\nabla^2f(x)]^{-1}\nabla f(x)$$, leading to the following update rule

$$
  x_{k+1} = x_k -\eta_k [\nabla^2f(x_k)]^{-1}\nabla f(x_k)\enspace, 
$$

with $$\eta_k>0$$ a step-size.
One can notice that this method involves the resolution of the following linear system involving the Hessian matrix

\begin{equation}\label{eq:linear_system_newton}
  \nabla^2f(x_k)v = \nabla f(x_k)\enspace.
\end{equation}

Note that the function $$f$$ is often non-convex. In this case, the Hessian matrix is not positive definite. One way to circumvent this issue is to regularize the Hessian. Thus the linear system \eqref{eq:linear_system_newton} is replaced by

\begin{equation}\label{eq:linear_system_newton2}
  [\nabla^2f(x_k)\textcolor{green}{ + \lambda I}]v = \nabla f(x_k)\enspace.
\end{equation}

for some constant $$\lambda>0$$.

To solve the subproblem \eqref{eq:linear_system_newton2} efficiently, one may use the Conjugate Gradient method<d-cite key=Hestenes1952CG></d-cite><d-cite key=Nocedal2006></d-cite>.

<p class="framed">
  <b class="underline">Conjugate gradient to solve \eqref{eq:linear_system_newton2}</b><br>
  <b>Input</b> Initialization \(v_0\)<br>

  <b>Initialization</b>
  $$
    r_0 = (\nabla^2f(x_0) \textcolor{green}{+ \lambda I}) v_0 - \nabla f(x_k),\quad p_0 = -r_0,\quad t = 0
  $$

  <b>While</b> \(r_t \neq 0\)
  \begin{align*}
    \alpha_t  &=\frac{r_t^\top r_t}{p_t^\top (\textcolor{purple}{\nabla^2f(x_t) p_t}\textcolor{green}{+ \lambda p_t})} \\
    v_{t+1} &=v_t + \alpha_t p_t \\
    r_{t+1} &=r_t  + \alpha_t(\textcolor{purple}{\nabla^2f(x_t) p_t}\textcolor{green}{+ \lambda p_t}) \\
    \beta_{t+1} &=\frac{r_{t+1}^\top r_{t+1}}{r_t^\top r_t} \\
    p_{t+1} &=-r_{t+1} + \beta_{t+1} p_t\\
    t &=t + 1
  \end{align*}
</p>

This algorithm finds the solution of \eqref{eq:linear_system_newton2} after at most $$d$$ iterations<d-cite key="Nocedal2006"></d-cite>. In practice, one can stop the algorithm earlier if the approximate solution is accurate enough. This is the truncated Newton algorithm<d-cite key=Dembo1983></d-cite>.

We observe that each iteration involves the HVP $$\textcolor{purple}{\nabla^2f(x_t) p_t}$$. Thus, an efficient implementation of the HVP is crucial for the overall algorithm performance. 

### Bilevel optimization with approximate implicit differentiation

Bilevel optimization problems are problems that are gaining more and more popularity in the machine learning community thanks to their wide range of applications such as hyperparameter selection<d-cite key=Lorraine2020MillionsofHyperparameters></d-cite>, meta-learning<d-cite key=Rajeswaran2019imaml></d-cite> or neural architecture search<d-cite key=Liu2019darts></d-cite>. In bilevel optimization, one wants to solve the following problem
\begin{equation}\label{eq:bilevel_pb}
  \min_{x\in\mathbb{R}^d} h(x) = F(x, y^* (x))\quad\text{with}\quad y^*(x) = \arg\min_{y\in\mathbb{R}^p} G(x, y)\enspace.
\end{equation}
for two functions $$F$$ and $$G$$ defined from $$\mathbb{R}^d\times\mathbb{R}^p$$. When $$F$$ is differentiable and $$G$$ is twice differentiable and strongly convex in $$y$$, one can derive from the implicit function theorem an expression for the gradient of $$h$$. Indeed, the chain rule applied to $$F$$ gives us

$$
  \nabla h(x) = \nabla _x F(x, y^*(x)) + [Dy^*(x)]^\top \nabla_y G(x, y^*(x))\enspace.
$$

where $$Dy^*(x) = \left[\frac{\partial y^*_i}{\partial x_j}(x)\right]_{\substack{1\leq i\leq p\\ 1 \leq j\leq d}}\in\mathbb{R}^{p\times d}$$ is the Jacobian matrix of $$y^*$$. To get the expression of this Jacobian, one can leverage the first order optimality condition that $$y^*(x)$$ must satisfy for any $$x\in\mathbb{R}^d$$

$$
\nabla_y G(x, y^*(x)) = 0\enspace.
$$

The implicit function theorem enables us to differentiate this equation with respect to $$x$$. This yields

$$
  Dy^*(x) = - [\nabla^2_{yy} G(x, y^*(x))]^{-1} \nabla_{yx}^2 G(x, y^*(x))\enspace.
$$

As a consequence, the gradient of $h$ can be written as

\begin{equation}\label{eq:grad_h}
  \nabla h(x) = \nabla_x F(x, y^* (x)) + \nabla^2_{xy} G(x, y^* (x))v^*(x)
\end{equation}
where $$v^*(x)$$ is the solution of the following linear system

$$
  \nabla^2_{yy}G(x, y^*(x))v = \nabla_y F(x, y^*(x))\enspace.
$$

Approximate Implicit Differentiation-based (AID) algorithms are "gradient descent-like" algorithms that replace in the expression oh the hypergradient \eqref{eq:grad_h} $$y^*(x)$$ and $$v^*(x)$$ by some approximations $$y$$ and $$v$$<d-cite key="Pedregosa2016"></d-cite><d-cite key="Ghadimi2018"></d-cite><d-cite key="Arbel2022amigo"></d-cite><d-cite key="Ji2021stocbio"></d-cite>. Let us denote the approximate hypergradient

$$
\bar\nabla h(x_t;y_{t+1},v_{t+1}) = \nabla_x F(x_t, y_{t+1}) + \nabla^2_{xy} G(x_t, y_{t+1})v_{t+1}\enspace.
$$

AID-based algorithms take the following form

<p class="framed">
  <b class="underline">AID-based algorithm to solve \eqref{eq:bilevel_pb}</b><br>
  <!-- <b>Input</b>: starting guess \(\xx_0\), step-size \(\step > 0\) and momentum
    parameter \(\mom \in (0, 1)\).<br>
  \(\xx_1 = \xx_0 - \dfrac{\step}{\mom+1} \nabla f(\xx_0)\) <br> -->
  <b>Input</b> Initializations \(y_0\), \(v_0\) and \(x_0\)<br>

  <b>For</b> \(t = 0,\dots, T-1\):<br>

    <span class="marge">Take for \(y_{t+1}\) an approximation of \(y^*(x_{t+1})\)<br></span>

    <span class="marge">Take for \(v_{t+1}\) an approximation of the solution of the linear system</span>
    
    \begin{equation}\label{eq:approx_linear_syst}
      \nabla^2_{yy}G(x_{t}, y_{t+1})v = \nabla_y F(x_{t}, y_{t+1})
    \end{equation}

    <span class="marge">Update \(x\)</span>
    
    $$
      x_{t+1} = x_t - \gamma_t \bar\nabla h(x_t;y_{t+1},v_{t+1})
    $$
</p>


As for Newton's method, AID involves an inverse Hessian-vector product. In the deterministic setting, one can use several CG steps<d-cite key=Pedregosa2016></d-cite> to get an approximate solution.

In many machine learning scenarii, $$F$$ and $$G$$ are expectations:

$$
  F(x,y) = \mathbb{E}_\xi[F(x,y;\xi)]\quad G(x,y) = \mathbb{E}_\zeta[G(x,y;\zeta)]
$$

Thus, it is more efficient to use stochastic solvers to solve \eqref{eq:approx_linear_syst}. There are mainly two approaches in the stochastic bilevel literature to solve \eqref{eq:approx_linear_syst} in a stochastic fashion. The first is the Neumann iterations. It leverages the following identity for a small enough parameter $$\eta>0$$

$$
  [\nabla^2_{yy} G(x,y)]^{-1} = \eta\sum_{k=0}^{+\infty}[I - \eta\nabla^2_{yy} G(x, y)]^k
$$

There are several variants of Neumann iterations<d-cite key=Ghadimi2018></d-cite><d-cite key=Ji2021stocbio></d-cite>. One of them<d-cite key=Ji2021stocbio></d-cite> is to take 

$$
  v_{t+1} = \sum_{q=0}^Q \prod_{k=0}^q(I - \nabla^2 G(x_t, y_{t+1};\zeta_k))\nabla_y F(x_t, y_{t+1};\xi)
$$

for independently sampled $$\zeta_0,\dots,\zeta_K$$ and $$\xi$$.


The second way to get an approximation of the solution of \eqref{eq:approx_linear_syst} is to make stochastic gradient steps (or variant)<d-cite key="Arbel2022amigo"></d-cite><d-cite key=Dagreou2022SABA></d-cite><d-cite key=Dagreou2023SRBA></d-cite> in the quadratic form 

$$
  v\mapsto \frac12\langle\nabla^2_{yy}G(x_t, y_{t+1})v, v\rangle + \langle\nabla_y F(x_t, y_{t+1}), v\rangle\enspace
$$

Since the gradient of this quadratic form is simply given by

$$
  \textcolor{purple}{\nabla^2_{yy}G(x_t,y_{t+1})v} - \nabla_y F(x_t, y_{t+1})
$$

this method also requires HVP computations.




### The HVPs for the study of the loss landscape

The study of the geometry of neural networks is an active field that aims at understanding the links between training dynamics, local geometry of the training loss and generalization. One way to study the local geometry of a neural network is to find the distribution of the eigenvalues of its Hessian matrix. Indeed, depending on the sign of the eigenvalues of the Hessian, one can for instance distinguish local minima, local maxima and saddle points. 

{% include figure.html path="assets/img/2024-05-07-bench-hvp/hess_eig.png" class="img-fluid" %}

In several papers<d-cite key=Ghorbani2019></d-cite><d-cite key=Dauphin2014></d-cite><d-cite key=Sagun2018></d-cite><d-cite key=Foret2021SAM></d-cite>, the Hessian spectrum is computed thanks to the Lanczos algorithm<d-cite key=Lanczos1950></d-cite>. This algorithm is a modification of the power method where each new iterate is taken in the orthogonal complement of the previous iterates.

<p class="framed">
  <b class="underline">Lanczos' algorithm</b><br>

  <b>Input</b> Initial vector \(v_0\).<br>
  <b>Initialization</b>
  $$
    w'_0 = \textcolor{purple}{\nabla^2f(x)v_0},\quad \alpha_0 = w_0'^\top v_0,\quad w_0 = w_0' - \alpha_0 v_0
  $$

  <b>For</b> \(i = 1,\dots, k-1\):<br>

  \begin{align*}
    \beta_i &= \|w_{i-1}\|\\
    v_{i} &= \frac{w_{i-1}}{\beta_{i}}\\
    w_i' &= \textcolor{purple}{\nabla^2f(x)v_i}\\
    \alpha_i &= w_i'^\top v_i\\
    w_i &= w_i' - \alpha_i v_i - \beta_iv_{i-1}
  \end{align*}
</p>

We observe once again that the Hessian information is accessed through HVPs rather than the full Hessian matrix itself.

We now turn to the practical computation of HVPs.

## Naive computation of HVPs
Since we are interested in computing $$\nabla^2 f(x)v$$, the simplest way to do it is to compute the Hessian matrix and then multiply it by the vector $$v$$.

```python
def hvp_naive(f, x, v):
  return jax.hessian(f) @ v
```
However, this method is quite cumbersome. The computation and the storage of the full Hessian matrix have $$\mathcal{O}(d^2)$$ complexities where $$d$$ is the input dimension.


## A quick detour by automatic differentiation

An important tool that enables the computation of HVPs without computing the Hessian matrix is automatic differentiation. It allows to compute automatically the derivatives of differentiable functions. Automatic differentiation is mostly known for the successful application of backpropagation (aka reverse-mode automatic differentiation) in deep learning<d-cite key="Rumelhart1986"></d-cite>. However, there exists a second mode of automatic differentiation which is the forward mode. In what follows, we provide a brief explanation of both modes. For a more detailed explanation, we refer the reader to the survey by Baydin et al.<d-cite key="Baydin2018"></d-cite>.
In what follows, we consider a differentiable function $$f:\mathbb{R}^d\to\mathbb{R}^p$$. For any $$x\in\mathbb{R}^d$$, we also consider $$f(x) = (f_1(x),\dots,f_p(x))^\top\in\mathbb{R}^p$$. The Jacobian matrix of $$f$$ at $$x$$ is the matrix of its partial derivatives

$$
  Df(x) = \left[\frac{\partial f_i}{\partial x_j}(x)\right]_{\substack{1\leq i\leq p\\ 1 \leq j\leq d}} = \begin{bmatrix} \nabla f_1(x)^\top \\ \vdots \\ \nabla f_p(x)^\top\end{bmatrix}\in\mathbb{R}^{p\times d}\enspace.
$$

### Computational graph

A key ingredient of automatic differentiation is the computational graph. It is a directed acyclic graph that represents the succession of elementary operations required the evaluate a function. For instance, the computational graph of the function defined by $$f(x) = e^{x_1^2} + \sin(x_2)$$ is the following.

{% include figure.html path="assets/img/2024-05-07-bench-hvp/computational_graph.png" class="img-fluid" %}

The vertices $$o_i$$ represent the intermediate states of the evalutation of $$f(x)$$


### Forward mode

The chain rule enables us to leverage the computational to evaluate efficiently the derivatives of a function. Let us assume that the output $$f_i(x)$$ corresponds to the vertex $o_K$ and the input $$x_j$$ corresponds to the vertex $$o_j$$ in the computational graph of $$f$$. Then, the chain rule gives us

\begin{equation}\label{eq:chain_rule_forward}
  \frac{\partial f_i}{\partial x_j}(x) = \frac{\partial o_K}{\partial o_j}=
  \sum_{k\in\text{parents}(K)} \frac{\partial o_K}{\partial o_k}\frac{\partial o_k}{\partial o_j}\enspace.
\end{equation}

For any $$k\in\text{parents}(K)$$, the quantity $$\frac{\partial o_K}{\partial o_k}$$ is easy to compute since $$o_K$$ and $$o_k$$ are connected by an elementary operation we know how to differentiate. For the quantity $$\frac{\partial o_k}{\partial o_j}$$, we can compute it recursively thanks to \eqref{eq:chain_rule_forward} during a forward pass in the computational graph and using the fact that $$\frac{\partial o_j}{\partial o_j} = 1$$. Note that for $$v\in\mathbb{R}^d$$, by initializing by $$\frac{\partial o_j}{\partial o_j} = v_j$$ for $$j\in\{1,\dots,d\}$$, the forward differentiation actually returns the Jacobian-vector product (JVP) of $$f$$ at $$x$$ in the direction $$v$$., that is

$$
  Df(x)v = \begin{bmatrix} \nabla f_1(x)^\top v \\ \dots \\ \nabla f_p(x)^\top v\end{bmatrix}^\top\in\mathbb{R}^p\enspace.
$$

In terms of complexity, the forward automatic differentiation has the same complexity as the evaluation of $$f$$ since it does not require supplementary passes through its computational graph.

In Jax, the code to get the JVP of $$f$$ at $$x$$ in the direction $$v$$ is the following

```python
def jvp(f, x, v):
  return jax.jvp(f, (x, ), (v, ))[1]
```

To understand properly what is happening when using forward differentiation, let us consider for fixed $$x\in\mathbb{R}^2$$ the function $$f:\mathbb{R}^2\to\mathbb{R}$$ defined on $$\mathbb{R}^2\times\mathbb{R}^{2\times 2}$$ by

\begin{equation}\label{eq:mlp}
  f(U, W) = \frac12(UW^\top x)^2\enspace.
\end{equation}

If we code ourselves the forward differentiation to get the JVP, we get the following code

```python
def jvp(U, W, vu, vw):
    # Forward diff of f
    z1 = W @ x
    vz1 = vw @ x

    z2 = U @ z1
    vz2 = U @ vz1 + vu @ z1

    output = vz2 @ z2

    return output
```

The observation we can make is that the time complexity of forward differentiation is roughly twice the time complexity of the evaluation of $$f$$, due to the computations of the dual numbers `vz1` and `vz2`.

### Reverse mode
The reverse mode is the most known mode of automatic differentiation. Let us assume that the output $$f_i(x)$$ corresponds to the vertex $o_K$ and the input $$x_j$$ corresponds to the vertex $$o_j$$ in the computational graph of $$f$$. Then, the chain rule gives us

\begin{equation}\label{eq:chain_rule_backward}
  \frac{\partial f_i}{\partial x_j}(x) = \frac{\partial o_K}{\partial o_j}=
  \sum_{k\in\text{children}(j)} \frac{\partial o_K}{\partial o_k}\frac{\partial o_k}{\partial o_j}\enspace.
\end{equation}

For $$k\in\text{children}(j)$$, the quantity $$\frac{\partial o_k}{\partial o_j}$$ is easy to compute since the vertices $$o_k$$ and $$o_j$$ are linked by an elementary operation we know how to differentiate. To get $$\frac{\partial o_K}{\partial o_k}$$, one can apply the formula \eqref{eq:chain_rule_backward} recursively during a backward pass in the computational graph with the initialization $$\frac{\partial o_K}{\partial o_K} = 1$$. Note that if $$v\in\mathbb{R}^p$$, if we initialize the backward pass with $$\frac{\partial o_K}{\partial o_K} = v_i$$ (recall that $$o_K = f_i(x)$$), then the reverse automatic differentiation returns the vector-Jacobian product (VJP) of $$f$$ at $$x$$ in the direction $$v$$, that is

$$
  v^\top Df(x) = \sum_{i=1}^p v_i\nabla f_i(x)\enspace.
$$

If $$f$$ is real-valued (*i.e.* $$p = 1 $$), one can observe that its gradient is simply the VJPs of $$f$$ with the one-dimensional vector $$1$$.

At the end of the day, the reverse automatic differentiation requires a forward pass in the computational graph to get the intermediate states $$o_i$$ and a backward pass to propagate the intermediate partial derivatives. Hence, the complexity of the reverse automatic differentiation is twice the complexity of the evaluation of $$f$$. Moreover, it suffers from a memory burden because of the storing of the intermediate states $$o_i$$. 

In Jax, we can get the VJP of $$f$$ at $$x$$ in the direction $$v$$ by the following code

```python
def vjp(f, x, v):
  return jax.vjp(f, x)[1](v)
```

Let us observe what happens if we code manually the backpropagation to get the gradient of the previous function $$f$$ defined by $$f(U, W) = \frac12(UW^\top x)^2$$.

``` python
def gradient(U, W):
    ## Forward pass
    z1 = W @ x
    z2 = U @ z1
    output = .5 * jnp.linalg.norm(z2)**2

    ## Reverse pass
    dz2 = z2
    dU = jnp.outer(dz2, z1)
    dz1 = U.T @ dz2
    dW = jnp.outer(dz1, x)
    
    return dU, dW
```
## HVPs with automatic differentiation
Thanks to automatic differentiation, it is possible to compute HVPs without computing and storing the full Hessian matrix. The [JAX documentation](https://jax.readthedocs.io/en/latest/notebooks/autodiff_cookbook.html) describes three ways to do it.

### Forward-over-reverse
The HVPs have another interpretation coming from differential calculus. They are the directional derivatives of the gradient of $$f$$ in the direction $v$<d-cite key="Pearlmutter1994"></d-cite>.

$$
\nabla^2f(x) v = \lim_{\epsilon\to 0} \frac1\epsilon[\nabla f(x+\epsilon v)-\nabla f(x)]
$$

Now let us take a step back. If we have a differentiable vector field $$\phi:\mathbb{R}^p\to\mathbb{R}^p$$, the directional derivative of $$\phi$$ at $$x$$ in the direction $$v$$ is exactly the product between its Jacobian matrix at $$x$$ $$D\phi(x)$$ and the vector $$v$$. This Jacobian-Vector product (JVP) can be efficiently computed by automatic differentiation in forward mode<d-cite key="Baydin2018"></d-cite>. In this case, its computational complexity is twice the complexity of the evaluation of the vector field $$\phi$$. This operation is implemented in automatic differentiation frameworks like Jax<d-cite key="jax2018github"></d-cite> or PyTorch<d-cite key="Paszke2019"></d-cite>.

Hence, to get the HVP, one can apply the previous principle by tacking $$\phi = \nabla f$$. The implementation in Jax is taking only two lines of code. 

```python
def hvp_forward_over_reverse(f, x, v):
  return jax.jvp(jax.grad(f), (x, ), (v, ))[1]
```
In this case, `jax.grad(f)(x)` is computed by backward automatic differentiation, whose complexity is two times the complexity of evaluating $$f$$. Thus, the temporal complexity of `hvp_forward_over_reverse` is roughly four times the complexity of the evaluation of $$f$$.

To better see what happens, let us consider again our function $$f$$ defined by \eqref{eq:mlp}. The Python code of the `forward-over-reverse` HVP is the following.

```python
def forward_over_reverse(U, W, vu, vw):
    # Forward through f
    z1 = W @ x
    vz1 = vw @ x

    z2 = U @ z1
    vz2 = U @ vz1 + vu @ z1

    z3 = z2  # dz2
    vz3 = vz2

    # Backward through f
    z4 = jnp.outer(z3, z1)  # dU
    vz4 = jnp.outer(vz3, z1) + jnp.outer(z3, vz1) 

    z5 = U.T @ z3  # dz1
    vz5 = U.T @ vz3 + vu.T @ z3

    z6 = jnp.outer(z5, x)  # dW
    vz6 = jnp.outer(vz5, x)

    return vz4, vz6
  ```



### Reverse-over-reverse
The second way is the reverse-over-reverse mode. It leverages the fact that the HVP is also the gradient of the scalar product between the gradient of $$f$$ and $$v$$.

$$
\nabla^2f(x)v = \nabla[\langle\nabla f(.),v\rangle](x)
$$

Therefore, these lines of JAX implement the HVP
```python
def hvp_reverse_over_reverse(f, x, v):
  return jax.grad(lambda x: jnp.vdot(jax.grad(f)(x), v))
```
Since the gradients are computed by backpropagation, the complexity of `hvp_reverse_over_reverse` is twice the complexity of `jax.grad(f)`, which is roughly four times the complexity of the evaluation of $$f$$.

Writting down the code of the reverse-over-reverse HVP for our function $$f$$ defined by \eqref{eq:mlp} makes us well understand the differences between this mode and the `forward-over-reverse` mode. Particularly, one can notice that there are twice as many elementary operations in the `reverse-over-reverse` mode than in the `forward-over-reverse` mode. Moreover, in terms of memory footprint, the `reverse-over-reverse` more requires storing the values of the vertices of the computational graph of the gradient of $$f$$, while the `forward-over-reverse` only needs to store the values of the vertices of the computational graph of $$f$$. Thus, the former is less efficient than the latter.

```python
def reverse_over_reverse(U, W, vu, vw):
    # Forward through grad(f)
    ## Forward through f
    z1 = W @ x
    z2 = U @ z1
    output = .5 * jnp.linalg.norm(z2)**2

    ## Reverse through f
    ### output = .5 * jnp.linalg.norm(z2)**2
    z3 = z2  # dz2

    ### z2 = U @ z1
    z4 = jnp.outer(z3, z1) # dU
    z5 = U.T @ z3 # dz1
    z6 = jnp.outer(z5, x) # dW

    # Dot product <grad(f), v>
    output = jnp.sum(vu * z4) + jnp.sum(vw * z6)

    # Backward through <grad(f),v>
    ## output = jnp.sum(vu * z4) + jnp.sum(vw * z6)
    dz6 = vw
    dz4 = vu

    ## z6 = jnp.outer(z5, x)
    dz5 = dz6 @ x

    ## z5 = U.T @ z3
    dz3 = U @ dz5
    dU = jnp.outer(z3, dz5)

    ## z4 = jnp.outer(z3, z1)
    dz3 += dz4 @ z1
    dz1 = dz4.T @ z3

    ## z3 = z2
    dz2 = dz3

    ## z2 = U @ z1
    dz1 += dz2 * U
    dU += jnp.outer(dz2, z1)

    ## z1 = W @ x
    dW = jnp.outer(dz1, x)

    return dU, dW
  ```

### Reverse-over-forward
Actually, the quantity $$\langle \nabla f(x),v\rangle$$ is also the product between the Jacobian matrix of $$f$$ and the vector $$v$$. Therefore, it can be computed using forward automatic differentiation.

```python
def hvp_reverse_over_forward(f, x, v):
  jvp_fun = lambda x: jax.jvp(f, (x, ), (v, ))[1]
  return jax.grad(jvp_fun)(x)
```

This method a more efficient than the previous one. Indeed, since we backpropagate only once, the memory burden is lower than for `hvp_reverse_over_reverse`. Moreover, the overall time complexity is roughly half the complexity of `hvp_reverse_over_reverse`.

```python
def reverse_over_forward(U, W, vu, vw):
    # Forward diff of f to get jvp(f, (U, W), (vu, vw))
    z1 = W @ x
    vz1 = vw @ x

    z2 = U @ z1
    vz2 = U @ vz1 + vu @ z1

    output = vz2 @ z2

    # Backward pass through jvp(f, (U, W), (vu, vw))
    ## output = vz2 @ z2
    dz2 = vz2
    dvz2 = z2

    ## vz2 = U @ vz1 + vu @ z1
    dz1 = vu.T @ dvz2
    dvu = jnp.outer(dvz2, z1) 
    dvz1 = U.T @ dvz2
    dU = jnp.outer(dvz2, vz1)

    ## z2 = U @ z1
    dz1 += U.T @ dz2
    dU += jnp.outer(dz2, z1)

    ## vz1 = vw @ x
    dvw = jnp.outer(dvz1, x)
    
    ## z1 = W @ x
    dW = jnp.outer(dz1, x)

    return dU, dW
```

### Forward-over-forward
Is it possible to compute HVPs in a forward-over-forward fashion? In other words, can we get an HVP by computing the JVP of $$\langle \nabla f(.), v\rangle$$ and a suitable vector $$d\in\mathbb{R}^p$$?
Recall that the JVP of a vector field and a tangent vector $$d$$ is its directional derivative in the direction $$d$$.
By a simple Taylor expansion and using the fact that the Hessian matrix is symmetric, we have

$$
   \langle \nabla f(x+d), v\rangle \approx \langle \nabla f(x), v\rangle + \langle \nabla^2 f(x)v, d\rangle\enspace.
$$

Hence, to get all the coordinate of $$\nabla^2f(x) v$$, one needs to compute all de JVPs between $$\langle \nabla f(.), v\rangle$$ and the vectors of the canonical basis of $$\mathbb{R}^p$$ $$e_1,\dots,e_p$$. As a consequence, forward-over-forward mode requires $p$ evaluations of $$\langle \nabla f(x), v\rangle$$. Hence, it has a complexity $$p$$ times larger than the complexity of evaluating $$f$$.


## Benchmark with deep learning architectures

We make a benchmark of the computation of HVPs with different deep-learning architectures. We consider the [ResNet50](https://huggingface.co/docs/transformers/model_doc/resnet)<d-cite key="He2015resnet"></d-cite> (25,610,152 parameters), a [ViT-base](https://huggingface.co/docs/transformers/model_doc/vit) <d-cite key="Dosovitskiy2021"></d-cite> (86,567,656 parameters) and a [Bert-base](https://huggingface.co/docs/transformers/model_doc/bert#transformers.FlaxBertForTokenClassification) <d-cite key="Devlin2019"></d-cite> (109,483,778 parameters) architectures. We use the Flax implementation of these architectures available in the [transformers package](https://huggingface.co/docs/transformers/) by [Hugging Face 🤗](https://huggingface.co).

Except for the comparison between Jax and PyTorch, the computations are made with Jax. Note that to get the most efficient computations, we use the just-in-time compilation provided by the Jax package. 

The benchmark has been run on GPUs Nvidia A100. Each HVP/gradient computation is done on a single GPU.


### Time complexity

The first comparison we make is a comparison in terms of wall-clock time between the different ways to compute HVPs and also the computation of a gradient by backpropagation. For each architecture, we compute the gradient of the model with respect to the parameters by backpropagation. We also compute the HVPs in `forward-over-reverse`, `reverse-over-forward` and `reverse-over-reverse` modes. For each computation, we measure the time taken, and for the HVP, we subtract the time taken by a gradient computation, to get only the time of the overhead required by the HVP computation.
Each computation is jit-compiled and is run 100 times. We plot on the left of the figure, the median computation time and also the 20% and 80% percentile. The computations are done with a batch size of 64. The first observation we make is that, in practice, an HVP computation takes between twice and three times the time taken by a gradient computation for the three architectures.  Also, one can notice that the most efficient way to compute HVPs is the `forward-over-reverse` mode for the ResNet50 and BERT architectures and the `reverse-over-forward` for the ViT architecture. We also observe that in the case of the Resnet50, the `reverse-over-reverse` is much longer than the other modes. This is not the case for the other architectures. **<span style="color:red;"> This has to be clarified.</span>**

We also report on the right figure the computational time of the `forward-over-reverse` method with respect to the batch size. We observe that the computational time scales linearly with the batch size.

{% include figure.html path="assets/img/2024-05-07-bench-hvp/bench_hvp_time.png" class="img-fluid" %}


### Memory complexity

We also compare the memory footprint of each approach. On the left of the following figure, we have the memory footprint of an HVP computation with a batch size of 64 for each architecture. We observe that for each architecture, the memory cost of the HVP is roughly twice the memory cost of the gradient. Note that the memory footprint of the BERT architecture is less important than the one of the two others. This is due to the convolutions that include the ResNet50 and ViT architectures. Indeed, a single convolution itself represents a quite small number of parameters (depending on the size of the kernel) but its output is high dimensional resulting in a high number of activations that are stored. 

On the right, we compare the evolution of the memory cost of the `forward-over-reverse` method with respect to the batch size. As expected, it scales linearly with the batch size.


{% include figure.html path="assets/img/2024-05-07-bench-hvp/bench_hvp_memory_jax.png" class="img-fluid" %}

### What about PyTorch?

It is also possible to compute HVPs with PyTorch. One possibility is to use the Jax-like API [`torch.func`](https://pytorch.org/docs/stable/func.html) and code the HVP in the same way as in Jax.