---
layout: distill
title: How to compute Hessian-vector products?
description: The product between the Hessian of a function and a vector, so-called the Hessian-vector product (HVP), is a fundamental quantity to study the variation of a function. It is ubiquitous in traditional optimization and machine learning. However, the computation of HVPs is often considered prohibitive in the context of deep learning, driving practitioners to use proxy quantities to evaluate the loss geometry. Standard automatic differentiation theory predicts that the computational complexity of an HVP is of the same order of magnitude as the complexity of computing a gradient. The goal of this blog post is to provide a practical counterpart to this theoretical result, showing that modern automatic differentiation frameworks, `JAX `and `PyTorch``, allow for efficient computation of these HVPs in standard deep learning cost functions.
date: 2024-05-07
future: true
htmlwidgets: true

# Anonymize when submitting
authors:
  - name: Anonymous

# must be the exact same name as your blogpost
bibliography: 2024-05-07-bench-hvp.bib  

# Add a table of contents to your post.
#   - make sure that TOC names match the actual section names
#     for hyperlinks within the post to work correctly. 
#   - please use this format rather than manually creating a markdown table of contents.
toc:
  - name: What are HVPs and where are they useful?
  - subsections:
    - name: Inverse Hessian-vector products (iHVPs) in optimization
    - name: HVPs for the study of the loss landscape

  - name: A quick detour by automatic differentiation
  - subsections:
    - name: Computational graph
    - name: Forward mode
    - name: Reverse mode
  - name: Naive computation of HVPs
  - name: HVPs with automatic differentiation
    subsections:
      - name: Forward-over-reverse
      - name: Reverse-over-reverse
      - name: Reverse-over-forward
  - name: Benchmark with deep learning architectures
    subsections:
      - name: Time complexity
      - name: Memory complexity

# Below is an example of injecting additional post-specific styles.enb
# This is used in the 'Layouts' section of this post.
# If you use this post as a template, delete this _styles block.
_styles: >
  .framed {
    border: 1px var(--global-text-color) dashed !important;
    padding: 20px;
  }
  .marge {
    margin-left: 20px;
  }
---

Hessian-vector products (HVPs) play a central role in the study and the use of the geometric property of the loss function of deep neural networks<d-cite key=Foret2021SAM></d-cite>, as well as in many recent bilevel optimizers<d-cite key=Arbel2022amigo></d-cite>.
However, computing such quantity is often considered prohibitive by practitioners, discouraging them from using algorithms that rely on HVPs.

With this blog post, we aim to convince the practitioners that with modern automatic differentiation (AD) frameworks such as `JAX` or `PyTorch`, HVPs can be efficiently evaluated. Indeed, standard AD theory predicts that the computational cost of an HVP is of the same order as the cost of computing a gradient. After a brief introduction on why HVPs are useful for optimization and ML applications and on the basis of AD, we explain in detail the AD-based methods to compute an HVP and the reason for their efficiency. We then compare the different methods to compute HVPs for several deep neural network architectures in terms of time and memory for both `JAX` and `PyTorch`. Our results illustrate the complexity predicted by the theory, showing that computing an HVP is not much more expensive than computing a gradient. This opens an avenue to develop efficient second-order informed methods for neural networks.

## What are HVPs and where are they useful?

Let us first introduce the notion of Hessian and HVP. We will consider in this post a twice differentiable function $$f:\mathbb{R}^d\to\mathbb{R}$$ that goes from a vector $$x$$ in space $$\mathbb{R}^d$$ to a real number in $$\mathbb{R}$$. This typically corresponds to a function that maps the value of the parameters $$x$$ of a neural network to the loss $$f(x)$$.
For such a function, standard AD can be used to efficiently compute the gradient of the loss $$\nabla f(x) = \left[ \frac{\partial f}{\partial x_i}(x)\right]_{1\le i \le d} \in \mathbb{R}^d$$, using the so-called backpropagation.
The Hessian matrix of $$f$$ at $$x$$ is the matrix of its second-order partial derivatives

$$
  \nabla^2 f(x) = \left[\frac{\partial^2f}{\partial x_i\partial x_j}(x)\right]_{1\leq i,j\leq d}\in\mathbb{R}^{d\times d}\enspace.
$$

This matrix corresponds to the derivative of the gradient and captures how the gradient will change when moving $$x$$. To evaluate the variation of the gradient when moving $$x$$ in the direction $$v\in\mathbb{R}^d$$, one can compute the quantity $$\nabla^2 f(x) v\in\mathbb{R}^d$$. This is the so-called Hessian-vector product (HVP).

Let us review some use cases of HVPs in optimization and machine learning.

### Inverse Hessian-vector products (iHVPs) in optimization
When trying to find the minimum of the function $$f$$, methods that account for the second-order information often rely on the product between the inverse Hessian and a vector to find a good update direction.
For instance, Newton's method relies on update rules of the form

$$
  x_{k+1} = x_k - \eta_k[\nabla^2f(x_k)]^{-1}\nabla f(x_k)
$$

for some step-size $$\eta_k>0$$.

When evaluating the term $$[\nabla^2f(x_k)]^{-1}\nabla f(x_k)$$, it would be very inefficient to first compute the full Hessian matrix $$\nabla^2f(x_k)$$, then invert it and finally multiply this with the gradient $$\nabla f(x_k)$$.
Instead, one computes the inverse Hessian-Vector Product (iHPV) by solving the following linear system

\begin{equation}\label{eq:linear_system}
  \nabla^2f(x)v = b\enspace.
\end{equation}

with $$b = \nabla f(x_k)$$.
This approach is much more efficient as it avoids computing and storing the full Hessian matrix, and only computes the inverse of the matrix in the direction $$v$$.

A second use case for the iHVP in optimization is with bilevel optimization. In bilevel optimization, one wants to solve the following problem

\begin{equation}\label{eq:bilevel_pb}
  \min_{x\in\mathbb{R}^d} h(x) = F(x, y^* (x))\quad\text{with}\quad y^*(x) = \arg\min_{y\in\mathbb{R}^p} G(x, y)\enspace.
\end{equation}

The gradient of the function $$h$$ can be computed using the implicit function theorem, giving the following expression

$$
  \nabla h(x) = \nabla_x F(x, y^* (x)) - \nabla_{xy}G(x, y^*(x))[\nabla_{yy}G(x, y^*(x))]^{-1}\nabla_y G(x, y^*(x))\enspace.
$$

Here, the term $$\nabla^2_{yy} G(x, y)$$ is the Hessian of the function $$G$$ relatively to $$y$$. Thus, this quantity also requires computing an iHVP.

To compute the iHVP, many methods proposed in the literature rely on HVPs to solve \eqref{eq:linear_system} without ever accessing $$\nabla^2 f$$, like Neumann iterates or the Conjugate Gradient method. These methods are based on the computation of HVPs, as illustrated with the highlighted terms in the Conjugate Gradient method<d-cite key=Hestenes1952CG></d-cite><d-cite key=Nocedal2006></d-cite>.

<p class="framed">
  <b class="underline">Conjugate gradient to solve \eqref{eq:linear_system}</b><br>
  <b>Input</b> Initialization \(v_0\)<br>

  <b>Initialization</b>
  $$
    r_0 = \textcolor{orange}{\nabla^2f(x) v_0} - b,\quad p_0 = -r_0,\quad t = 0
  $$

  <b>While</b> \(r_t \neq 0\)
  \begin{align*}
    \alpha_t  &=\frac{r_t^\top r_t}{p_t^\top \textcolor{orange}{\nabla^2f(x) p_t}} \\
    v_{t+1} &=v_t + \alpha_t p_t \\
    r_{t+1} &=r_t  + \alpha_t\textcolor{orange}{\nabla^2f(x) p_t} \\
    \beta_{t+1} &=\frac{r_{t+1}^\top r_{t+1}}{r_t^\top r_t} \\
    p_{t+1} &=-r_{t+1} + \beta_{t+1} p_t\\
    t &=t + 1
  \end{align*}
</p>


In the bilevel literature, other strategies are employed. Some authors propose to use Neumann iterations<d-cite key="Ghadimi2018"></d-cite><d-cite key="Ji2021stocbio"></d-cite> to solve \eqref{eq:linear_system}. Others propose to use gradient descent steps or variant<d-cite key="Arbel2022amigo"></d-cite><d-cite key="Dagreou2022SABA"></d-cite> in the quadratic form $$v\mapsto \frac12\langle\nabla^2f(x)v, v\rangle - \langle b, v\rangle$$.

All these methods to solve \eqref{eq:linear_system} share a common feature: they involve HVPs computations in their iterations. Thus, an efficient implementation of HVPs is crucial for the overall algorithm performance.

### HVPs for the study of the loss landscape

The study of the geometry of neural networks is an active field that aims at understanding the links between training dynamics, local geometry of the training loss and generalization. One way to study the local geometry of a neural network is to find the distribution of the eigenvalues of its Hessian matrix. Indeed, depending on the sign of the eigenvalues of the Hessian, one can for instance distinguish local minima, local maxima and saddle points. As an illustration, the following figure shows how the sign of the eigenvalues of the Hessian matrix of a function affects the shape of the function's landscape around a stationary point.

{% include figure.html path="assets/img/2024-05-07-bench-hvp/hess_eig.png" class="img-fluid" %}
 

In several papers<d-cite key=Ghorbani2019></d-cite><d-cite key=Dauphin2014></d-cite><d-cite key=Sagun2018></d-cite><d-cite key=Foret2021SAM></d-cite>, an approximation of the Hessian spectrum is computed thanks to the Lanczos algorithm<d-cite key=Lanczos1950></d-cite>. This algorithm is a modification of the power method where each new iterate is taken in the orthogonal complement of the previous iterates.

<p class="framed">
  <b class="underline">Lanczos' algorithm</b><br>

  <b>Input</b> Initial vector \(v_0\).<br>
  <b>Initialization</b>
  $$
    w'_0 = \textcolor{orange}{\nabla^2f(x)v_0},\quad \alpha_0 = w_0'^\top v_0,\quad w_0 = w_0' - \alpha_0 v_0
  $$

  <b>For</b> \(i = 1,\dots, k-1\):<br>

  \begin{align*}
    \beta_i &= \|w_{i-1}\|\\
    v_{i} &= \frac{w_{i-1}}{\beta_{i}}\\
    w_i' &= \textcolor{orange}{\nabla^2f(x)v_i}\\
    \alpha_i &= w_i'^\top v_i\\
    w_i &= w_i' - \alpha_i v_i - \beta_iv_{i-1}
  \end{align*}
</p>

We observe once again that the Hessian information is accessed through HVPs rather than the full Hessian matrix itself.

We now turn to the practical computation of HVPs.


## A quick detour by automatic differentiation

Automatic differentiation is an important tool to compute automatically the derivatives of differentiable functions. In machine learning, it is used everywhere to compute gradients of deep learning cost functions<d-cite key="Rumelhart1986"></d-cite>. There are two modes in automatic differentiation; the forward mode that computes Jacobian-vector products (JVPs) and the reverse mode that computes vector-Jacobian products (VJPs). Since the gradient of a scalar function is a special case of the VJP, the reverse mode is the most frequently used in machine learning.

In what follows, we present briefly the notion of computational graph and the two automatic differentiation modes. For a more detailed explanation, we refer the reader to the survey by Baydin et al.<d-cite key="Baydin2018"></d-cite>. 

We consider a differentiable function $$f:\mathbb{R}^d\to\mathbb{R}^p$$. For any $$x\in\mathbb{R}^d$$, we also consider $$f(x) = (f_1(x),\dots,f_p(x))^\top\in\mathbb{R}^p$$. The Jacobian matrix of $$f$$ at $$x$$ is the matrix of its partial derivatives

$$
  Df(x) = \left[\frac{\partial f_i}{\partial x_j}(x)\right]_{\substack{1\leq i\leq p\\ 1 \leq j\leq d}} = \begin{bmatrix} \nabla f_1(x)^\top \\ \vdots \\ \nabla f_p(x)^\top\end{bmatrix}\in\mathbb{R}^{p\times d}\enspace.
$$

### Computational graph

A key ingredient of automatic differentiation is a computational graph associated with the code that evaluates a function. It is a directed acyclic graph that represents the succession of elementary operations required the evaluate a function. Let us consider for instance a two-layer linear perceptron $$f_x:\mathbb{R}^2\times \mathbb{R}^{2\times 2}\to \mathbb{R}$$ defined for a provided $$x\in\mathbb{R}^2$$ by

\begin{equation}\label{eq:mlp}
  f_x(U, W) = \frac12(UWx)^2\enspace.
\end{equation}

A possible computational graph to get $$f_x(U, W)$$ is the following

{% include figure.html path="assets/img/2024-05-07-bench-hvp/computational_graph.png" class="img-fluid"%}

In this graph, the vertices $$z_i$$ represent the intermediate states of the evaluation of $$f$$. Hence, the complexity of a function evaluation depends on the complexity of the considered computational graph. In terms of memory, to get the vertex $$z_i$$, we use the values stored by its parents in the graph. Thus, the memory footprint of the evaluation of the function is linked to the maximum number of parents that can have a vertex in the computational graph. Particularly, in our case, each node has at most two parents. The Python code to compute $$f$$ by following the previous computational graph is the following

```python
def f(U, W):
    z1 = W @ x
    z2 = U @ z1
    output = .5 * z2**2
    return output
```

### Forward mode

The chain rule enables us to leverage the computational graph to evaluate efficiently the derivatives of a function. Let us consider this computational graph fragment.

{% include figure.html path="assets/img/2024-05-07-bench-hvp/forward_diff.png" class="img-fluid" %}

To compute the derivative of the node $$z_i$$ with respect its ancestor $$z_j$$, the chain rule yields

\begin{equation}\label{eq:chain_rule_forward}
  \frac{\partial z_i}{\partial z_j}=
  \sum_{k\in\text{parents}(i)} \textcolor{red}{\frac{\partial z_i}{\partial z_k}}\textcolor{blue}{\frac{\partial z_k}{\partial z_j}}\enspace.
\end{equation}

If $$z_k$$ is a parent of $$z_i$$, the quantity $$\textcolor{red}{\frac{\partial z_i}{\partial z_k}}$$ is easy to compute since $$z_i$$ and $$z_k$$ are connected by an elementary operation we know how to differentiate. For the quantity $$\textcolor{blue}{\frac{\partial z_k}{\partial z_j}}$$, we can compute it recursively thanks to \eqref{eq:chain_rule_forward} during a forward pass in the computational graph and using the fact that $$\frac{\partial z_j}{\partial z_j} = 1$$. Note that by initializing by $$\frac{\partial z_j}{\partial z_j} = v$$ for some vector $$v$$, the forward differentiation in \eqref{eq:chain_rule_forward}  returns the Jacobian-vector product (JVP) $$\frac{\partial z_i}{\partial z_j} v$$.

Thus, computing a JVP can be achieved by a forward pass in a computational graph with the supplementary overhead of the computation of the successive JVPs. Therefore, the time complexity of forward differentiation is roughly twice the time complexity of the evaluation of $$f$$.

To understand properly what is happening when using forward differentiation, let us go back to the linear MLP defined in \eqref{eq:mlp}.
If we code ourselves the forward differentiation to get the JVP, we obtain the following code

``` python
def jvp(U, W, vu, vw):
    # Forward diff of f
    z1 = W @ x
    vz1 = vw @ x  # Directional derivative of W -> W @ x in the direction vw

    z2 = U @ z1
    vz2 = U @ vz1 + vu @ z1  #  Directional derivative of (U, z_1) -> U @ z1 in the direction (vu, vz1)
  
    output = vz2 @ z2  # Directional derivative of z2 -> .5*z2**2 in the direction vz2 

    return output
```

In comparison with the code of the evaluation of $$f$$, there are two more operations corresponding to the computation of the dual variable `vz1` and `vz2`. In terms of memory, if we consider the computational of the JVP as coded in the previous snippet, the maximum number of parents of a vertex is four. This maximum is achieved by the vertex `vz2` which has the vertices `U`, `vz1`, `vu` and `z1` as parents.

In JAX, we can get the JVP of $$f$$ at $$x$$ in the direction $$v$$ with `jax.jvp(f, (x, ), (v, ))[1]`.

### Reverse mode
Now, let us present the reverse mode automatic differentiation. We consider the following computational graph snippet

{% include figure.html path="assets/img/2024-05-07-bench-hvp/reverse_diff.png" class="img-fluid" %}

Instead of expressing $$\frac{\partial z_i}{\partial z_j}$$ according to the parents nodes of $$z_i$$, one can express it according to the children nodes of $$z_j$$

\begin{equation}\label{eq:chain_rule_backward}
  \frac{\partial z_i}{\partial z_j}=
  \sum_{k\in\text{children}(j)}\textcolor{blue}{\frac{\partial z_i}{\partial z_k}}\textcolor{red}{\frac{\partial z_k}{\partial z_j}}\enspace.
\end{equation}

If $$z_k$$ is a child of $$z_j$$, then the quantity $$\textcolor{red}{\frac{\partial z_k}{\partial z_j}}$$ is easy to compute since the vertices $$z_k$$ and $$z_j$$ are linked by an elementary operation we know how to differentiate. To get $$\textcolor{blue}{\frac{\partial z_i}{\partial z_k}}$$, one can apply the formula \eqref{eq:chain_rule_backward} recursively during a backward pass in the computational graph with the initialization $$\frac{\partial z_i}{\partial z_i} = 1$$. If we initialize the backward pass with $$\frac{\partial z_i}{\partial z_i} = v$$, then the reverse automatic differentiation returns the vector-Jacobian product (VJP) $$v \frac{\partial z_i}{\partial z_j}$$.

At the end of the day, the reverse automatic differentiation requires a forward pass in the computational graph to get the intermediate states $$z_i$$ and a backward pass to propagate the intermediate partial derivatives. Hence, the complexity of the reverse automatic differentiation is twice the complexity of the evaluation of $$f$$. Moreover, it suffers from a memory burden because of the storing of all the intermediate states $$z_i$$ computed during the forward pass. 

Let us observe what happens if we code manually the backpropagation to get the gradient of the previous function $$f_x$$ defined by $$f_x(U, W) = \frac12(UW^\top x)^2$$.

``` python
def gradient(U, W):
    ## Forward pass
    z1 = W @ x
    z2 = U @ z1
    output = .5 * z2**2

    ## Reverse pass
    ### output = .5 * z2**2
    dz2 = z2

    ### z2 = U @ z1
    dU = jnp.outer(dz2, z1)
    dz1 = U.T @ dz2

    ### z1 = W @ x
    dW = jnp.outer(dz1, x)
    
    return dU, dW
```

This function returns the gradient of $$f$$. At reading this code, we understand one needs to store all the intermediate values of the forward pass in the graph. Indeed, if we look at the case of `z1` which is the first node computed, it is used four steps later for the computation of `dU`.

To get the VJP in JAX, one can use `jax.vjp(f, x)[1](v)`.


## Naive computation of HVPs
Since we are interested in computing $$\nabla^2 f(x)v$$, the simplest way to do it is to compute the Hessian matrix and then multiply it by the vector $$v$$. This can be achieved in `JAX`` by calling `jax.hessian(f) @ v`.

However, this method is quite cumbersome making it impossible to use for deep neural networks. Indeed, the storage of the full Hessian matrix has $$\mathcal{O}(d^2)$$ complexity where $$d$$ is the input dimension.

Nevertheless, we will see in what follows that computing and storing the full Hessian matrix is not mandatory to compute HVPs.

## HVPs without explicit Hessian computation
A clever use of automatic differentiation enables computing HVPs without computing the Hessian. The [JAX documentation](https://jax.readthedocs.io/en/latest/notebooks/autodiff_cookbook.html) describes three ways to do it.

### Forward-over-reverse
The HVPs have another interpretation coming from differential calculus. They are the directional derivatives of the gradient of $$f$$ in the direction $v$<d-cite key="Pearlmutter1994"></d-cite>:

$$
\nabla^2f(x) v = \lim_{\epsilon\to 0} \frac1\epsilon[\nabla f(x+\epsilon v)-\nabla f(x)]\enspace.
$$

Now let us take a step back. If we have a differentiable vector field $$\phi:\mathbb{R}^p\to\mathbb{R}^p$$, the directional derivative of $$\phi$$ at $$x$$ in the direction $$v$$ is exactly the product between its Jacobian matrix at $$x$$ and the vector $$v$$. As we have seen before, this Jacobian-Vector product (JVP) can be efficiently computed by forward differentiation. In this case, its computational complexity is twice the complexity of the evaluation of the vector field $$\phi$$. 

Hence, to get the HVP, one can apply the previous principle by tacking $$\phi = \nabla f$$. The implementation in `JAX`` is only two lines of code. 

```python
def hvp_forward_over_reverse(f, x, v):
  return jax.jvp(jax.grad(f), (x, ), (v, ))[1]
```
In this case, `jax.grad(f)(x)` is computed by backward automatic differentiation, whose complexity is two times the complexity of evaluating $$f$$. Thus, the temporal complexity of `hvp_forward_over_reverse` is roughly four times the complexity of the evaluation of $$f$$.

To better see what happens, let us consider again our function $$f$$ defined by \eqref{eq:mlp}. The Python code of the `forward-over-reverse` HVP is the following.

```python
def forward_over_reverse(U, W, vu, vw):
    # Forward through f
    z1 = W @ x
    vz1 = vw @ x

    z2 = U @ z1
    vz2 = U @ vz1 + vu @ z1

    z3 = z2  # dz2
    vz3 = vz2

    # Backward through f
    z4 = jnp.outer(z3, z1)  # dU
    vz4 = jnp.outer(vz3, z1) + jnp.outer(z3, vz1) 

    z5 = U.T @ z3  # dz1
    vz5 = U.T @ vz3 + vu.T @ z3

    z6 = jnp.outer(z5, x)  # dW
    vz6 = jnp.outer(vz5, x)

    return vz4, vz6
  ```

The take-home message of this part is that, after computing the gradient of $$f$$, one can consider a computational graph of this gradient and perform forward differentiation through this new computational graph. Here, the variables `z1`,..., `z6` are the vertices of a computational graph of the gradient of $$f$$.

### Reverse-over-reverse
The second way is the reverse-over-reverse mode. It leverages the fact that the HVP is also the gradient of the scalar product between the gradient of $$f$$ and $$v$$.

$$
\nabla^2f(x)v = \nabla[\langle\nabla f(.),v\rangle](x)
$$

Therefore, these lines of `JAX`` implement the HVP
```python
def hvp_reverse_over_reverse(f, x, v):
  return jax.grad(lambda x: jnp.vdot(jax.grad(f)(x), v))
```
Since the gradients are computed by backpropagation, the complexity of `hvp_reverse_over_reverse` is twice the complexity of `jax.grad(f)`, which is roughly four times the complexity of the evaluation of $$f$$.

Writting down the code of the reverse-over-reverse HVP for our function $$f$$ defined by \eqref{eq:mlp} makes us understand the differences between this mode and the `forward-over-reverse` mode. Particularly, one can notice that there are more elementary operations in the `reverse-over-reverse` mode than in the `forward-over-reverse` mode. Moreover, in terms of memory footprint, the `reverse-over-reverse` requires storing the values of the vertices of the computational graph of the gradient of $$f$$, while the `forward-over-reverse` only needs to store the values of the vertices of the computational graph of $$f$$. Thus, the former is less efficient than the latter.

```python
def reverse_over_reverse(U, W, vu, vw):
    # Forward through grad(f)
    ## Forward through f
    z1 = W @ x
    z2 = U @ z1
    output = .5 * jnp.linalg.norm(z2)**2

    ## Reverse through f
    ### output = .5 * jnp.linalg.norm(z2)**2
    z3 = z2  # dz2

    ### z2 = U @ z1
    z4 = jnp.outer(z3, z1) # dU
    z5 = U.T @ z3 # dz1
    z6 = jnp.outer(z5, x) # dW

    # Dot product <grad(f), v>
    output = jnp.sum(vu * z4) + jnp.sum(vw * z6)

    # Backward through <grad(f),v>
    ## output = jnp.sum(vu * z4) + jnp.sum(vw * z6)
    dz6 = vw
    dz4 = vu

    ## z6 = jnp.outer(z5, x)
    dz5 = dz6 @ x

    ## z5 = U.T @ z3
    dz3 = U @ dz5
    dU = jnp.outer(z3, dz5)

    ## z4 = jnp.outer(z3, z1)
    dz3 += dz4 @ z1
    dz1 = dz4.T @ z3

    ## z3 = z2
    dz2 = dz3

    ## z2 = U @ z1
    dz1 += dz2 * U
    dU += jnp.outer(dz2, z1)

    ## z1 = W @ x
    dW = jnp.outer(dz1, x)

    return dU, dW
  ```

### Reverse-over-forward
The quantity $$\langle \nabla f(x),v\rangle$$ is also the product between the Jacobian matrix of $$f$$ and the vector $$v$$. Therefore, it can be computed using forward automatic differentiation.

```python
def hvp_reverse_over_forward(f, x, v):
  jvp_fun = lambda x: jax.jvp(f, (x, ), (v, ))[1]
  return jax.grad(jvp_fun)(x)
```

This method a more efficient than the previous one. Indeed, since we backpropagate only once, the memory burden is lower than for the `reverse_over_reverse` fashion. In comparison with `forward-over-reverse`, the complexity is the same. However, one can notice that the `forward-over-reverse` enables computing at the same time the gradient of $$f$$ and the HVP, which is not the case for the `reverse-over-forward` mode. This feature can be useful in bilevel optimization for instance.

```python
def reverse_over_forward(U, W, vu, vw):
    # Forward diff of f to get jvp(f, (U, W), (vu, vw))
    z1 = W @ x
    vz1 = vw @ x

    z2 = U @ z1
    vz2 = U @ vz1 + vu @ z1

    output = vz2 @ z2

    # Backward pass through jvp(f, (U, W), (vu, vw))
    ## output = vz2 @ z2
    dz2 = vz2
    dvz2 = z2

    ## vz2 = U @ vz1 + vu @ z1
    dz1 = vu.T @ dvz2
    dvu = jnp.outer(dvz2, z1) 
    dvz1 = U.T @ dvz2
    dU = jnp.outer(dvz2, vz1)

    ## z2 = U @ z1
    dz1 += U.T @ dz2
    dU += jnp.outer(dz2, z1)

    ## vz1 = vw @ x
    dvw = jnp.outer(dvz1, x)
    
    ## z1 = W @ x
    dW = jnp.outer(dz1, x)

    return dU, dW
```

Now we turn to the main contribution of this blogpost, which is a benchmark of the different methods to compute HVPs.

## Benchmark with deep learning architectures

While these three methods compute the same outputs, the different ways of traversing the computational graph change their overall time and memory complexities. We now compare the computation of HVPs with these three methods for various deep-learning architectures. In order to cover a broad range of use cases, we consider a residual network ([ResNet34](https://huggingface.co/docs/transformers/model_doc/resnet)<d-cite key="He2015resnet"></d-cite>) and a transformer based architecture ([ViT-base](https://huggingface.co/docs/transformers/model_doc/vit)<d-cite key="Dosovitskiy2021"></d-cite>) for image classification as well as a transformer for natural language processing ([Bert-base](https://huggingface.co/docs/transformers/model_doc/bert#transformers.FlaxBertForTokenClassification).<d-cite key="Devlin2019"></d-cite>).
We use the Flax implementation of these architectures available in the [transformers package](https://huggingface.co/docs/transformers/) provided by [Hugging Face 🤗](https://huggingface.co).

All computations were run on an Nvidia A100 GPU with 40 GB of memory.


### Time complexity

The first comparison we make is a comparison in terms of wall-clock time between the different ways to compute HVPs and also the computation of a gradient by backpropagation. For each architecture, we compute the gradient of the model with respect to the parameters by backpropagation. We also compute the HVPs in `forward-over-reverse`, `reverse-over-forward` and `reverse-over-reverse` modes. For each computation, we measure the time taken, and for the HVP, we subtract the time taken by a gradient computation, to get only the time of the overhead required by the HVP computation.
The inputs for each architecture are generated randomly. For the ResNet34 architecture, we generated a batch of images of size 224x224x3. To limit out-of-memory issues in the experiments, we generated for the ViT architecture images of size 96x96x3. For the BERT architecture, we generated a batch of sequences of length 32.

We first use JAX. Each computation is jit-compiled and is run 90 times. We plot on the left of the figure, the median computation time and also the 20% and 80% percentile. The computations are done with a batch size of 128. The first observation we make is that, in practice, a HVP computation takes between twice and three times the time taken by a gradient computation for the three architectures, which is consistent with the theory. Moreover, the `reverse-over-reverse` is the slowest in all the cases.


We also report on the right figure the computational time of each method with respect to the batch size for the ResNet34 architecture. We observe, as expected, that the computational time scales linearly with the batch size.

{% include figure.html path="assets/img/2024-05-07-bench-hvp/bench_hvp_time_jax.png" class="img-fluid" %}

We run a similar experiment with the JAX-like API available in `PyTorch` [`torch.func`](https://pytorch.org/docs/stable/func.html). The results we get are more contrasted. If for the case of ResNet34, the scaling between the different methods is similar to the one we get with JAX. For ViT and BERT, the `forward-over-reverse` is surprisingly longer than the `reverse-over-reverse` method. Moreover, the scaling between the gradient and HVP computational time differs from the one we get with `JAX`. Indeed, for the ViT and BERT architecture, the HVP computations take between four and five more time than the gradient computations. This is a discrepancy with what we would expect in theory.

{% include figure.html path="assets/img/2024-05-07-bench-hvp/bench_hvp_time_torch.png" class="img-fluid" %}

### Memory complexity

We also compare the memory footprint of each approach. The following figure provides the results we get with jax jitted code. On the left, we represent the result for each method and model with a batch size of 64. On the right, we show the evolution of the memory footprint of each method for the ResNet34 with the batch size. Surprisingly, we could observe that the memory footprint of the different methods to compute HVPs does not vary for a given model. This is counterintuitive since we expect that the `reverse-over-reverse` method have a larger memory footprint due to the double backpropagation.

{% include figure.html path="assets/img/2024-05-07-bench-hvp/bench_hvp_memory_jax.png" class="img-fluid" %}

However, we do the same experiment by *disabling the JIT compilation*. The result we get corroborates the theory. Indeed, one can observe in the following figure that the memory footprint of the `reverse-over-reverse` method is larger than the one of the `forward-over-reverse` and `reverse-over-forward` methods. Moreover, it scales linearly with the batch size, which was not the case in the previous figure in the small batch size regime. In light of these two results, we can conjecture that the computational graph of the different HVP methods contains some redundancy that is optimized by the JIT compilation.

{% include figure.html path="assets/img/2024-05-07-bench-hvp/bench_hvp_memory_jax_without_jit.png" class="img-fluid" %}

In the following figure, we plot the results we get with the `PyTorch` implementation. One can observe that in all the cases the `forward-over-reverse` consumes much memory in comparison with the `reverse-over-forward` mode. The right plot of the evolution of the memory footprint with the batch size for the ResNet34 architecture evolves linearly as expected.

{% include figure.html path="assets/img/2024-05-07-bench-hvp/bench_hvp_memory_torch.png" class="img-fluid" %}



