---
layout: distill
title: >-
  A Critical Exploration of 'Bayesian Model Selection, Marginal Likelihood, and Generalization in Neural Networks'
description: >- 
  This blog post comprehensively reviews the ICML 2022 paper titled 'Bayesian Model Selection, the Marginal Likelihood, and Generalization.' The paper's central thesis, examining the log marginal likelihood (LML) and its variant, the conditional log marginal likelihood (CLML), in various machine learning settings, is thoroughly analyzed, and the post critically engages with the paper's methodologies and findings, particularly scrutinizing the CLML's applicability and effectiveness in deep learning scenarios. The review extends beyond summarization to challenge assumptions, compare with existing literature, and examine the evaluation. This deep dive aims to foster a better understanding of Bayesian methods in model evaluation, spotlighting both their strengths and limitations in the context of neural network generalization.
date: 2024-05-07
future: true
htmlwidgets: true
tags:
- Bayesian Neural Network
- Generalization
- Model Information
- Marginal Likelihood
- Laplace Approximation
- Model Selection

# Anonymize when submitting
authors:
  - name: Anonymous

# authors:
#   - name: Albert Einstein
#     url: "https://en.wikipedia.org/wiki/Albert_Einstein"
#     affiliations:
#       name: IAS, Princeton
#   - name: Boris Podolsky
#     url: "https://en.wikipedia.org/wiki/Boris_Podolsky"
#     affiliations:
#       name: IAS, Princeton
#   - name: Nathan Rosen
#     url: "https://en.wikipedia.org/wiki/Nathan_Rosen"
#     affiliations:
#       name: IAS, Princeton

# must be the exact same name as your blogpost
bibliography: 2024-05-07-clml.bib  

# Add a table of contents to your post.
#   - make sure that TOC names match the actual section names
#     for hyperlinks within the post to work correctly. 
#   - please use this format rather than manually creating a markdown table of contents.
toc:
  - name: Equations
  - name: Images and Figures
    subsections:
    - name: Interactive Figures
  - name: Citations
  - name: Footnotes
  - name: Code Blocks
  - name: Diagrams
  - name: Tweets
  - name: Layouts
  - name: Other Typography?

# Below is an example of injecting additional post-specific styles.
# This is used in the 'Layouts' section of this post.
# If you use this post as a template, delete this _styles block.
_styles: >
  .fake-img {
    background: #bbb;
    border: 1px solid rgba(0, 0, 0, 0.1);
    box-shadow: 0 0px 4px rgba(0, 0, 0, 0.1);
    margin-bottom: 12px;
  }
  .fake-img p {
    font-family: monospace;
    color: white;
    text-align: left;
    margin: 12px 0;
    text-align: center;
    font-size: 16px;
  }
  d-article .box-note,
  d-article .box-warning,
  d-article .box-error,
  d-article .box-important {
    padding: 15px 15px 15px 10px;
    margin: 20px 20px 20px 5px;
    border: 1px solid #eee;
    border-left-width: 5px;
    border-radius: 5px 3px 3px 5px;
  }
  d-article .box-note {
    background-color: #eee;
    border-left-color: #2980b9;
  }
  d-article .box-warning {
    background-color: #fdf5d4;
    border-left-color: #f1c40f;
  }
  d-article .box-error {
    background-color: #f4dddb;
    border-left-color: #c0392b;
  }
  d-article .box-important {
    background-color: #d4f4dd;
    border-left-color: #2bc039;
  }
  html[data-theme='dark'] d-article .box-note {
    background-color: #555555;
    border-left-color: #2980b9;
  }
  html[data-theme='dark'] d-article .box-warning {
    background-color: #7f7f00;
    border-left-color: #f1c40f;
  }
  html[data-theme='dark'] d-article .box-error {
    background-color: #800000;
    border-left-color: #c0392b;
  }
  html[data-theme='dark'] d-article .box-important {
    background-color: #006600;
    border-left-color: #2bc039;
  }
  d-article aside *,
  d-article figcaption {
    color: var(--global-text-color) !important;
  }
  d-article article p {
    text-align: justify;
    text-justify: inter-word;
    -ms-hyphens: auto;
    -moz-hyphens: auto;
    -webkit-hyphens: auto;
    hyphens: auto;
  }
  d-article aside {
    border: 1px solid #aaa;
    border-radius: 4px;
    padding: .5em .5em 0;
    font-size: 100%;
  }
  d-article aside p:first-child {
      margin-top: 0;
  }
  d-article details {
    border: 1px solid #aaa;
    border-radius: 4px;
    padding: .5em .5em 0;
  }
  d-article summary {
    font-weight: bold;
    margin: -.5em -.5em 0;
    padding: .5em;
    display: list-item;
  }
  d-article details[open] {
    padding: .5em;
  }
  d-article details[open] summary {
    border-bottom: 1px solid #aaa;
    margin-bottom: .5em;
  }
---

The paper, accepted as Long Oral at ICML 2022, discusses the *(log) marginal likelihood (LML)* in detail: its advantages, use-cases, and potential pitfalls, with an extensive review of related work. It further suggests using the ‚Äú*conditional (log) marginal likelihood (CLML)*‚Äù instead of the LML and shows that it captures the quality of generalization better than the LML.

The paper examines the behavior of LML and CLML in various settings: density models, Fourier features, Gaussian Processes, and deep neural networks, with various insightful toy experiments and larger experiments on CIFAR-10 and CIFAR-100.

The paper contrasts its findings with [Lyle et al. (2020) ‚ÄúA Bayesian Perspective on Training Speed and Model Selection‚Äù](https://arxiv.org/abs/2010.14499)<d-cite key="lyle2020bayesian"></d-cite> and [Immer et al. (2021)](https://arxiv.org/abs/2104.04975)<d-cite key="immer2021scalable"></d-cite>.

This paper review is in two halves:
the first half highlights interesting parts of the paper; the second half critically engages with some of the concepts, particularly the CLML for DNNs. It considers similarities between the suggested CLML, cross-validation, and validation sets & losses.

I focus on the experimental results for DNNs in more detail because this is where I can engage critically with the material the best. The paper looks at many more settings, particularly deep kernel learning and GPs. 

<aside class="box-note l-body" markdown="1">
üìë **TL;DR:** The proposed CLML provides a Bayesian motivation for using validation sets with deep neural networks in the large-data regime. Following additional experiments from the authors, it seems the validation loss using Bayesian model averaging provides an even stronger signal than the (sampled) CLML estimate for DNNs. However, some questions remain.
</aside>

<aside class="box-note l-body" markdown="1">
üëÅÔ∏è‚Äçüó®Ô∏è This review is based on [v1 of the paper](https://arxiv.org/abs/2202.11678v1)<d-cite key="lotfi2022bayesian"></d-cite>. After reaching out to the authors, they have [updated the paper, expanded the discussion section](/assets/2024-05-07/bmsmlg_v1_v2_version_comparison.pdf), and replied to a draft of this post. I shared two drafts with the authors (on 2022/5/30 & 31, respectively) before their reply (on 2022/6/3), and their comments refer to the first draft. This post is the second draft, except for minor edits. I have also added additional boxes marked with "üëÅÔ∏è‚Äçüó®Ô∏è" to refer to the authors' response. I include the reply at the end. I also reply with additional investigations given the updated experiments.
</aside>

# Details

| | Bayesian Model Selection, the Marginal Likelihood, and Generalization |
| :---        |    :----  |
| by | Sanae Lotfi, Pavel Izmailov, Gregory Benton, Micah Goldblum, Andrew Gordon Wilson |
|    | [**Accepted to ICML 2022 as Long Oral**](https://icml.cc/Conferences/2022/Schedule?showEvent=17992) |
| Paper Link | <https://arxiv.org/abs/2202.11678> |
| Code | <https://github.com/Sanaelotfi/Bayesian_model_comparison> |

<details markdown="1"><summary>BibTex</summary> 
```bibtex
@inproceedings{lotfi2022bayesian,
  title={Bayesian Model Selection, the Marginal Likelihood, and Generalization},
  author={Lotfi, Sanae and Izmailov, Pavel and Benton, Gregory and Goldblum, Micah and Wilson, Andrew Gordon},
  booktitle={International Conference on Machine Learning},
  pages={14223--14247},
  year={2022},
  organization={PMLR}
}
```
</details>

# Why?

<style>
d-article aside {
border: 1px solid #aaa;
border-radius: 4px;
padding: .5em .5em 0;
font-size: 90%;
}

d-article aside p:first-child {
    margin-top: 0;
}

d-article details {
border: 1px solid #aaa;
border-radius: 4px;
padding: .5em .5em 0;
}

d-article summary {
font-weight: bold;
margin: -.5em -.5em 0;
padding: .5em;
display: list-item;
}

d-article details[open] {
padding: .5em;
}

d-article details[open] summary {
border-bottom: 1px solid #aaa;
margin-bottom: .5em;
}
</style>

[Occam‚Äôs razor](https://en.wikipedia.org/wiki/Occam%27s_razor) is a much-applied principle in science:

> To decide between different possible explanations, we heavily rely on a notion of Occam‚Äôs razor ‚Äî that the ‚Äúsimplest‚Äù explanation of data consistent with our observations is most likely to be true.

The marginal likelihood, also known as model evidence $$p(\mathcal{D} \mid \mathcal{M}_i)$$ for different hypotheses $$\mathcal{M}_i$$, captures this notion. **Other things considered equal,** the highest model evidence points us towards the best hypothesis given data $$\mathcal{D}$$. The paper cites [an illustrative example from chapter 28](https://www.inference.org.uk/itprnn/book.pdf#page=355) of David MacKay‚Äôs ["Information Theory, Inference, and Learning Algorithms‚Äù](http://www.inference.org.uk/mackay/itila/book.html):

{% include figure.html path="assets/img/2024-05-07-clml/bmsmlg_sec4.1.png" class="img-fluid" %}
<div class="caption" markdown="1">
The marginal likelihood answers the question ‚Äúwhat is the probability that a prior model generated the training data?‚Äù. This question is subtly different from asking ‚Äúhow likely is the posterior, conditioned on the training data, to have generated withheld points drawn from the same distribution?‚Äù. Although the marginal likelihood is often used as a proxy for generalization (e.g. MacKay, 1992c; Immer et al., 2021; Daxberger et al., 2021), it is the latter question we wish to answer in deciding whether a model will provide good generalization performance."
</div>

{% capture caption %}
Excerpt from page 343 in David MacKay‚Äôs "Information Theory, Inference, and Learning Algorithms.‚Äù
{% endcapture %}
{% include figure.html path="assets/img/2024-05-07-clml/mackay_343.png" class="img-fluid" caption=caption%}

The **log marginal likelihood (LML)**, which is examined in this paper, is simply the log of the model evidence:

$$\log p(\mathcal{D} \mid \mathcal{M}_i).$$

The papers offers a concise pitch in its introduction:

> we aim to fundamentally re-evaluate whether the marginal likelihood is the right metric for predicting the generalization of trained models, and learning hyperparameters. We argue that it does a good job of prior hypothesis testing, which is exactly aligned with the question it is designed to answer. **However, we show that the marginal likelihood is only peripherally related to the question of which model we expect to generalize best after training, with significant implications for its use in model selection and hyperparameter learning.**

And overall:

> There is a great need for a more comprehensive exposition, clearly demonstrating the limits of the marginal likelihood, while acknowledging its unique strengths, **especially given the rise of the marginal likelihood in deep learning.**

# How?

Following the paper, I will summarize the different sections here. 

## Use-cases

Given the above, the case for the marginal likelihood in the paper focuses on:

- hypothesis testing;
- hyperparameter learning; and
- constraint learning.

## General Pitfalls

For the paper, the pitfalls are manifold. The most important is the claim that:

> **Marginal Likelihood is not generalization**
> 

The following explanation is helpful:

{% capture caption %}
The marginal likelihood answers the question ‚Äúwhat is the probability that a prior model generated the training data?‚Äù. This question is subtly different from asking ‚Äúhow likely is the posterior, conditioned on the training data, to have generated withheld points drawn from the same distribution?‚Äù. Although the marginal likelihood is often used as a proxy for generalization (e.g. MacKay, 1992c; Immer et al., 2021; Daxberger et al., 2021), it is the latter question we wish to answer in deciding whether a model will provide good generalization performance."
{% endcapture %}
{% capture max-width %}
" style="max-width: 25em;
{% endcapture %}
{% include figure.html path="assets/img/2024-05-07-clml/bmsmlg_sec4.1.png" class="img-fluid" max-width=max-width caption=caption%}

<aside class="box-note l-body" markdown="1">
‚ö†Ô∏è The first question might be mistaken for asking what the probability is of drawing the training data independently from a fixed prior, which is what the second question is about: what is the average performance of the *marginal predictions* for test data given the training data?

There is sometimes ambiguity in the literature between viewing a model as its parameter distribution (and using Bayesian model averaging to make predictions) or saying a specific parameter draw from the distribution is a model. The first question is about the latter, while the second question is about the former.

The marginal likelihood can be written as: 

$$p(\mathcal{D} \mid \mathcal{M}) = \mathbb{E}_{p(\omega)} p(\mathcal{D} \mid \omega, \mathcal{M}),$$ 

and the first question can be made clearer as: *what is the probability that **a draw from the prior distribution** generated the training data?*

As we will see below, the first question can also be viewed in the context of sequential predictions and learning curves. In information theory, the question would be: how well does the model compress the training data jointly? Looking at the joint predictive distribution is not yet a common concept in deep learning, however. 

Recent works by Ian Osband et al., starting with [The Neural Testbed: Evaluating Joint Predictions](https://arxiv.org/abs/2110.04629)<d-cite key="osband2022neural"></d-cite> can help build intuitions for joint predictions.
Similarly, a *gentler* introduction, comparing marginal and joint predictions, can also be found in our recent note [Marginal and Joint Cross-Entropies & Predictives for Online Bayesian Inference, Active Learning, and Active Sampling](https://arxiv.org/abs/2205.08766)<d-cite key="kirsch2022marginal"></d-cite>. 
</aside>

The following figure summarizes several of the pitfalls the paper examines: 

{% capture caption %}
<b>Note: typo in ‚Äú</b>(a)<b> Prior B‚Äù ‚Üí ‚Äú</b>(a)<b> Prior C‚Äù.</b>
{% endcapture %}
{% include figure.html path="assets/img/2024-05-07-clml/bmsmlg_fig1.png" class="img-fluid" caption=caption%}

The paper emphasizes the difference between hypothesis testing and model selection:

{% capture max-width %}
" style="max-width: 25em;
{% endcapture %}
{% include figure.html path="assets/img/2024-05-07-clml/bmsmlg_sec4.1_model_selection.png" max-width=max-width %}

So, in hypothesis testing, we want to determine which model class (hypothesis) provides the ‚Äúbest‚Äù explanation for the past data we have collected, i.e., the training data; whereas for model selection, we care about which model class provides the best performance for future, not-yet-seen data when we condition/train the model on the currently available training data. 

<aside class="box-note l-body" markdown="1">
‚òù Interestingly, according to Occam's razor, both should be related: the model class that provides the simplest explanation for the current training data should also generalize the best.
</aside>

## Estimating LML via the Laplace Approximation

Computing the marginal likelihood via sampling is generally intractable for (B)NNs. Estimating it from a prior, usually highly uninformative distribution leads to high-variance estimates when performing Monte Carlo sampling compared to sampling from the posterior distribution, which is more practical. The latter is the approach followed by the paper for the CLML using a Laplace approximation of the posterior.

A **Laplace approximation (LA)** estimates the posterior distribution by fitting a Gaussian on the second-order Taylor expansion. We can draw parameter samples from it or compute its entropy to approximate the posterior uncertainty. 

<aside class="box-note l-body" markdown="1">
üí° The LA can be motivated by the [Cram√©r‚ÄìRao bound](https://en.wikipedia.org/wiki/Cram%C3%A9r%E2%80%93Rao_bound) for statistical models, which have "true model parameters." Essentially, the bound tells us that as we take more and more data into account, our posterior will concentrate around the true model parameters for a well-specified model and thus become uni-modal.

A possible resolution for this contradiction might be that we should try to find the model with the largest model posterior, which we examine in the comments. Note that finding a good model prior can be very difficult, however.
</aside>

The paper notes that the LA has drawbacks: it only captures uncertainty around a single mode, leading to underestimation of the model uncertainty. The paper provides a beautiful example of this issue:

{% capture max-width %}
" style="max-width: 35em;
{% endcapture %}
{% include figure.html path="assets/img/2024-05-07-clml/bmsmlg_fig3.png" max-width=max-width %}

This is of particular importance for overparameterized models like DNNs, which have multiple diverse modes, as we know from deep ensembles ([Wilson, Izmailov (2021, blog post)](https://cims.nyu.edu/~andrewgw/deepensembles/);  [Wilson, Izmailov (2020)](https://arxiv.org/abs/2002.08791)<d-cite key="wilson2020bayesian"></d-cite>). The LA, however, only centers around a single one. 


## Conditional Marginal Likelihood

Given that the LML does not capture generalization, i.e., is not suited for model selection according to the paper but intended for hypothesis testing, the paper introduces the **conditional log marginal likelihood (CLML)**. 

The idea behind the CLML can be best understood by using the chain rule of probability/information theory:

$$
\begin{aligned}
\log p(\mathcal{D} \mid \mathcal{M}) &= \log p(y_1, \ldots, y_N \mid x_1, \ldots, x_n, \mathcal{M}) \\
&= \sum_{i=1}^n \log p(y_i \mid x_i, y_{i-1}, x_{i-1}, \ldots, y_1, x_1, \mathcal{M}) \\
&= \sum_{i=1}^n \log p(\mathcal{D}_i \mid \mathcal{D}_{< i}, \mathcal{M}),
\end{aligned}
$$

following notation also used by Lyle et al. (2020).

The paper notes that depending on the underlying model $$\mathcal{M}$$, the first $$m$$ terms might be very low without affecting the model's generalization capabilities later after seeing more data and advocates for dropping these terms from the sum above:

{% capture max-width %}
" style="max-width: 25em;
{% endcapture %}
{% include figure.html path="assets/img/2024-05-07-clml/bmsmlg_sec4.4.png" max-width=max-width %}

Hence, the **Conditional Marginal Likelihood (CLML)** is introduced as a *left-truncated* LML:

$$
\log p(\mathcal{D}_{\ge m} \mid \mathcal{D}_{< m}, \mathcal{M}) = \sum_{i=m}^n \log p(\mathcal{D}_i \mid \mathcal{D}_{< i}, \mathcal{M}),
$$

The paper notes that:

> Variants of the CLML were considered in Berger and Pericchi (1996) as an intrinsic Bayes factor for handling improper uniform priors in hypothesis testing, and [Fong and Holmes (2020)](https://arxiv.org/abs/1905.08737)<d-cite key="fong2020marginal"></d-cite> to show a connection with cross-validation and reduce the sensitivity to the prior.

We will refer to this and other prior literature below.

<aside class="box-note l-body" markdown="1">
‚ö†Ô∏è Given the generally assumed exchangeability of training data in Bayesian models, the ordering of the training data using indices $$i$$ is arbitrary. As the paper points out, the CLML depends on this arbitrary data order, and thus a permutation-invariant version of the CLML is proposed by averaging over different permutations. For larger experiments, this becomes very expensive, and the paper uses a single permutation instead (as noted in appendix D).
</aside>

The paper examines how the CLML performs compared to the LML. It begins by critically reviewing Lyle et al. (2020).

## On Training Speed and Learning Curves

The paper then introduces a **learning curve** as the graph of how $$p(\mathcal{D}_n \mid \mathcal{D}_{<n}, \mathcal{M})$$ changes over $$n$$ (potentially, as average over different permutations of the data order to be permutation-invariant); see (c) below, and compares to [Lyle et al. (2020) ‚ÄúA Bayesian Perspective on Training Speed and Model Selection‚Äù](https://arxiv.org/abs/2010.14499)<d-cite key="lyle2020bayesian"></d-cite>. 

A rough **TL;DR of Lyle et al. (2020)** is that the sum decomposition of the LML, shown earlier, is just the (discrete) area under the learning curve and that for DNNs, we can use the sum over mini-batch training losses as a proxy to predict generalization behavior in the spirit of Occam's razor.

Several examples of failures of LML to predict generalization are provided and connected to the notion of ‚Äútraining speed‚Äù:

{% capture max-width %}
" style="max-width: 50em;
{% endcapture %}
{% include figure.html path="assets/img/2024-05-07-clml/bmsmlg_fig4.png" max-width=max-width %}

The paper thus finds that ‚Äútraining speed‚Äù does not predict generalization performance. 

In particular, for neural networks, the paper reports the following interesting result:

> In Figure 10(a) (Appendix), we show the **correlation of the BMA test log-likelihood with the LML is positive for small datasets and negative for larger datasets, whereas the correlation with the CLML is consistently positive.** Finally, Figure 10(a) (Appendix) shows that the Laplace LML heavily penalizes the number of parameters, as in Section 4.3. We provide additional details in Appendix F.
> 

(where **BMA** stands for **Bayesian Model Average**, i.e., marginalizing over the posterior parameters)

# Model Selection

To show the advantages of the CLML over the LML, the paper examines the correlation with the generalization performance of 25 CNN and ResNet architectures on CIFAR-10 and CIFAR-100:

{% capture max-width %}
" style="max-width: 50em;
{% endcapture %}
{% include figure.html path="assets/img/2024-05-07-clml/bmsmlg_fig5.png" max-width=max-width caption="Figure 5" %}

The CLML is more strongly correlated with the generalization performance of the architectures than the LML. The LML suffers from higher first terms in the sum decomposition for more flexible models and is negatively correlated with test accuracy for smaller $$\lambda$$. The paper notes that the LML estimate using the LA's entropy is sensitive to the prior variance and the number of model parameters. This could also explain why larger models have worse LML.

The CLML is computed using predictions on the held-out data for parameter samples from the LA ("function space"), does not estimate the model entropy, and thus does not suffer from the same issues. 

# Hyperparameter Learning

<aside class="box-note l-body" markdown="1">
üèó **Left out for now: hyperparameter learning.**
</aside>

# And?
While the paper covers a lot more in its breadth, the second half mainly focuses on the CLML and examines the question of how the CLML is different from simply using a validation loss. Again, this is not a question directly examined in the paper, but given the prevalence of validation sets, it might be worth asking what the suggested method provides beyond that.

## Ignoring Model Priors

First, from a Bayesian point of view, using the model evidence (LML) is only valid for model selection when the model prior is uniform **(i.e., all other things are considered equal):**

The Bayesian view on model selection given data $$\mathcal{D}$$ is:

$$
p(\mathcal{M}_i \mid \mathcal{D}) \propto p(\mathcal{D} \mid \mathcal{M}_i) \; p(\mathcal{M}_i).
$$

Hence, maximizing $$p(\mathcal{D} \mid \mathcal{M}_i)$$ is not sufficient depending on what model prior we use.

In particular, the **minimum description length (MDL)** approach states that we want to select the model that minimizes both the length of describing its parameters and the data. 

That is, MDL is about compressing the model and data:

$$
\min_i H[\mathcal{M_i}, \mathcal{D}] = \min_i H[\mathcal{D} \mid \mathcal{M_i}] + H[\mathcal{M_i}],
$$

where $$H$$ is Shannon‚Äôs information content here (using the Practical IT notation). We can also rewrite this as

$$
\begin{aligned}
\arg \min_i H[\mathcal{M_i}, \mathcal{D}] &= \arg \min_i H[\mathcal{M_i} \mid \mathcal{D}] + H[\mathcal{D}] \\&= \arg \min_i H[\mathcal{M_i} \mid \mathcal{D}],
\end{aligned}
$$

and we can drop $$H[\mathcal{D}]$$ as it is independent of the model.

By ignoring the model complexity when we only use the LML, we might potentially ignore an essential part of Occam‚Äôs razor and the MDL principle.
However, many other papers also ignore the model complexity and assume a uniform model prior. A uniform prior is valid, for example, when choosing specific hyperparameters. Moreover, estimating a model's description length is challenging and an open problem. However, the LML only makes sense for model selection when comparing models of equal complexity. 

## Comparison to Prior Art

We expand on the prior art mentioned in the paper to be able to place it better in the relevant literature.

### CLML vs Cross-validation

As noted in the paper, [Fong and Holmes (2020)‚Äôs ‚ÄúOn the marginal likelihood and cross-validation‚Äù](https://arxiv.org/abs/1905.08737)<d-cite key="fong2020marginal"></d-cite> connects LML and leave-p-out cross-validation:

{% include figure.html path="assets/img/2024-05-07-clml/otmlacv_sec3.1.png" %}

We obtain:

{% include figure.html path="assets/img/2024-05-07-clml/otmlacv_prop2.png" %}

This follows from writing out the terms and swapping the order of the sums.

The paper finally defines something similar to the CLML:

{% include figure.html path="assets/img/2024-05-07-clml/otmlacv_ccv.png" %}

### CLML vs TSE-E

While Lyle et al. (2020) focus on the sum under the learning curve as a proxy for the LML, they only note preliminary empirical evidence for a connection to generalization for DNNs.
In the follow-up paper "Speedy Performance Estimation for Neural Architecture Search" by Ru et al. (2020), not cited here, the authors focus on using similar ideas for model selection.

In "Speedy Performance Estimation for Neural Architecture Search," they explain that:

> we hypothesise that an estimator of network training speed that assigns higher weights to later epochs may exhibit a better correlation with the true generalisation performance of the final trained network" and evaluate training speed estimators which discount the initial terms and sum over a left-truncated learning curve, amongst others. 

The particular estimator is referred to as TSE-E in the paper and shown to have a higher correlation with the generalization performance than the full-length TSE estimator, which is just a proxy for the LML from Lyle et al. (2020). Thus, the findings in this earlier paper are congruent with the CLML.

## CLML vs Validation Loss

A different way of examining what the CLML does is to note that the more data a model has trained with, the more its predictions for different samples will become independent. This follows from the model converging to a delta distribution and is, in particular, valid when using a Laplace approximation to the posterior, as is the case here.

In general, we have:

$$\begin{aligned}
&\log p(\mathcal{D}_{\ge m} \mid \mathcal{D}_{< m}, \mathcal{M}) \\&\quad = \sum_{i=m}^n p(\mathcal{D}_i \mid \mathcal{D}_{< m}, \mathcal{M})  - TC(D_m, \ldots, D_n \mid \mathcal{D}_{< m}, \mathcal{M}), 
\end{aligned}$$

where $$TC(D_m, \ldots, D_n \mid \mathcal{D}_{< m}, \mathcal{M})$$ is the total correlation of the samples in $$\mathcal{D}_{\ge m}$$ and is thus defined:

$$
\begin{aligned}
& TC(D_m, \ldots, D_n \mid \mathcal{D}_{< m}, \mathcal{M}) := \\
&\quad = \log p(D_m, \ldots, D_n \mid \mathcal{D}_{< m}, \mathcal{M}) - \sum_{i=n}^m \log p(D_i \mid \mathcal{D}_{< m}, \mathcal{M}).
\end{aligned}
$$

For $$m\to \infty$$, we have $$TC(D_m, \ldots, D_n \mid \mathcal{D}_{< m}) \to 0$$: The more the model converges, the more the LML is just the cross-entropy of the model trained on $$\mathcal{D}_{< m}$$, evaluated on the held-out data times the number of samples in $$\mathcal{D}_{\ge m}$$.

While this might not be true for DNNs overall---deep ensembles that capture multiple modes come to mind---it is true when using a Laplace approximation.

<aside class="box-warning l-body" markdown="1">
üí° (For large $$m$$) the CLML is just an estimator of the summed validation loss.
</aside>

To be precise, the paper always computes the **average CLML**: it divides the CLML by the number of elements in the $$\mathcal{D}_{\ge m}$$. As such, the (negative) validation loss (as negative cross-entropy estimate using the "validation set" $$\mathcal{D}_{\ge m}$$) and average CLML {% raw %}
$$\frac{\log p(\mathcal{D}_{\ge m} \mid \mathcal{D}_{\lt m}, \mathcal{M})} {|\mathcal{D}_{\ge m}|}$$ {% endraw %} are directly comparable.

In appendix D, the paper explains how many orderings the CLML is averaged over for permutation invariance for the neural architecture experiments on CIFAR-10 & CIFAR-100:

> When D is a large dataset such that $$\mathcal{D}_{< m}$$ and $$\mathcal{D}_{‚â• m}$$ are both sufficiently large, a single permutation may suffice.

The paper notes that the DNN experiments use an 80%/20% split on CIFAR-10 using **only 20 LA samples**: 
A model is trained using 80% of the training data and then evaluated on a 20% held-out "validation set." The CLML of this model is compared to the (BMA) test accuracy or test (cross-entropy) loss of a model trained on 100% of the training data. The LML is finally estimated using the LA of a model trained on 100% of the training data. 

Given the findings in ["Marginal and Joint Cross-Entropies & Predictives for Online Bayesian Inference, Active Learning, and Active Sampling‚Äù](https://arxiv.org/abs/2205.08766)<d-cite key="kirsch2022marginal"></d-cite>, we can hypothesise that 20 LA samples are probably not sufficient to obtain a good CLML estimate (*which corresponds to estimating a joint prediction*). It is more likely that we only obtain a sample of the validation loss (*as sum of marginal predictions*) than a good sample of the CLML. 

<aside class="box-warning l-body" markdown="1">
‚ùì How does the CLML compare to the validation loss? Could we just compute the validation loss directly instead of the CLML? 
</aside>

Sadly, the paper does not compare the validation loss and their CLML estimate. The difference between the two quantities would tell us how well the 20 LA samples account for the total correlation.

<aside class="box-note l-body" markdown="1">
üëÅÔ∏è‚Äçüó®Ô∏è In the updated version of the paper, the authors compute both the CLML and the non-BMA validation loss. Overlaying the two seems to point towards the validation loss and the CLML being exactly equal for one model type (CNNs). The results are not entirely consistent between *v1* and *v2* of the paper. In particular, the CLML in *v2* seems to perform worse than the BMA validation loss in *v1*.
</aside>

## Laplace Approximation for LML vs Sampling for CLML

There might be an apple vs oranges comparison because the LML is computed using the LA while the CLML is computed via sampling:

$$\begin{aligned}
\log p(\mathcal D_{\ge m} \mid \mathcal D_{< m}, \mathcal{M} ) &\approx \log \sum_{k=1}^K \frac{1}{K}\, p(\mathcal{D}_{\ge m} \mid w_k, \mathcal M ) \\
&= \log \sum_{k=1}^K \frac{1}{K}\, \prod_{j=m}^n p(y_j \mid x_j, w_i, \mathcal M ),
\end{aligned}$$

where $$w_k \sim p(\omega_k \mid \mathcal{D}_{<m}, \mathcal M)$$ are parameter samples drawn from the LA for the model trained with a subset of the training data (80% of the training data).

<aside class="box-warning l-body" markdown="1">
‚ùì One could also compute the CLML via the LA: the CLML is just the LML for 100% of the training data when using the model with 80% of the training data as prior. It would be interesting to investigate this estimate, too.
</aside>

## Unexpected Anti-Correlation

Previously, we have highlighted the finding from Section 5 that:

> **correlation of the BMA test log-likelihood with the LML is positive for small datasets and negative for larger datasets, whereas the correlation with the CLML is consistently positive.**

Yet, the more data we have, the more the LML should correlate with the final performance simply because the model's LA will converge, as the following informal reasoning shows:

If we use $$H[Y \mid X, \mathcal{D}_{\le n}, \mathcal{M}]$$ to denote the average cross-entropy (loss) of the model class $$\mathcal M$$ when trained on $$\mathcal D_{\le n}$$ with $$n$$ samples on the test distribution and $$H[\mathcal{D}_{\le n} \mid \mathcal M]$$ to denote the (negative) LML, we expect:

$$
\frac{H[\mathcal{D}_{\le n} \mid \mathcal M]}{n} \overset{n\to\infty}{\to} H[Y \mid X, \mathcal D _{\infty}, \mathcal M].
$$

This is just saying the averaged (negative) LML converges to generalization loss of the model with ‚Äúoptimal‚Äù model parameters as we train on more and more data. In particular, the first few terms will matter less and less. 

Staying informal (but hopefully still correct), we see that for the correlation coefficient, dividing by $$n$$ cancels out:

$$
\begin{aligned}
&\rho_{\frac{H[\mathcal{D}_{\le n} \mid \mathcal M]}{n} ,\, H[Y \mid X, \mathcal D _{\le n}, \mathcal M]} = \\ &\quad =\frac{Cov[\frac{H[\mathcal{D}_{\le n} \mid \mathcal M]}{n} , H[Y \mid X, \mathcal D _{\le n}, \mathcal M]]}{\sqrt{Var[\frac{H[\mathcal{D}_{\le n} \mid \mathcal M]}{n}] \, Var[ H[Y \mid X, \mathcal D _{\le n}, \mathcal M]]}} \\
& \quad = \frac{Cov[H[\mathcal{D}_{\le n} \mid \mathcal M], H[Y \mid X, \mathcal D _{\le n}, \mathcal M]]}{\sqrt{Var[H[\mathcal{D}_{\le n} \mid \mathcal M]] \, Var[ H[Y \mid X, \mathcal D _{\le n}, \mathcal M]]}} = \\
&\quad = \rho_{H[\mathcal{D}_{\le n} \mid \mathcal M], \, H[Y \mid X, \mathcal D _{\le n}, \mathcal M]}  
\end{aligned}
$$

And hence:

$$
\rho_{H[\mathcal{D}_{\le n} \mid \mathcal M], \, H[Y \mid X, \mathcal D _{\le n}, \mathcal M]} = \rho_{\frac{H[\mathcal{D}_{\le n} \mid \mathcal M]}{n}, \, H[Y \mid X, \mathcal D _{\le n}, \mathcal M]} \overset{n\to\infty}{\to} 1
$$

Given this, LML and generalization loss should correlate positively with sufficient data. 

<aside class="box-warning l-body" markdown="1">
‚òù The paper observes anti-correlation here, however, which raises questions about the inferences above, about the quality of the LML estimate, or the amount of data that is needed for the correlation to become positive again.
</aside>

<aside class="box-note l-body" markdown="1">
üëÅÔ∏è‚Äçüó®Ô∏è The authors reply to this, effectively agreeing that the LML indeed has that behavior as can be seen in the fourier model experiment---see (2) in their response below.
</aside>


## DNN Experiments: Validation Loss vs CLML

Lastly, the initially published DNN experiments in the paper did not compute the CLML but the validation loss. This has been fixed in *v2* of the paper.

<aside class="box-note l-body" markdown="1">
‚òù  Given the previous arguments, however, this is not a big issue. Indeed, the expectation is that CLML behaves like the validation loss anyway.
</aside>

<aside class="box-note l-body" markdown="1">
üëÅÔ∏è‚Äçüó®Ô∏è The authors have fixed this bug now and present initial experiment results for the affected figure 5b in the main paper. I compare the new results between *v1* and *v2* below and *find that the **BMA validation loss** performs better than the **CLML**.*
</aside>

The [`logcml_` files in the repository](https://github.com/Sanaelotfi/Bayesian_model_comparison/tree/main/Laplace_experiments/cifar) contain the code to compute the CLML for partially trained models. However, instead of computing

$$
\begin{aligned}
\log p(\mathcal D_{\ge m} \mid \mathcal D_{< m}, \mathcal{M} ) \approx \log \sum_{k=1}^K \frac{1}{K}\, p(\mathcal{D}_{\ge m} \mid w_k, \mathcal M ) \\
= \log \sum_{k=1}^K \frac{1}{K}\, \prod_{j=m}^n p(y_j \mid x_j, w_k, \mathcal M ),
\end{aligned}
$$

the code computes:

$$
\begin{aligned}
&\frac{1}{|\mathcal{D}_{\ge m}|}\,\sum_{j=m}^n \log p(\mathcal D_{j} \mid \mathcal D_{< m}, \mathcal{M} ) \approx \\
&\quad =\frac{1}{|\mathcal{D}_{\ge m}|}\,\sum_{j=m}^n \log \sum_{k=1}^K \frac{1}{K}\, p(y_j \mid x_j, w_k, \mathcal M ),
\end{aligned}
$$

which is the validation cross-entropy loss of the BMA (of the model trained with 80% of the training data).

<details markdown="1"><summary><b>Detailed Code Review</b></summary>    
The high-level [code](https://github.com/Sanaelotfi/Bayesian_model_comparison/tree/c6f0da1d49374c0dda6ee743e5b02bcf3e158e96/Laplace_experiments/cifar/logcml_cifar10_resnets.py#L295) that computes the CLML is:

{% highlight python linenos %}
bma_accuracy, bma_probs, all_ys = get_bma_acc(
    net, la, trainloader_test, bma_nsamples, 
    hessian_structure, temp=best_temp
)
cmll = get_cmll(bma_probs, all_ys, eps=1e-4)
{% endhighlight %}

[`get_bma_acc`](https://github.com/Sanaelotfi/Bayesian_model_comparison/tree/c6f0da1d49374c0dda6ee743e5b02bcf3e158e96/Laplace_experiments/cifar/logcml_cifar10_resnets.py#L149) marginalizes over the LA samples before returning `bma_probs`: 

{% highlight python linenos %}
[...]
for sample_params in params:
    sample_probs = []
    all_ys = []
    with torch.no_grad():
        vector_to_parameters(sample_params, net.parameters())
        net.eval()
        for x, y in loader:
            logits = net(x.cuda()).detach().cpu()
            probs = torch.nn.functional.softmax(logits, dim=-1)
            sample_probs.append(probs.detach().cpu().numpy())
            all_ys.append(y.detach().cpu().numpy())
        sample_probs = np.concatenate(sample_probs, axis=0)
        all_ys = np.concatenate(all_ys, axis=0)
        all_probs.append(sample_probs)

all_probs = np.stack(all_probs)
bma_probs = np.mean(all_probs, 0)
bma_accuracy = (np.argmax(bma_probs, axis=-1) == all_ys).mean() * 100

return bma_accuracy, bma_probs, all_ys
{% endhighlight %}

The important line is #18: `bma_probs = np.mean(all_probs, 0)` which marginalizes over the predictions and returns the BMA prediction for each sample.

Finally, [`get_cmll`](https://github.com/Sanaelotfi/Bayesian_model_comparison/tree/c6f0da1d49374c0dda6ee743e5b02bcf3e158e96/Laplace_experiments/cifar/logcml_cifar10_resnets.py#L170) computes the validation loss for each sample independently (after applying a bit of label smoothing):
{% highlight python linenos %}
def get_cmll(bma_probs, all_ys, eps=1e-4):
    log_lik = 0      
    eps = 1e-4
    for i, label in enumerate(all_ys):
        probs_i = bma_probs[i]
        probs_i += eps
        probs_i[np.argmax(probs_i)] -= eps * len(probs_i)
        log_lik += np.log(probs_i[label]).item()
    cmll = log_lik/len(all_ys)
    
    return cmll
{% endhighlight %}
</details>

The DNN experiments in Section 5 and Section 6 of the paper (*v1*) thus did not estimate the CLML per-se but computed the BMA validation loss of a partially trained model (80%) and find that this correlates positively with the test accuracy and test log-likelihood of the fully trained model (at 100%).
This is not surprising because it is well-known that the validation loss of a model trained 80% of the data correlates positively with the test accuracy (and generalization loss).

# Conclusion

It would be interesting to determine how the CLML estimated via LA compares with the LML estimated via LA. Similarly, comparing the CLML to the validation loss would be interesting.

However, the expectation is that an estimate via sampling will not be different very different from the validation loss simply because sampling-based approaches do not seem to be working well for estimating joint predictive distributions. 

<aside class="box-note l-body" markdown="1">
üëâ Beyond providing a more principled view on using a validation set and a beautiful exposition of previous literature and various pitfalls and use-cases, this paper surfaces exciting research questions---especially in the small-data regime. Several follow-up experiments could provide further insights.
</aside>

---

# Author Response

The following response sadly seems to target the first draft mainly. However, it is also helpful for the final blog post and provides additional context.

<blockquote markdown="1">
Thanks for your interest in our paper and your comments. Here are our comments about the blog as it is currently framed:

(1) Thank you for pointing out a bug in the CLML computation for Figure 5b. We note that this bug is only relevant to a single panel of a single figure in the main text. We have re-run this experiment with the right CLML, and the results, attached here, are qualitatively the same. In summary, it was a very minor part of the paper, and even for that part it did not affect the take-away. We also attach the results of the correlation between the BMA test accuracy and the negative validation loss. You suggest in your post that the validation loss might correlate better with the BMA test accuracy than the CLML given that we use 20 samples for NAS. Our empirical results show the opposite conclusion. Additionally, we are not suggesting the CLML as a replacement to cross-validation but rather as a minor way to modify the LML for improvements in predicting generalization. Finally, we attach results for different sample sizes (20 samples vs. 100 samples) to address your comments on the sample size used to estimate the CLML. As we can see in the figure, the Spearman correlation factor is quite similar. 20 samples appears to provide a reasonable estimate of the CLML for these purposes, and is different from validation loss.

{% capture max-width %}
" style="max-width: 20em;
{% endcapture %}
{% include figure.html path="assets/img/2024-05-07-clml/rebuttal_1.png" max-width=max-width %}
{% include figure.html path="assets/img/2024-05-07-clml/rebuttal_2.png" max-width=max-width %}
{% include figure.html path="assets/img/2024-05-07-clml/rebuttal_3.png" max-width=max-width %}

(2) Your post currently opens by suggesting that there is something wrong with our experiments, likely either an LML approximation or a CLML issue, because we note that the LML correlates more poorly with generalization for larger datasets (where ‚Äúlarge‚Äù is relative in the context of a specific experiment). A few points here: (i) this result is actually completely expected. The LML is in fact non-monotonic in how well it predicts generalization. For small datasets, the prior should be reasonably predictive of generalization. For intermediate datasets, the first terms in the LML decomposition have a negative effect on the correlation with generalization. For asymptotically large datasets, the first terms have a diminishing effect, and we get a consistent estimator; (ii) almost all of our experiments are exact, and we see this behaviour in the exact experiments for the Fourier model. For example, for the Fourier feature experiment in Fig 4(d), LML picks the better generalizing model for n < 50 and n > 296. For n in [50, 296] it picks the wrong model. For large neural network models, it is reasonable that the exact LML could pick the wrong model for CIFAR-sized datasets. (iii) any potential issues with the CLML are not relevant to these considerations, which are about the behaviour of the LML.

(3) Your post currently suggests that issues with approximate inference could be responsible for our take-aways, rather than issues with the LML in general. But as we note in (2), almost all of our experiments use the exact LML and CLML: the density model, Fourier features, Gaussian processes, and deep learning exps on DKL, and there was never any bug associated with CLML computation in these experiments. The takeaways for the Laplace experiments are consistent with the exact experiments, and also expected, as above. While it‚Äôs true that the CLML can be estimated more effectively than the LML for the Laplace experiments, this is actually an advantage of the CLML that we note in the paper. The LML results also stand on their own, as we discuss above.

(4) Your post places a lot of importance on Figure 5, as if it is the main result of the paper and our main ‚ÄúDNN‚Äù experiments. We stand by the results of Figure 5, but it is a relatively minor component of the paper. As we‚Äôve mentioned most of our results are exact, including our DKL experiments, which are certainly the most substantial DNN experiments, with practically exciting results for transfer and few-shot learning. The DKL experiments are actually where we expect the CLML to be practically useful, and currently they seem to be overlooked in the post.

(5) The blog seems to question the learning curve experiments, but these experiments in Figure 4 are exact, with no Laplace approximation, and relatively straightforward.

(6) Your post seems to be negative about the CLML, presenting its similarity with cross-validation as a potential drawback, and implying the skepticism about the CLML should affect the interpretation of our take-aways. Two points here: (i) as above, the CLML is independent of most of our take-aways, which are about the properties of the LML; (ii) our goal with the CLML was not to introduce something starkly different from cross-validation, but to show how a very minor modification to the LML could improve alignment with generalization. Moreover, the DKL CLML results are quite promising as an efficient way to do gradient based estimation of a large number of hyperparameters.

(7) The blog opens as if it is leading up to some fatal flaw. But as above, (i) the LML considerations are independent of the CLML, (ii) most of the experiments are exact, (iii) the trends for the exact and approximate inference procedures are the same and are naturally understandable and explainable, such as the non-monotonic trend in how well the LML correlates with generalization, and (iv) the CLML bug only affected Figure 5, panel b, and when it‚Äôs corrected the qualitative take-away is the same as before.

We appreciate your interest and effort in reading the paper, and we think your questions will improve the clarity of the paper, which we have updated with an acknowledgement to you. Given the above considerations, we do think there would need to be substantial revisions to the blog post to accurately and fairly reflect the paper. We would appreciate being able to see the revisions before it‚Äôs posted.

Best wishes,\\
Sanae, Pavel, Greg, Micah, Andrew
</blockquote>

##  Ablation: CLML vs. BMA Validation Loss vs. (non-BMA) Validation Loss

Let us examine the new results: 

In the three panels below, two panels show test accuracy vs. validation loss; one shows test accuracy vs. CLML. The left-most panel is the BMA test accuracy vs. (negative) BMA validation loss, the middle panel is vs. the CLML, and the right-most panel is vs. the (negative) non-BMA validation loss. 

Note that the left-most panel is from *v1*, which was accidentally computing the BMA validation loss, and whose axis label I have adapted from *v1* for clarity. The two other plots are from *v2* after fixing the bug. See commits [here](https://github.com/Sanaelotfi/Bayesian_model_comparison/commit/a579aa292723dc20a6105ec8f4fff1045dd9a9fd) for fixing the CLML estimation and [here](https://github.com/Sanaelotfi/Bayesian_model_comparison/commit/3fa8ca2ecb314ee881f6c95a602ef58b9ccd3620) for computing the non-BMA validation loss. 

{% capture width %}
" style="width: 20em;
{% endcapture %}
<div class="row mt-3">
  <div class="col-sm mt-3 mt-md-0">
    {% include figure.html path="assets/img/2024-05-07-clml/bmsmlg_bma_validation_loss.svg" class="img-fluid" caption="BMA Neg Validation Loss" width=width %}
  </div>
  <div class="col-sm mt-3 mt-md-0">
    {% include figure.html path="assets/img/2024-05-07-clml/bmsmlg_clml.svg" class="img-fluid" caption="CLML" width=width %}
  </div>
</div>
<div class="row mt-3">
  <div class="col-sm mt-3 mt-md-0">
    {% include figure.html path="assets/img/2024-05-07-clml/bmsmlg_validation_loss.svg" class="img-fluid" caption="Validation Loss" width=width %}
  </div>  
{% capture width %}
" style="width: 5em;
{% endcapture %}
  <div class="col-sm mt-3 mt-md-0">
    {% include figure.html path="assets/img/2024-05-07-clml/bmsmlg_plot_legend.svg" class="img-fluid" caption="Leg" width=width %}
  </div>
</div>

At first glance, there might be an observer effect in the experiments for the validation loss. The BMA validation loss in *v1* performs better than the CLML in *v2*, while the non-BMA validation loss in *v2* underperforms the CLML in *v2*. I asked the authors about it: they pushed the respective code (see link above) and explained that the updated, right-most panel computes the **non-BMA** validation loss, i.e., without LA samples. To me, it seems surprising that there is such a difference between the non-BMA validation loss and BMA validation loss: *the non-BMA validation loss is more than one nat worse on average than the BMA validation loss, based on visual inspection*. Note that the plots here and in the paper compute the average CLML and average validation loss and are thus directly comparable.

The authors said in their response that:

> You suggest in your post that the validation loss might correlate better with the BMA test accuracy than the CLML given that we use 20 samples for NAS. Our empirical results show the opposite conclusion.

This is only partially true. 
The BMA validation loss (which was accidentally computed in *v1* instead of the CLML) correlates very well with the BMA test accuracy.
This is not surprising given that this is the frequentist purpose of using validation sets. If validation sets were not correlating well with the test accuracy, we would not be using them in practice. ü§ó As such, I wonder why the non-BMA validation loss negatively correlates with the BMA test accuracy for ResNets and overall in the *v2* results.
Thus, only the non-BMA validation loss supports the opposite conclusion in *v2* of the paper and in the authors' response. 

Yet what is also surprising is how well the BMA validation loss does vs. the CLML:

<aside class="box-error l-body" markdown="1">
üî• The BMA validation loss correlates even better than the CLML with BMA test accuracy: the Spearman's rank correlation is better for the BMA validation loss than for the CLML across all $$\lambda$$s.

Does this mean we should use the BMA validation loss rather than estimating the CLML for DNNs?
</aside>

## Ablation: LA Sample Size

Secondly, when we compare the reported values between BMA validation loss and CLML, we notice that the CLML is lower than the BMA validation loss by half a nat for $$\lambda=10^2$$ and generally for CNNs.

<aside class="box-note l-body" markdown="1">
üëâ I would expect that the (average) CLML to be better (higher) than the negative BMA validation loss and not worse (lower) because the CLML ought to be able to compress the validation set better using the joint predictive distribution than the marginal predictive distribution for the validation loss.  


Intuitively, the CLML can take the validation data into account in a way that the validation loss cannot given the sequential conditioning of the former. 

Hence, the CLML should generally be lower-bounded by the (BMA) validation loss and upper-bounded by the (BMA) test loss up to sample variance, assuming there are not many outliers in the data.
The fact that the CLML is worse points toward the LA samples not being able to capture the implicit posterior in the joint predictive distribution. (This is a bit handwavy, but I think it is a reasonable interpretation and in line with our note on marginal and joint predictive distributions.)
</aside>

However, it seems, even though the new experiments in *v2* are supposed to reproduce the ones from *v1*, and I assume that the same model checkpoints were used for re-evaluation (as retraining is not necessary), both CLML and non-BMA validation loss are off by about half a nat for the CNNs. As such, the above consideration might hold but might not provide the answer here.

Instead, I have overlaid the non-BMA validation loss and the CLML plots, both from *v2*, with a "difference blend": it shows the absolute difference between the colors for overlapping data points (the circles üî¥ and triangles üî∫), leading to black where there is a match, negative (green-ish) color for CLML, and positive (sepia) color for validation losses. I used the background grids to match the plots but hid the ones from CLML afterward---as such, the strong overlay is because the values are so close.

{% capture width %}
" style="width: 25em;
{% endcapture %}
{% include figure.html path="assets/img/2024-05-07-clml/bmsmlg_difference_overlay_plot.svg" class="img-fluid" width=width %}


Surprisingly---or rather as predicted when the LA does not really do much---it turns out that the validation loss for the CNNs (üî¥) mostly fully matches the estimated CLML with 20 LA samples following a visual inspection. To be more precise, either the models have already sufficiently converged, or the CLML estimate is not actually capturing the correlations between points and thus ends up being very similar to the validation loss.

<aside class="box-warning l-body" markdown="1">
As said before, it would be great to actually compute the difference between CLML and BMA validation loss explicitly. Two things are not clear to me given the results in *v1* and *v2* I have examined:

1. Why is there a difference in nats between BMA validation loss in *v1* and the non-BMA validation loss and CLML in *v2*? Given that CLML matches the non-BMA validation loss for CNNs in *v2*, I would expect the BMA validation loss to be close to that, too.

2. Why is there different behavior for ResNets (üî∫) in the BMA validation loss in *v1* and the CLML in *v2* compared to the non-BMA validation loss in *v2*? The low correlation coefficients of the non-BMA validation loss seem to come from that, as *v2* also explains in an added paragraph in appendix H.

</aside>

{% capture width %}
" style="width: 25em;
{% endcapture %}
{% include figure.html path="assets/img/2024-05-07-clml/rebuttal_3.png" class="img-fluid" width=width %}


This changes my interpretation of the sample ablation in the author's response. The ablation shows no difference between 20 and 100 LA samples, with 100 LA even samples having a slightly lower rank correlation. So it seems x5 more LA samples are not enough to make a difference, or the Laplace posterior cannot capture the posterior as well as we would like. It would be interesting to examine this further. For the previously mentioned note, we have run some toy experiments on MNIST with 10,000 MC Dropout samples previously and did not find good adaptation. Obviously, the LA is not MC Dropout, and this is speculation, but it seems in line with my prior experience.

Tweets by [@RobertRosenba14](https://twitter.com/RobertRosenba14) and [@stanislavfort](https://twitter.com/stanislavfort) also explain why sampling might not be fruitful for estimating joint distributions and compute the CLML in high-dimensional spaces---see [here](https://twitter.com/stanislavfort/status/1529865444701577216) and [here](https://twitter.com/RobertRosenba14/status/1517465854157500419).

Given the above, it is fair to say that 
the estimate of the CLML is probably not as good as hoped, and further experiments might be needed to tease out when the CLML provides more value than the BMA validation loss. Note, however, that this question has not been explicitly examined in the paper. Instead, the paper compares LML and CLML with distinct estimation methods for DNNs.

<aside class="box-error l-body" markdown="1">
üî• Given the overall results of CLML vs. LML and the observation that the BMA validation loss seems to work very well for DNNs, even better than the CLML estimate, it might be worth ablating the performance of the validation loss for DKL and other methods as a separate research question. Then, we can see what we gain from the CLML exactly. 
*Maybe a validation set "is all you need" - after all - the validation loss is easier to estimate.* But then, we knew that validation sets work really well for DNNs all along, despite them being "less principled," didn't we?
</aside>
