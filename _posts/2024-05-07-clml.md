---
layout: distill
title: >-
  A Critical Exploration of the Conditional Log Marginal Likelihood
description: >-
  This blog post examines one of the contributions of the Outstanding ICML 2022 paper 'Bayesian Model Selection, the Marginal Likelihood, and Generalization,' which itself critically examines the role of the log marginal likelihood (LML) and introduces the conditional log marginal likelihood (CLML) as a potential improvement for model selection. 
  The post delves into the key ideas, methodologies, and findings across various machine learning settings, with a particular focus on the effectiveness of CLML in deep learning scenarios. It also incorporates insights from discussions with the authors and analyzes updates made to the paper, including a bug fix in the CLML computation for DNN experiments and additional experiments comparing CLML, BMA validation loss, and non-BMA validation loss.
  The post goes beyond summarization by comparing CLML to existing techniques such as cross-validation and training speed estimation, examining the relationship between LML and generalization performance, and discussing the limitations of the Laplace approximation for LML and CLML estimation. 
  The post critically assesses the novelty and utility of CLML in deep learning model selection based on the updated experiments and highlights the need for further research.'
date: 2024-05-07
future: true
htmlwidgets: true
tags:
- Bayesian Neural Network
- Generalization
- Model Information
- Marginal Likelihood
- Laplace Approximation
- Model Selection

# Anonymize when submitting
authors:
  - name: Anonymous

# authors:
#   - name: Albert Einstein
#     url: "https://en.wikipedia.org/wiki/Albert_Einstein"
#     affiliations:
#       name: IAS, Princeton
#   - name: Boris Podolsky
#     url: "https://en.wikipedia.org/wiki/Boris_Podolsky"
#     affiliations:
#       name: IAS, Princeton
#   - name: Nathan Rosen
#     url: "https://en.wikipedia.org/wiki/Nathan_Rosen"
#     affiliations:
#       name: IAS, Princeton

# must be the exact same name as your blogpost
bibliography: 2024-05-07-clml.bib  

# Add a table of contents to your post.
#   - make sure that TOC names match the actual section names
#     for hyperlinks within the post to work correctly. 
#   - please use this format rather than manually creating a markdown table of contents.
toc:
  - name: "Introduction"
    subsections:
    - name: "Paper Details"
  - name: "Why?"
  - name: "How?"
    subsections:
    - name: "Use-cases"
    - name: "General Pitfalls"
    - name: "Estimating LML via the Laplace Approximation"
    - name: "Conditional Marginal Likelihood"
    - name: "On Training Speed and Learning Curves"
    - name: "Model Selection"
  - name: "And?"
    subsections:
    - name: "Ignoring Model Priors"
    - name: "Comparison to Prior Art"
    - name: "CLML vs Validation Loss"
    - name: "Laplace Approximation for LML vs Sampling for CLML"
    - name: "Unexpected Anti-Correlation"
    - name: "DNN Experiments: Validation Loss vs CLML"
  - name: "Author Response"
    subsections:
    - name: "Ablation: CLML vs. BMA Validation Loss vs. (non-BMA) Validation Loss"
    - name: "Ablation: LA Sample Size"
  - name: "Conclusion"
  
# Below is an example of injecting additional post-specific styles.
# This is used in the 'Layouts' section of this post.
# If you use this post as a template, delete this _styles block.
_styles: >
  .fake-img {
    background: #bbb;
    border: 1px solid rgba(0, 0, 0, 0.1);
    box-shadow: 0 0px 4px rgba(0, 0, 0, 0.1);
    margin-bottom: 12px;
  }
  .fake-img p {
    font-family: monospace;
    color: white;
    text-align: left;
    margin: 12px 0;
    text-align: center;
    font-size: 16px;
  }
  d-article .box-note,
  d-article .box-warning,
  d-article .box-error,
  d-article .box-important {
    padding: 15px 15px 15px 10px;
    margin: 20px 20px 20px 5px;
    border: 1px solid #eee;
    border-left-width: 5px;
    border-radius: 5px 3px 3px 5px;
  }
  d-article .box-note {
    background-color: #eee;
    border-left-color: #2980b9;
  }
  d-article .box-warning {
    background-color: #fdf5d4;
    border-left-color: #f1c40f;
  }
  d-article .box-error {
    background-color: #f4dddb;
    border-left-color: #c0392b;
  }
  d-article .box-important {
    background-color: #d4f4dd;
    border-left-color: #2bc039;
  }
  html[data-theme='dark'] d-article .box-note {
    background-color: #555555;
    border-left-color: #2980b9;
  }
  html[data-theme='dark'] d-article .box-warning {
    background-color: #7f7f00;
    border-left-color: #f1c40f;
  }
  html[data-theme='dark'] d-article .box-error {
    background-color: #800000;
    border-left-color: #c0392b;
  }
  html[data-theme='dark'] d-article .box-important {
    background-color: #006600;
    border-left-color: #2bc039;
  }
  d-article aside *,
  d-article details summary,
  d-article figcaption {
    color: var(--global-text-color) !important;
  }
  d-article article p {
    text-align: justify;
    text-justify: inter-word;
    -ms-hyphens: auto;
    -moz-hyphens: auto;
    -webkit-hyphens: auto;
    hyphens: auto;
  }
  d-article aside {
    border: 1px solid #aaa;
    border-radius: 4px;
    padding: .5em .5em 0;
    font-size: 100%;
  }
  d-article aside p:first-child {
      margin-top: 0;
  }
  d-article details {
    border: 1px solid #aaa;
    border-radius: 4px;
    padding: .5em .5em 0;
  }
  d-article summary {
    font-weight: bold;
    margin: -.5em -.5em 0;
    padding: .5em;
    display: list-item;
  }
  d-article details[open] {
    padding: .5em;
  }
  d-article details[open] summary {
    border-bottom: 1px solid #aaa;
    margin-bottom: .5em;
  }
  html[data-theme='dark'] d-article blockquote {
    border-left-color: #f1c40f;
  }
---


{% raw %}
<div style="display: none;">
$$\require{mathtools}
\DeclareMathOperator{\opExpectation}{\mathbb{E}}
\newcommand{\E}[2]{\opExpectation_{#1} \left [ #2 \right ]}
\newcommand{\simpleE}[1]{\opExpectation_{#1}}
\newcommand{\MidSymbol}[1][]{\:#1\:}
\newcommand{\given}{\MidSymbol[\vert]}
\DeclareMathOperator{\opmus}{\mu^*}
\newcommand{\IMof}[1]{\opmus[#1]}
\DeclareMathOperator{\opInformationContent}{H}
\newcommand{\ICof}[1]{\opInformationContent[#1]}
\newcommand{\xICof}[1]{\opInformationContent(#1)}
\DeclareMathOperator{\opEntropy}{H}
\newcommand{\Hof}[1]{\opEntropy[#1]}
\newcommand{\xHof}[1]{\opEntropy(#1)}
\DeclareMathOperator{\opMI}{I}
\newcommand{\MIof}[1]{\opMI[#1]}
\DeclareMathOperator{\opTC}{TC}
\newcommand{\TCof}[1]{\opTC[#1]}
\newcommand{\CrossEntropy}[2]{\opEntropy(#1 \MidSymbol[\Vert] #2)}
\DeclareMathOperator{\opKale}{D_\mathrm{KL}}
\newcommand{\Kale}[2]{\opKale(#1 \MidSymbol[\Vert] #2)}
\DeclareMathOperator{\opJSD}{D_\mathrm{JSD}}
\newcommand{\JSD}[2]{\opJSD(#1 \MidSymbol[\Vert] #2)}
\DeclareMathOperator{\opp}{p}
\newcommand{\pof}[1]{\opp(#1)}
\newcommand{\hpof}[1]{\hat{\opp}(#1)}
\newcommand{\pcof}[2]{\opp_{#1}(#2)}
\newcommand{\hpcof}[2]{\hat\opp_{#1}(#2)}
\DeclareMathOperator{\opq}{q}
\newcommand{\qof}[1]{\opq(#1)}
\newcommand{\hqof}[1]{\hat{\opq}(#1)}
\newcommand{\qcof}[2]{\opq_{#1}(#2)}
\newcommand{\varHof}[2]{\opEntropy_{#1}[#2]}
\newcommand{\xvarHof}[2]{\opEntropy_{#1}(#2)}
\newcommand{\varMIof}[2]{\opMI_{#1}[#2]}
\newcommand{\w}{\boldsymbol{\theta}}
\newcommand{\W}{\boldsymbol{\Theta}}
\newcommand{\h}{\boldsymbol{\phi}}
\newcommand{\H}{\boldsymbol{\Phi}}
\DeclareMathOperator{\opf}{f}
\newcommand{\fof}[1]{\opf(#1)}
\newcommand{\xNset}{(\x_n)_{n=1}^N}
\newcommand{\XNtuple}{(\X_n)_{n=1}^N}
\newcommand{\XNset}{\{\X_n\}_{n=1}^N}
\newcommand{\xNset}{\{\x_n\}_{n=1}^N}
\newcommand{\XNsetk}{\{\X_n\}_{n=N-k+1}^N}
\newcommand{\XNkset}{\{\X_n\}_{n=1}^{N-k}}
\newcommand{\XNoset}{\{\X_n\}_{n=1}^{N-1}}
\newcommand{\y}{y}
\newcommand{\Y}{Y}
\newcommand{\L}{\boldsymbol{L}}
\newcommand{\x}{\boldsymbol{x}}
\newcommand{\X}{\boldsymbol{X}}
\newcommand{\pdata}[1]{\hpcof{\text{data}}{#1}}
\newcommand{\normaldist}[1]{\mathcal{N}(#1)}
$$
</div>
{% endraw %}

## Introduction

The paper "Bayesian Model Selection, the Marginal Likelihood, and Generalization," accepted as Long Oral at ICML 2022, examines the importance and challenges of model selection in machine learning, focusing on the log marginal likelihood (LML) and proposing a variant, the conditional log marginal likelihood (CLML). The authors argue that while LML is a useful tool for hypothesis testing, it may not be the best metric for predicting the generalization performance of trained models or learning hyperparameters. They introduce CLML as a potential improvement and demonstrate its effectiveness across various settings, including density models, Fourier features, Gaussian Processes, and deep neural networks.

The paper presents a comprehensive exposition of the limitations of LML and the strengths of CLML, supported by insightful toy experiments and larger experiments on CIFAR-10 and CIFAR-100. The findings are contrasted with prior work, such as Lyle et al. (2020) "A Bayesian Perspective on Training Speed and Model Selection"<d-cite key="lyle2020bayesian"></d-cite> and Immer et al. (2021)<d-cite key="immer2021scalable"></d-cite>, highlighting the need for a more comprehensive understanding of Bayesian methods in model evaluation.

This review is structured in two parts:

- The first half summarizes the key ideas and findings of the paper, while also introducing the basic concepts from Bayesian deep learning and model selection that are relevant to the paper's contributions.
- The second half critically engages with the concepts, particularly the effectiveness of CLML for deep neural networks (DNNs), and draws comparisons to cross-validation and validation sets & losses in DNN experiments.

While the paper covers a wide range of settings, including deep kernel learning and Gaussian Processes, this review focuses primarily on the implications for deep learning.

<aside class="box-note l-body" markdown="1">
📑 **TL;DR:** The proposed CLML provides a Bayesian motivation for using validation sets with deep neural networks in the large-data regime. The experiments in the different versions of the paper show that while the CLML outperforms the LML, the validation loss using Bayesian model averaging provides an even stronger signal than the (sampled) CLML estimate for DNNs. This raises questions about the practical advantages of CLML over traditional validation methods in deep learning and machine learning.
</aside>

<aside class="box-note l-body" markdown="1">
👁️‍🗨️ This review is based on both the [v1](https://arxiv.org/abs/2202.11678v1) and [updated JMLR version](https://www.jmlr.org/papers/volume24/22-0720/22-0720.pdf) of the paper "Bayesian Model Selection, the Marginal Likelihood, and Generalization" by Lotfi et al. The authors have addressed some of the concerns raised in the initial drafts of this review, expanding the discussion section and providing additional experiments in the updated version. The review has been updated to reflect these changes and to provide a more general .
</aside>

### Paper Details

| | Bayesian Model Selection, the Marginal Likelihood, and Generalization |
| :---        |    :----  |
| by | Sanae Lotfi, Pavel Izmailov, Gregory Benton, Micah Goldblum, Andrew Gordon Wilson |
|    | [**Accepted to ICML 2022 as Long Oral**](https://icml.cc/Conferences/2022/Schedule?showEvent=17992) |
| Paper Link | <https://arxiv.org/abs/2202.11678> |
| Code | <https://github.com/Sanaelotfi/Bayesian_model_comparison> |

<details markdown="1"><summary>BibTex</summary> 
```bibtex
@inproceedings{lotfi2022bayesian,
  title={Bayesian Model Selection, the Marginal Likelihood, and Generalization},
  author={Lotfi, Sanae and Izmailov, Pavel and Benton, Gregory and Goldblum, Micah and Wilson, Andrew Gordon},
  booktitle={International Conference on Machine Learning},
  pages={14223--14247},
  year={2022},
  organization={PMLR}
}
```
</details>

## (Bayesian) Model Selection

In our daily lives, we're often faced with choices that require us to sift through competing explanations or decisions. Imagine you hear your doorbell ring. You might think it's the delivery you've been waiting for, a neighbor dropping by, or perhaps you didn't hear anything at all, and it was just your imagination. In deciding between these options, you're likely to lean towards the simplest explanation that aligns with your expectations—say, the long-awaited delivery. This inclination towards simplicity has a formal counterpart in scientific discovery and machine learning, known as [Occam’s razor](https://en.wikipedia.org/wiki/Occam%27s_razor):

<aside class="box-important l-body" markdown="1">
*Occam’s razor* is the principle that, all else being equal, the simplest explanation tends to be the right one.
</aside>

This concept is brilliantly illustrated using [an example from chapter 28](https://www.inference.org.uk/itprnn/book.pdf#page=355) of David MacKay’s seminal book, ["Information Theory, Inference, and Learning Algorithms”](http://www.inference.org.uk/mackay/itila/book.html), where the essence of selecting between models based on their evidence is laid out succinctly.

{% capture caption %}
Excerpt from page 343 in David MacKay’s "Information Theory, Inference, and Learning Algorithms.”
{% endcapture %}
{% include figure.html path="assets/img/2024-05-07-clml/mackay_343.png" zoomable=True class="img-fluid" caption=caption alt="Occam’s razor --- How many boxes are in the picture (figure 28.1)? In particular, how many boxes are in the vicinity of the tree? If we looked with x-ray spectacles, would we see one or two boxes behind the trunk (figure 28.2)? (Or even more?) Occam’s razor is the principle that states a preference for simple theories. ‘Accept the simplest explanation that fits the data’. Thus according to Occam’s razor, we should deduce that there is only one box behind the tree. Is this an ad hoc rule of thumb? Or is there a convincing reason for believing there is most likely one box? Perhaps your intuition likes the argument ‘well, it would be a remarkable coincidence for the two boxes to be just the same height and colour as each other’. If we wish to make artificial intelligences that interpret data correctly, we must translate this intuitive feeling into a concrete theory." 

title="Occam’s razor --- How many boxes are in the picture (figure 28.1)? In particular, how many boxes are in the vicinity of the tree? If we looked with x-ray spectacles, would we see one or two boxes behind the trunk (figure 28.2)? (Or even more?) Occam’s razor is the principle that states a preference for simple theories. ‘Accept the simplest explanation that fits the data’. Thus according to Occam’s razor, we should deduce that there is only one box behind the tree. Is this an ad hoc rule of thumb? Or is there a convincing reason for believing there is most likely one box? Perhaps your intuition likes the argument ‘well, it would be a remarkable coincidence for the two boxes to be just the same height and colour as each other’. If we wish to make artificial intelligences that interpret data correctly, we must translate this intuitive feeling into a concrete theory."%}

How can we express this using mathematics?

**From Philosophical Principle to Mathematical Statement**

Let's first connect Occam's razor to **Maximum Likelihood Estimation (MLE)** before diving into the background and (Bayesian) model selection.

In information theory, the information content of an event $$x$$ is defined as $$-\log_2 \pof{x}$$---this is also called *Shannon's information content*---where $$\pof{x}$$ is the probability of that event occurring according to a given model, and I use the basis $$2$$ for logarithms and thus measure information in bits. I will drop the basis of the logarithm for the rest of the post. The information content measures the optimal encoding length in bits for the event $$x$$ under the model specified by the probability distribution $$\pof{\cdot}$$. In the context of probabilistic modeling, variables that cannot be directly observed are called latent variables. Occam's razor suggests that we should prefer simpler explanations for latent variables, given the observed data.

Consider a model with a latent variable $$z$$ and observed data $$x$$. The model specifies a probability distribution $$\pof{z \given x}$$. According to Occam's razor, we prefer simpler explanations, which correspond to smaller values of $$-\log \pof{z \given x}$$. Using Bayes' theorem, we can rewrite this as:

$$-\log \pof{z \given x} = -\log \pof{x \given z} - \log \pof{z} + \log \pof{x}.$$

Given that $$\pof{x}$$ is independent of $$z$$, we can omit it from our optimization goal. Additionally, if we posit a uniform or non-informative prior for $$z$$, implying that all potential values of $$z$$ are equally probable before observing $$x$$, then $$\pof{z}$$ becomes a constant and can also be disregarded in our objective. This simplifies our preference to:

$$\text{minimize } -\log \pof{x \given z}.$$

Equivalently, we can maximize $$\pof{x \given z}$$, which is the likelihood of the observed data $$x$$ given the latent variable $$z$$. 

<div class="l-gutter" markdown="1" style="height: 0px">
<aside>
As MacKay notes, \(\pof{x \given z}\) as a function in \(z\) is called a likelihood, while \(\pof{x \given z}\) as a function in \(x\) is called a probability.
</aside>
</div>

When making a decision and selecting a single value for $$z$$, this leads to the maximum likelihood estimation (MLE) approach.

In summary, the connection between Occam's razor and MLE relies on the following assumptions:

1. Shannon's information content is a valid measure of complexity.
2. The prior distribution for the latent variables is uniform (or uninformative).
3. Simpler explanations, as measured by the information content, are preferred (Occam's razor).

Under these assumptions, the preference for simpler explanations leads to the MLE approach, where the most likely value of the latent variable is chosen given the observed data.

Optimizing the MLE is a common approach and the most straightforward when we have a parametric model because we can directly optimize the likelihood function. Still, this is not easy for deep learning models because they have a large number of parameters and the loss function is non-convex.

### Maximum Likelihood and Maximum-a-Posteriori Estimate

However, the assumption of a uniform or non-informative prior for the latent variables is not always valid or desirable. In many cases, we have prior knowledge about the latent variables that should be incorporated into our model. This leads us to consider the **Maximum A Posteriori (MAP) Estimation** as an alternative to MLE, which specifically addresses this scenario.

In MAP estimation, $$\pof{z}$$ is not constant, so we cannot drop it---we can still drop $$\pof{x}$$, however---and maximize the joint distribution $$\pof{z, x}$$, or equivalently:

$$\text{minimize } -\log \pof{x, z}=-\log \pof{x \given z} - \log \pof{z}.$$

Before we go further, let us delve into some important background notation.

## Background: Information-Theoretic Notation and Concepts

TK ask if I should cite the other post TK

Information theory deals with the communication of information<d-footnote>See the excellent <a href="https://colah.github.io/posts/2015-09-Visual-Information/">"Visual Information Theory"</a> by Chris Olah for a visual introduction to information theory.</d-footnote>. In this post, we use a unified information-theoretic notation to express various quantities related to probability distributions and their relationships<d-footnote>It largely follows "<a href="https://arxiv.org/abs/2106.12062">A Practical & Unified Notation for Information-Theoretic Quantities in ML</a>".</d-footnote>. Here are some key concepts we will use:

The **information content** of an event $$x$$ is denoted as $$\Hof{x}$$ and is defined as $$-\log_2 \pof{x}$$. It represents the minimum amount of information needed to describe the occurrence of $$x$$ given an underlying probability distribution.
In machine learning, this information content is often used as a minimization objective, represented as the negative log-likelihood or cross-entropy when averaged over a dataset.

The **entropy** $$\Hof{X}$$ of a random variable $$X$$ is the expectation of its information content:

$$
\Hof{X} \triangleq \E{\pof{x}}{\Hof{x}} = \E{\pof{x}}{-\log \pof{x}}.
$$

The entropy measures the average amount of information needed to describe the random variable $$X$$. It provides a measure of uncertainty or randomness associated with $$X$$. We can similarly define the entropy of a conditional distribution $$\Hof{X \given Y}$$ and the joint entropy $$\Hof{X, Y}$$.

The **mutual information** $$\MIof{X;Y}$$ between two random variables $$X$$ and $$Y$$ is a measure of the amount of information that one random variable contains about the other. It is defined as:

$$
\begin{aligned}
\MIof{X;Y} & \triangleq \Hof{X} - \Hof{X \given Y} \\
&= \Hof{Y} - \Hof{Y \given X} \\
&= \Hof{X} + \Hof{Y} - \Hof{X, Y}.
\end{aligned}
$$

We will also use the **Kullback-Leibler divergence** $$\Kale{\pof{X}}{\qof{X}}$$ and the **cross-entropy** $$\CrossEntropy{\pof{X}}{\qof{X}}$$:

$$
\begin{aligned}
\CrossEntropy{\pof{X}}{\qof{X}} & = \E{\pof{x}}{-\log \qof{x}}\\
\Kale{\pof{X}}{\qof{X}} & = \CrossEntropy{\pof{X}}{\qof{X}} - \Hof{X}
\end{aligned}
$$

The cross-entropy quantifies the average number of bits needed to encode samples drawn from the true distribution $$\pof{X}$$ using a different distribution $$\qof{X}$$. The Kullback-Leibler divergence is a measure of the difference between two probability distributions and captures the additional bits needed to encode samples from $$\pof{X}$$ compared to encoding them using the true distribution $$\qof{X}$$.

### Expressing Occam's Razor in Information-Theoretic Terms

Taking this notation into account, we can express Occam's razor as:

$$\text{prefer small } \Hof{z \given x},$$

where $$Z$$ is the latent variable and $$X$$ is the observed data. Note that $$x$$ and $$z$$ are individual realizations of the random variables $$X$$ and $$Z$$, respectively.

The MLE and MAP objectives are accordingly:

$$\text{minimize } \Hof{x \given z}$$ for MLE and $$\Hof{x, z}$$ for MAP.

This just measures the number of bits we need to encode the observed data given the latent variable for MLE and the number of bits to encode both the observed data and the latent variable for MAP. This relates Occam's razor to Kolmogorov complexity and the minimum description length principle<d-footnote>See the excellent <a href="https://en.wikipedia.org/wiki/Minimum_description_length">Wikipedia article on Minimum Description Length</a> for more details.</d-footnote>.

## Hyperparameter Learning and Model Selection

In machine learning, we often also need to determine the best hyperparameters for a model, or select a model (architecture) to use from several discrete options. The latter is known as **model selection**. The overall goal is to find the hyperparameters or model that generalizes best to new, unseen data.

We can view both as a random variable $$\H$$, that either represents the model choice as categorical distribution, or the hyperparameters as a continuous distribution. Thus, $$\H$$ is "just" (another) latent variable that we need to infer from the data. We will keep using $$\x$$ for data points. Often, we have a side channel and then we use $$\y$$ for the predictions and $$\x$$ for the side channel, but we will not need this distinction here, so I'll stick to $$\x$$. 

All in all, Occam's razor again gives us:

$$\text{prefer small } \Hof{\x \given \H}.$$

### Estimation Difficulties

Making this more complex, we usually also have parameters $$\W$$ for a given set of hyperparameters $$\H$$ that we need to "learn." 
For given $$\w$$ and $$\h$$, we can easily compute the likelihood $$\pof{\x \given \w, \h}$$, but to get to $$\pof{\x \given \h}$$, we need to integrate over all possible $$\w$$:

$$\pof{\x \given \h} = \int \pof{\x \given \w, \h} \pof{\w \given \h} \, d\w,$$

and this integration step is not easy, especially for high-dimensional $$\w$$.

<aside class="box-note l-body" markdown="1">
In research on (Bayesian) model selection, the term **marginal likelihood** is often used to refer to $$\pof{\x \given \h}$$. This is because it is the likelihood of the observed data given the hyperparameters, marginalized over all possible parameter values $$\W$$. It is also known as the **model evidence**.
</aside>

## Bayesian Model Averaging

At this point, we can compare the marginal likelihood to **Bayesian Model Averaging (BMA)**, which marginalizes over the model parameters $$\W$$ when making predictions to account for uncertainty. It is useful when dealing with complex models and high-dimensional parameter spaces in a low-data regime, as it provides a more robust and comprehensive approach to making predictions. This contrasts with the MLE or MAP as defined above which uses a single parameter value $$\w$$ to make predictions. The predictive distribution for a new data point $$\x'$$ using BMA is given by:

$$\pof{\x' \given \x, \h} = \int \pof{\x' \given \x, \w, \h} \pof{\w \given \x, \h} , d\w,$$

where $$\pof{\w \given \x, \h}$$ is the posterior distribution of the parameters given the data, and $$\pof{\x' \given \x, \w, \h}$$ is the probability of the new data point given the parameters, hyperparameters, and training data. 

<aside class="box-error l-body" markdown="1">
For this post, we refer to a **model** for some hyperparameters $$\h$$ as the probability distribution $$\pof{\x, \w \given \h}$$, which includes both the parameters $$\w$$ and the data $$\x$$. 

When we refer to a model's predictions, we refer to the marginal predictive distribution:

$$\pof{\x \given \h} = \E{\pof{\w \given \h}}{\pof{\x \given \w, \h}}.$$

Depending on the context, we might talk about the prior distributions or posterior distributions after conditioning on additional data, but we will try to be very clear about that.
</aside>

While BMA offers benefits, it is challenging to compute, particularly when dealing with high-dimensional parameter spaces commonly encountered in deep learning models. To make BMA computationally tractable, various approximation methods, such as Markov Chain Monte Carlo (MCMC) and Variational Inference, are have been suggested.

Comparing the BMA to the marginal likelihood, we see that the two match in the case of individual data points. In the case of multiple data points, that is conditioning on datasets, the marginal likelihood is much more complex because we use "BMA" to refer to only making predictions for a single new data point, while the marginal likelihood is computed for many points. Otherwise, the two are equivalent. Let's discuss the case of multiple data points in detail next to see why computing the marginal likelihood on datasets is even more challenging.

<aside class="box-note l-body" markdown="1">
Going forward, we will focus on the marginalized quantities and condition on different hyperparameters $$\h$$. Obviously, the separation between hyperparameters $$\H$$ and parameters $$\W$$ is somewhat artificial. Let's keep that in mind for later.
</aside>

## Datasets instead of Individual Data Points

So far, we have only described everything as if we only had a single data point $$x$$. However, in practice, we often have a dataset $$\xNset = \{\x_1, \x_2, \ldots, \x_N\}$$ of points that we want to learn from or make predictions for.

### Joint Marginal Information and Cross-Entropy

The easiest way to extend the previous definitions is to simply substitute $$\xNset$$ for $$\x$$ and assume we can compute a likelihood for the entire dataset using its joint predictive distribution and maximize it:

$$\pof{\xNset \given \h} = \int \pof{\x_1, \x_2, \ldots, \x_N \given \w, \h} \, \pof{\w \given \h} \, d\w.$$

In line with Occam's razor, we equivalently prefer smaller $$\Hof{\xNset \given \h}$$.

<aside class="box-note l-body" markdown="1">
For multiple data points, $$\Hof{\xNset \given \h}$$ is often referred to as the negative *marginal log likelihood* or (negative) *log marginal likelihood (LML)*, and it is a key quantity in Bayesian model selection<d-footnote>Personally, I prefer log marginal likelihood as that matches the order of the terms, but that might just be the German in me.</d-footnote>.

As the negative log likelihood is Shannon's information content, we will use the term **joint marginal information** or **joint log marginal likelihood** to be unambiguous (see below).
</aside>

If our model fulfills exchangeability, that is the order of the $$\x_n$$ does not matter, we could equivalently take an expectation over all permutations of the data, obtaining the **joint marginal cross-entropy**:

$$
\CrossEntropy{\pdata{\X_1, ...,\X_n}}{\pof{\X_1, ... \X_n \given \h}},
$$

where $$\pdata{\cdot}$$ is an empirical data distribution that allows us to draw samples *without replacement*---in which case both are indeed equivalent as you can easily check.

With exchangeability, we will simply write $$\CrossEntropy{\pdata{\XNset}}{\pof{\XNset \given \h}}$$ instead of using the tuple notation $$\xNset$$ as the order of the data points does not matter.

Vice-versa if a model does not fulfill exchangeability, we can induce exchangeability by averaging over all permutations of the data points via ensembling. For example, deep learning models using stochastic gradient descent are generally not exchangeable---the order and make-up of the batches does matter---but we can effectively make them exchangeable by training multiple models and averaging their predictions: in the limit of infinite models, the resulting ensemble will be exchangeable.

The joint marginal cross-entropy turns a potentially non-exchangeable joint information into an exchangeable one by taking an expectation.

### Marginal Information and Cross-Entropy

Before we go on to understand the joint cross-entropy expression better, we should consider the space of alternatives as this is not the only way to extend the previous definitions. 

For example, we could take the average of the likelihoods for individual data points:

$$ \frac{1}{N} \sum_{n=1}^N \pof{\x_n \given \h}. $$

If we assume an underlying data distribution $$\pdata{x}$$, we can also express this as trying to estimate:

$$ \E{\pdata{\x}}{\pof{\x \given \h}} = \int \pof{\x \given \h} \, \pdata{\x} \, d\x. $$

This gives us a score for data likelihood on average.

From the perspective of Occam's razor, simply taking the average likelihood is not the most principled approach. Instead, we can leverage information theory, which has been our tool of choice so far. Recall that we prefer small values of $$\Hof{\x \given \h}$$. If we take the expectation of this quantity over the data distribution, we obtain the *individual* cross-entropy:

$$\CrossEntropy{\pdata{\X}}{\pof{\X \given \h}} = \E{\pdata{\x}}{-\log \pof{\x \given \h}}.$$

This cross-entropy measures the average number of bits needed to encode the data using the model's probability distribution. Since it does not involve a joint distribution, we refer to it simply as the **marginal cross-entropy**.

Obviously, the marginal cross-entropy and the average likelihood are not the same. Using the convexity of the negative logarithm and Jensen's inequality, we can show that the marginal cross-entropy is always larger than the negative logarithm of the average likelihood:

$$
\begin{aligned}
\CrossEntropy{\pdata{\X}}{\pof{\X \given \h}} &= \E{\pdata{\x}}{-\log \pof{\x \given \h}} \\
&\geq -\log \E{\pdata{\x}}{\pof{\x \given \h}} \\
&\approx -\log \frac{1}{N} \sum_{n=1}^N \pof{\x_n \given \h}.
\end{aligned}
$$

<aside class="box-note l-body" markdown="1">
The not-marginalized cross-entropy $$\Hof{\xNset \given \w, \w}$$ is a common loss function in machine learning. Minimizing it is equivalent to minimizing the Kullback-Leibler divergence between the model's distribution and the true data distribution. Sometimes, it is also referred to as the **negative log-likelihood (NLL)** or the **negative log-likelihood loss**.
</aside>

The NLL is often used in machine learning to evaluate the performance of a model *after* training, typically on a held-out *validation set*. This is equivalent to computing the cross-entropy on the empirical data distribution of the validation set, conditioned on the model parameters learned from the training data. 

But it is crucial to distinguish this from the cross-entropy computed on the prior distribution of the model parameters before seeing any data, which is not as useful for evaluating a trained model's performance. Only the NLL on a validation set conditioned on the training data gives us an estimate of the model's generalization ability after training.

The same obviously holds for the quantities marginalized over the model model parameters.

### Marginal Cross-Entropy vs Joint Cross-Entropy

It is not clear from Occam's razor what aggregate metric on $$\Hof{\x \given \h}$$ we ought to prefer. We could also use the median or some other quantile of the information content of the data distribution as a summary statistics to inform us about the model's performance on the dataset. This might be a more robust approach than using the mean, as it is less sensitive to outliers.

Crucially, the marginal cross-entropy and related summary statistics measure the model's performance using the "prior" parameter distribution, not the posterior conditioned on data.
The joint distribution captures something else, however. We can see this most clearly when using the chain rule.
Using the chain rule on joint information yields:

$$ 
\begin{aligned}
\pof{\xNset \given \h} &= \pof{\x_1, \x_2, \ldots, \x_N \given \h} \\
&= \pof{\x_1 \given \h} \pof{\x_2 \given \x_1, \h} \ldots \pof{\x_N \given \x_1, \x_2, \ldots, \x_{N-1}, \h} \\
&= \sum_{n=1}^N \pof{\x_n \given \x_1, \ldots, \x_{n-1}, \h}.
\end{aligned}
$$

---or using the information content:

$$
\begin{aligned}
\Hof{\xNset \given \h} &= \Hof{\x_1 \given \h} + \Hof{\x_2 \given \x_1, \h} + \ldots + \Hof{\x_N \given \x_1, \x_2, \ldots, \x_{N-1}, \h} \\
&= \sum_{n=1}^N \Hof{\x_n \given \x_1, \ldots, \x_{n-1}, \h}.
\end{aligned}
$$

Each term is a **conditional marginal information** on the previous data points.
Similarly, when we take an expectation over the data distribution, we obtain a chain of **conditional marginal cross-entropies**:

$$
\begin{aligned}
& \CrossEntropy{\pdata{\XNtuple}}{\pof{\XNtuple \given \h}} = \\
&\quad = \CrossEntropy{\pdata{\X_1}}{\pof{\X_1 \given \h}} + \CrossEntropy{\pdata{\X_2 \given \X_1}}{\pof{\X_2 \given \X_1, \h}}  \\ 
&\quad \quad + \ldots + \CrossEntropy{\pdata{\X_N \given \X_{N-1}, ..., \X_1}}{\pof{\X_N \given \X_1, \X_2, \ldots, \X_{N-1}, \h}} \\
&\quad = \sum_{n=1}^N \CrossEntropy{\pdata{\X_n \given \X_{n-1}, ..., \X_1}}{\pof{\X_n \given \X_{n-1}, \ldots, \X_1, \h}}.
\end{aligned}
$$

Each of the terms in the sum is of a conditional marginal cross-entropy conditioned on the previous data points. This is a very different quantity from the marginal cross-entropy, which we recognize in the first term.

The following visualization summarizes the relationship between the conditional and joint marginal cross-entropies and information. Importantly, the chain rule tells us that the area under the curve of the conditional quantities is equal to the joint quantity. This is a key insight that we will use in the following discussion.

{% comment %} 
include figure.html path="assets/img/2024-05-07-clml/area_under_curve.png" 
class="l-screen-inset img-fluid rounded z-depth-1" 
{% endcomment %}

<figure markdown="1" class="l-page rounded z-depth-1"> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/2024/assets/img/2024-05-07-clml/area_under_curve-480.webp"> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/2024/assets/img/2024-05-07-clml/area_under_curve-800.webp"> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/2024/assets/img/2024-05-07-clml/area_under_curve-1400.webp"> <img class="img-fluid" src="/2024/assets/img/2024-05-07-clml/area_under_curve.png" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> 
<figcaption class="caption" markdown="1">
*The relationship between the conditional and joint marginal cross-entropies and information as a function of dataset size.*
**Left plot**: The conditional marginal cross-entropy (blue line) is plotted for a fictitious multi-class classification problem with 10 classes. As the dataset size increases, the conditional marginal cross-entropy decreases and converges to the best achievable loss for the given model hypothesis $$\h$$. The area under the conditional marginal cross-entropy curve (shaded in orange) represents the joint marginal cross-entropy, which is the sum of the conditional marginal cross-entropies over the dataset, as per the chain rule.
**Right plot**: Similarly, the conditional marginal information (green line) is plotted for the same setup. The conditional marginal information is a noisy estimate of the conditional marginal cross-entropy, as it is computed on individual data points. The area under the conditional marginal information curve (shaded in red) represents the joint information, which is the sum of the conditional marginal informations over the dataset.
</figcaption>
</figure>

<aside class="box-note l-body" markdown="1">
Intuitively, we can think of the joint cross-entropy as measuring the performance of the model during progressive training. This also captures the *online performance* of the model: how does the model adapt to new data points? 

The conditional marginal cross-entropy, on the other hand, measures the performance of the model without updating the model parameters, which is a more static view of a model's performance.
</aside>

This brings us back to the earlier question of what we prefer and want to use for model selection. Let's consider:

1. Obviously, the marginal cross-entropy as in the first term is quite likely useless for model selection with deep learning models, as it is not conditioned on any data and thus does not capture the model's performance after training.

2. If we care about the model's "generalization" performance *after training on $$N-1$$ data points* without further adaptation, the marginal cross-entropy on the last data point is the most relevant quantity:

    $$\CrossEntropy{\pdata{\X_N \given \X_{N-1}, ..., \X_1}}{\pof{\X_N \given \X_{N-1}, \ldots, \X_1, \h}}$$
  
    It measures the model's performance on the last data point after having seen all previous data points, similar to a "leave-one-out" metric. Indeed, it is equivalent to [leave-one-out cross-validation](https://www.wikiwand.com/en/Cross-validation_(statistics)#Leave-one-out_cross-validation) when we have an empirical data distribution consisting of $$N$$ data points and we sample without replacement.
    
3. More generally, it is equivalent to cross-validation when we hold out more than one data point for evaluation from the empirical data distribution:

    $$ \CrossEntropy{\pdata{\X' \given \X_{N-k}, ..., \X_{1}}}{\pof{\X' \given \X_{N-k}, ..., \X_{1}, \h}}.$$

    This is the same expression as in **(2.)** but we assume there are more samples to draw from in the empirical data distribution $$\pdata{\x'}$$. We call this term the conditional marginal cross-entropy and keep in mind its connection to cross-validation.

4. On the other hand, if we care about the model's performance as an online learner, or let's say *in-context learner in the case of LLMs*, the joint marginal cross-entropy is the more relevant quantity because it measures the adaptability of the model to new data points. 

5. Obviously, in this case, we would also not want to use the joint marginal cross-entropy on the prior but we would want to condition on some data first to be closer to the actual use-case of the model, which will have been (pre-)trained already. As such, we would be interested in estimating a **conditional joint marginal cross-entropy**:

    $$ \CrossEntropy{\pdata{\XNsetk \given \XNkset}}{\pof{\XNsetk \given \XNkset, \h}}. $$

6. All these quantities are equally valid as far as Occam's razor is concerned---and we have already connected Occam's razor to MLE and MAP, which are often not even considered as properly Bayesian. 

7. We have also not discussed how we can efficiently estimate these quantities, especially in the case of deep learning models. More importantly, we have already considered that the marginal likelihood, BMA, and the joint cross-entropy (as an expectation over the marginal likelihood) are not easy to estimate.

While we will look at the paper in detail in the second half of this blog post, this discussion already brought us to one of the main points of the paper: 

<aside class="box-important l-body" markdown="1">
The marginal likelihood and its approximations are not the only tools we have for model selection and hyperparameter learning and often not even the tools we should use. 
We always need to consider the context in which we want to use a model and data, and then choose the most relevant metric for our use-case. 
</aside>

This is a very important point, and it is also a point that has not been considered sufficiently in the literature on model selection and hyperparameter learning, where the model evidence and marginal likelihood have been presented as the ultimate criterion for model selection and hyperparameter learning. In practice, we rarely update a model on additional data during inference---this is changing with the advent of LLMs and strong in-context learners, but it is still not the norm.

TK add a schema of all the quantities using "reasonable" terminology and include the conventional terminology in parentheses TK

But why has the marginal likelihood been the preferred choice for model selection so far then?

### Different Data Regimes

(TK need to use consistent heading styles! TK)

To sketch a possible answer for this, a fruitful question is: when do the conditional marginal cross-entropy and joint marginal cross-entropy actually lead to different outcomes for model selection and hypothesis testing?
TK do I talk about hypothesis testing before? TK Obviously, there are different scenarios: one is hypothesis testing using discrete model choices, another is hyperparameter tuning. TK AND WHAT ELSE TK

For the discrete case, we can reduce the question to one about ranking: if we have two possible hyperparameter choices $$\h_1$$ and $$\h_2$$, when do we get the same ranking $$\h_1 \succ \h_2$$ for the conditional marginal cross-entropy and the joint marginal cross-entropy? 

#### Model Misspecification

First, let's look at the case when we have a lot of data available. Here, model misspecification, which is a common cause of concern, is the most important factor:

<aside class="box-note l-body" markdown="1">
**Model misspecification** occurs when the assumed model class does not contain the true data-generating process. In other words, there are no model parameters that give us a model that matches the data-generating distribution. Thus, even with infinite data, the model would not be able to perfectly capture the underlying reality that produced the observed data. This can lead to biased parameter estimates, incorrect inferences, and suboptimal predictions. It is a common issue in real-world applications, as our models are often simplifications of complex phenomena.
</aside>

As the renowned statistician George Box famously stated:

<blockquote markdown="1">
  All models are wrong, but some are useful.
  <footer markdown="1">
    --- <cite>George Box, Science and Statistics (1976)</cite>
  </footer>
</blockquote>

When working with real-world data, we must always assume that our models are misspecified (to some degree). Models always simplify complex systems in some ways and cannot capture every nuance of the data-generating process. Consequently, the goal of model selection is not to find the "true" model but rather to identify the most useful model that balances simplicity, interpretability, and predictive performance. Without model misspecification, we would always end up at an MLE that matches the data-generating model in the infinite data limit as the [Bernstein-von Mises' theorem](https://www.wikiwand.com/en/Bernstein%E2%80%93von_Mises_theorem) tells us<d-footnote>There are likely fewer caveats to this statement than the naive interpretation of the theorem implies because we are usually not interested in converging towards some unique/identifiable parameters but rather in the predictions matching the data-generating process's TK refer to the other blog post TK.</d-footnote>.

TK find box1976science TK

#### Infinite Data Limit

But let's go back to our question when the different quantities lead to similar rankings.

While a conditional joint marginal cross-entropy as a sum of conditional marginal cross-entropies is obviously larger than each individual one, if we divide the joint marginal cross-entropy by the number of samples in the conditional joint distribution, we obtain an average of conditional marginal cross-entropies, which can be more easily related:

$$
\begin{aligned}m
& \frac{1}{N-k} \CrossEntropy{\pdata{\XNsetk \given \XNkset}}{\pof{\XNsetk \given \XNkset, \h}} \\
&\quad = \sum_{n=N-k+1}^N \frac{1}{N-k} \CrossEntropy{\pdata{\X_n \given \X_{n-1}, ..., \X_1}}{\pof{\X_n \given \X_{n-1}, ..., \X_1, \h}}.
\end{aligned}
$$

Here, Bernstein-von Mises' theorem concretely tells us that the posterior distribution of the model parameters converges to a normal distribution around the MLE as the number of data points goes to infinity (as long as the MLE is unique/identifiable). This means that the later terms in the chain rule decomposition of the joint cross-entropy will converge to the same value in the infinite sample limit as the data we condition one becomes infinite. If we take the limit, we can obviously ignore the first terms in the chain rule decomposition of the joint cross-entropy, and we will get the same average value for the terms of the joint cross-entropy (one per sample in the joint) and the conditional marginal cross-entropy. This also matches a similar result on entropy rates in TK Thomas & Cover TK. Overall, we have (without formal proof):

$$
\begin{aligned}
&\lim_{N \to \infty} \frac{1}{N} \CrossEntropy{\pdata{\XNset}}{\pof{\XNset \given \h}} = \\
&\quad = \lim_{N \to \infty} \frac{1}{N} \sum_{n=1}^N \CrossEntropy{\pdata{\X_n \given \X_1, ..., \X_{n-1}}}{\pof{\X_n \given \X_1, ..., \X_{n-1}, \h}} \\
&\quad = \lim_{N \to \infty} \CrossEntropy{\pdata{\X' \given \XNset}}{\pof{\X' \given \XNset, \h}}.
\end{aligned}
$$

Given sufficient data (in the infinite sample limit), we see that either of these quantities will lead to the same ranking of different hyperparameters/model hypotheses. In reverse, we can thus expect to see meaningful differences only in low-data regimes, where the model is not yet fully adapted to the data. 

Finally, in the infinite data limit, we don't need to take an expectation on the data we condition on for the conditional marginal cross-entropy (as the model parameters will still have converged):

$$
\begin{aligned}
&\lim_{N \to \infty} \CrossEntropy{\pdata{\XNsetk \given \XNkset}}{\pof{\XNsetk \given \XNkset, \h}} \\
&\quad = \lim_{N \to \infty} \CrossEntropy{\pdata{\XNset}}{\pof{\XNset \given \xNset \h}},
\end{aligned}
$$

for any $$\xNset \sim \pdata{\xNset}$$ as $$n \to \infty$$.

All in all, this shows that with sufficient data, the marginal likelihood (joint cross-entropy in expectation) and conditional marginal cross-entropy (validation loss in expectation) will end up producing the same ranking of different model hypotheses or hyperparameter choices. The catch is that "sufficient data" might be a very large amount of data, especially for highly expressive models like neural networks.

Hence, we only expect these quantities to be meaningfully different in the low-data regime. So let's focus on the low-data regime now. In particular, with the caveat above, what is the low-data regime is for different $$\h$$?

#### Prior-Data Conflict

Even if we were to converge to the same generalization loss in the infinite data limit, different choices of hyperparameters, inducing different priors, might affect the convergence speed and the quality of the model's predictions in the low-data regime. 

<aside class="box-note l-body" markdown="1">
When a prior distribution prevents the model from quickly converging to the MLE because it does not place sufficient probability density (or mass) on the MLE in the infinite data limit, this is called a **prior-data conflict**. When we perform hyperparameter tuning, we are effectively trying to resolve this conflict by finding a prior that is more aligned with the data distribution.
</aside>

Let's concretize this a bit: if we assumed that for different $$\H$$, we will converge to the same generalization performance in the infinite data limit, we would still expect different generalization "speeds" in the low-data regime. This is because the prior distribution of the model parameters will affect the convergence speed and the quality of the model's predictions in this regime.

<figure markdown="1" class="l-page rounded z-depth-1"> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/2024/assets/img/2024-05-07-clml/prior_conflict_and_model_misspecification-480.webp"> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/2024/assets/img/2024-05-07-clml/prior_conflict_and_model_misspecification-800.webp"> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/2024/assets/img/2024-05-07-clml/prior_conflict_and_model_misspecification-1400.webp"> <img class="img-fluid" src="/2024/assets/img/2024-05-07-clml/prior_conflict_and_model_misspecification.png" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> 
<figcaption class="caption" markdown="1">
*The relationship between the conditional marginal cross-entropy and dataset size under different modeling scenarios.*
**Left plot --- Prior-data conflict**: The conditional marginal cross-entropy is plotted for three different model priors ($$\h_1$$, $$\h_2$$, $$\h_3$$) in a multi-class classification problem with 10 classes. All priors converge to the same achievable loss in the infinite data limit, but their convergence speeds differ due to varying degrees of alignment of the prior distribution with the data distribution. A prior that places more probability mass near the maximum likelihood estimate (MLE) will lead to faster convergence.
**Right plot --- Model misspecification**: The conditional marginal cross-entropy is plotted for three different model hypotheses ($$\h_1$$, $$\h_2$$, $$\h_3$$) in the same setup. Due to the model class not containing the true data-generating process, here each hypothesis converges to a different best achievable loss in the infinite data limit, representing the minimum achievable misspecification error. 
*Real-world models often face a combination of prior-data conflict and model misspecification, of course.*
</figcaption>

---

<aside class="box-important l-body" markdown="1">
In the infinite data limit, the marginal likelihood (or equivalently the joint cross-entropy), and the conditional marginal cross-entropy will converge to the same value when averaged over the data distribution. This means that given enough data, all three metrics - marginal likelihood, joint cross-entropy, and conditional marginal cross-entropy - will end up producing the same ranking of different model hypotheses or hyperparameter choices.
Crucially, the conditional marginal cross-entropy is equivalent to cross-validation in expectation. Cross-validation is the gold standard for model selection in machine learning practice, where we estimate a model's generalization performance by evaluating it on held-out validation data after training on the rest.
Therefore, with sufficient data, using the marginal likelihood for model selection is equivalent to using validation sets, and both will make the same model choice as the joint cross-entropy in expectation. The catch is that "sufficient data" might be a very large amount of data, especially for highly expressive models like neural networks.
In low-data regimes, the metrics can differ meaningfully, and the conditional marginal cross-entropy (or cross-validation) is often the more reliable choice for model selection targeting generalization performance. The joint cross-entropy may be preferable if we care about a model's sequential prediction performance.
</aside>

TK

TK What was the novelty of the previous exposition? TK

---

Feedback from Claude

Here are some more critical observations and potential areas for improvement:

1. Clarity of the main contribution: While your article provides valuable insights and connections, the main novel contribution could be more clearly articulated. It would be helpful to have a concise statement early in the article that highlights the specific gap in the literature you are addressing and how your exposition advances the understanding of model selection and hyperparameter learning in the Bayesian context.

2. Depth of analysis: Some of the discussions, such as the comparison between marginal cross-entropy and joint cross-entropy, could benefit from a more in-depth analysis. Providing a more rigorous mathematical treatment, along with concrete examples or simulations, could strengthen your arguments and make the distinctions more compelling.

3. Connection to existing literature: While you mention MacKay's work, it would be valuable to provide a more comprehensive overview of the existing literature on model selection and hyperparameter learning in the Bayesian community. This would help contextualize your contribution and demonstrate how your exposition builds upon or differs from previous work.

4. Limitations and potential counterarguments: Addressing potential limitations of your approach or anticipating counterarguments from the Bayesian community could make your article more robust. For example, discussing the computational challenges associated with estimating the marginal likelihood or joint cross-entropy for complex models could provide a more balanced perspective.

5. Implications for practice: While you touch on the practical implications of different model selection criteria, providing more concrete guidance or recommendations for practitioners could enhance the impact of your article. Discussing how your insights can be applied in real-world scenarios, along with any limitations or caveats, would make your exposition more valuable to the applied machine learning community.

6. Consistency and coherence: Ensure that the structure and flow of your article are consistent and coherent throughout. Make sure that each section builds upon the previous ones and contributes to the overall narrative. Reviewing the transitions between sections and the logical progression of ideas could help improve the overall clarity and impact of your exposition.

7. Reproducibility: If any simulations or experiments are included in the article, ensure that the code and data are made available to enable reproducibility and encourage further research in this area.

Please keep in mind that these comments are meant to be constructive and help strengthen your already valuable contribution. Addressing these points could help your article withstand the scrutiny of critical reviewers and make a more significant impact in the Bayesian machine learning community.

---


The marginal likelihood, also known as model evidence $$p(\mathcal{D} \mid \mathcal{M}_i)$$ for different hypotheses $$\mathcal{M}_i$$, captures this notion. **Other things considered equal,** the highest model evidence points us towards the best hypothesis given data $$\mathcal{D}$$. The paper cites [an illustrative example from chapter 28](https://www.inference.org.uk/itprnn/book.pdf#page=355) of David MacKay’s ["Information Theory, Inference, and Learning Algorithms”](http://www.inference.org.uk/mackay/itila/book.html)<d-cite key="mackay2003information"></d-cite>:

{% capture caption %}
Excerpt from page 343 in David MacKay’s "Information Theory, Inference, and Learning Algorithms.”
{% endcapture %}
{% include figure.html path="assets/img/2024-05-07-clml/mackay_343.png" class="img-fluid" caption=caption%}

The **log marginal likelihood (LML)**, which is examined in this paper, is simply the log of the model evidence:

$$\log p(\mathcal{D} \mid \mathcal{M}_i).$$

The papers offers a concise summary in its introduction:

> we aim to fundamentally re-evaluate whether the marginal likelihood is the right metric for predicting the generalization of trained models, and learning hyperparameters. We argue that it does a good job of prior hypothesis testing, which is exactly aligned with the question it is designed to answer. **However, we show that the marginal likelihood is only peripherally related to the question of which model we expect to generalize best after training, with significant implications for its use in model selection and hyperparameter learning.**

And overall:

> There is a great need for a more comprehensive exposition, clearly demonstrating the limits of the marginal likelihood, while acknowledging its unique strengths, **especially given the rise of the marginal likelihood in deep learning.**

## How?

Following the paper, this post will summarize some of the different sections here. 

### Use-cases

Given the above, the case for the marginal likelihood in the paper focuses on:

- hypothesis testing;
- hyperparameter learning; and
- constraint learning.

### General Pitfalls

For the paper, the pitfalls of LML are manifold. The most important is the claim that:

> **Marginal Likelihood is not generalization**

The following explanation is helpful:

{% capture title %}
The marginal likelihood answers the question “what is the probability that a prior model generated the training data?”. This question is subtly different from asking “how likely is the posterior, conditioned on the training data, to have generated withheld points drawn from the same distribution?”. Although the marginal likelihood is often used as a proxy for generalization (e.g. MacKay, 1992c; Immer et al., 2021; Daxberger et al., 2021), it is the latter question we wish to answer in deciding whether a model will provide good generalization performance."
{% endcapture %}
{% assign title = title | xml_escape %}
{% capture max-width %}
" style="max-width: 25em;
{% endcapture %}
{% include figure.html path="assets/img/2024-05-07-clml/bmsmlg_sec4.1.png" class="img-fluid" max-width=max-width title=title%}

<aside class="box-note l-body" markdown="1">
⚠️ The first question might be mistaken for asking what the probability is of drawing the training data independently from a fixed prior, which is what the second question is about: what is the average performance of the *marginal predictions* for test data given the training data?

There is sometimes ambiguity in the literature between viewing a model as its parameter distribution (and using Bayesian model averaging to make predictions) or saying a specific parameter draw from the distribution is a model. The first question is about the latter, while the second question is about the former.

The marginal likelihood can be written as: 

$$p(\mathcal{D} \mid \mathcal{M}) = \mathbb{E}_{p(\omega)} p(\mathcal{D} \mid \omega, \mathcal{M}),$$ 

and the first question can be made clearer as: *what is the probability that **a draw from the prior distribution** generated the training data?*

As we will see below, the first question can also be viewed in the context of sequential predictions and learning curves. In information theory, the question would be: how well does the model compress the training data jointly? Looking at the joint predictive distribution is not yet a common concept in deep learning, however. 

Recent works by Ian Osband et al., starting with [The Neural Testbed: Evaluating Joint Predictions](https://arxiv.org/abs/2110.04629)<d-cite key="osband2022neural"></d-cite> can help build intuitions for joint predictions.
Similarly, a *gentler* introduction, comparing marginal and joint predictions, can also be found in the arXiv note [Marginal and Joint Cross-Entropies & Predictives for Online Bayesian Inference, Active Learning, and Active Sampling](https://arxiv.org/abs/2205.08766)<d-cite key="kirsch2022marginal"></d-cite>. 
</aside>

The following figure summarizes several of the pitfalls the paper examines: 

{% capture caption %}
<b>Note: typo in “</b>(a)<b> Prior B” → “</b>(a)<b> Prior C”.</b>
{% endcapture %}
{% include figure.html path="assets/img/2024-05-07-clml/bmsmlg_fig1.png" class="img-fluid" caption=caption%}

The paper emphasizes the difference between hypothesis testing and model selection:

{% capture max-width %}
" style="max-width: 25em;
{% endcapture %}
{% include figure.html path="assets/img/2024-05-07-clml/bmsmlg_sec4.1_model_selection.png" max-width=max-width %}

So, in hypothesis testing, we want to determine which model class (hypothesis) provides the “best” explanation for the past data we have collected, i.e., the training data; whereas for model selection, we care about which model class provides the best performance for future, not-yet-seen data when we condition/train the model on the currently available training data. 

<aside class="box-note l-body" markdown="1">
☝ Interestingly, according to Occam's razor, both should be related: the model class that provides the simplest explanation for the current training data should also generalize the best.
</aside>

### Estimating LML via the Laplace Approximation

Computing the marginal likelihood via sampling is generally intractable for (B)NNs. Estimating it from a prior, usually highly uninformative distribution leads to high-variance estimates when performing Monte Carlo sampling compared to sampling from the posterior distribution, which is more practical. The latter is the approach followed by the paper for the CLML using a Laplace approximation of the posterior.

A **Laplace approximation (LA)** estimates the posterior distribution by fitting a Gaussian on the second-order Taylor expansion of the MLE (maximum likelihood) or MAP (maximum-a-posteriori) estimate <d-cite key="murphy2012machine"></d-cite>. We can draw parameter samples from it or compute its entropy to approximate the posterior uncertainty. 

<aside class="box-note l-body" markdown="1">
💡 The LA can be motivated by the [Cramér–Rao bound](https://en.wikipedia.org/wiki/Cram%C3%A9r%E2%80%93Rao_bound) for statistical models, which have "true model parameters." Essentially, the bound tells us that as we take more and more data into account, our posterior will concentrate around the true model parameters for a well-specified model and thus become uni-modal.

A possible resolution for this contradiction might be that we should try to find the model with the largest model posterior, which we examine in the comments. Note that finding a good model prior can be very difficult, however.
</aside>

The paper notes that the LA has drawbacks: it only captures uncertainty around a single mode, leading to underestimation of the model uncertainty. The paper provides a beautiful example of this issue:

{% capture max-width %}
" style="max-width: 35em;
{% endcapture %}
{% include figure.html path="assets/img/2024-05-07-clml/bmsmlg_fig3.png" max-width=max-width %}

This is of particular importance for overparameterized models like DNNs, which have multiple diverse modes, as we know from deep ensembles ([Wilson, Izmailov (2021, blog post)](https://cims.nyu.edu/~andrewgw/deepensembles/);  [Wilson, Izmailov (2020)](https://arxiv.org/abs/2002.08791)<d-cite key="wilson2020bayesian"></d-cite>). The LA, however, only centers around a single one. 


### Conditional Marginal Likelihood

Given that the LML does not capture generalization, i.e., is not suited for model selection according to the paper but intended for hypothesis testing, the paper introduces the **conditional log marginal likelihood (CLML)**. 

The idea behind the CLML can be best understood by using the chain rule of probability/information theory:

$$
\begin{aligned}
\log p(\mathcal{D} \mid \mathcal{M}) &= \log p(y_1, \ldots, y_N \mid x_1, \ldots, x_n, \mathcal{M}) \\
&= \sum_{i=1}^n \log p(y_i \mid x_i, y_{i-1}, x_{i-1}, \ldots, y_1, x_1, \mathcal{M}) \\
&= \sum_{i=1}^n \log p(\mathcal{D}_i \mid \mathcal{D}_{< i}, \mathcal{M}),
\end{aligned}
$$

following notation also used by Lyle et al. (2020)<d-cite key="lyle2020bayesian"></d-cite>.

The paper notes that depending on the underlying model $$\mathcal{M}$$, the first $$m$$ terms might be very low without affecting the model's generalization capabilities later after seeing more data and advocates for dropping these terms from the sum above:

{% capture max-width %}
" style="max-width: 25em;
{% endcapture %}
{% include figure.html path="assets/img/2024-05-07-clml/bmsmlg_sec4.4.png" max-width=max-width %}

Hence, the **Conditional Marginal Likelihood (CLML)** is introduced as a *left-truncated* LML:

$$
\log p(\mathcal{D}_{\ge m} \mid \mathcal{D}_{< m}, \mathcal{M}) = \sum_{i=m}^n \log p(\mathcal{D}_i \mid \mathcal{D}_{< i}, \mathcal{M}),
$$

The paper notes that:

> Variants of the CLML were considered in Berger and Pericchi (1996) as an intrinsic Bayes factor for handling improper uniform priors in hypothesis testing, and [Fong and Holmes (2020)](https://arxiv.org/abs/1905.08737)<d-cite key="fong2020marginal"></d-cite> to show a connection with cross-validation and reduce the sensitivity to the prior.

We will refer to this and other prior literature below.

<aside class="box-note l-body" markdown="1">
⚠️ Given the generally assumed exchangeability of training data in Bayesian models, the ordering of the training data using indices $$i$$ is arbitrary. As the paper points out, the CLML depends on this arbitrary data order, and thus a permutation-invariant version of the CLML is proposed by averaging over different permutations. For larger experiments, this becomes very expensive, and the paper uses a single permutation instead (as noted in appendix D).
</aside>

The paper examines how the CLML performs compared to the LML. It begins by critically reviewing Lyle et al. (2020)<d-cite key="lyle2020bayesian"></d-cite>.

### On Training Speed and Learning Curves

The paper then introduces a **learning curve** as the graph of how $$p(\mathcal{D}_n \mid \mathcal{D}_{<n}, \mathcal{M})$$ changes over $$n$$ (potentially, as average over different permutations of the data order to be permutation-invariant); see (c) below, and compares to [Lyle et al. (2020) “A Bayesian Perspective on Training Speed and Model Selection”](https://arxiv.org/abs/2010.14499)<d-cite key="lyle2020bayesian"></d-cite>. 

A rough **TL;DR of Lyle et al. (2020)** is that the sum decomposition of the LML, shown earlier, is just the (discrete) area under the learning curve and that for DNNs, we can use the sum over mini-batch training losses as a proxy to predict generalization behavior in the spirit of Occam's razor.

Several examples of failures of LML to predict generalization are provided and connected to the notion of “training speed”:

{% capture max-width %}
" style="max-width: 50em;
{% endcapture %}
{% include figure.html path="assets/img/2024-05-07-clml/bmsmlg_fig4.png" max-width=max-width %}

The paper thus finds that “training speed” does not predict generalization performance. 

In particular, for neural networks, the paper reports the following interesting result:

> In Figure 10(a) (Appendix), we show the **correlation of the BMA test log-likelihood with the LML is positive for small datasets and negative for larger datasets, whereas the correlation with the CLML is consistently positive.** Finally, Figure 10(a) (Appendix) shows that the Laplace LML heavily penalizes the number of parameters, as in Section 4.3. We provide additional details in Appendix F.
> 

(where **BMA** stands for **Bayesian Model Average**, i.e., marginalizing over the posterior parameters)

### Model Selection

To show the advantages of the CLML over the LML, the paper examines the correlation with the generalization performance of 25 CNN and ResNet architectures on CIFAR-10 and CIFAR-100:

{% capture max-width %}
" style="max-width: 50em;
{% endcapture %}
{% include figure.html path="assets/img/2024-05-07-clml/bmsmlg_fig5.png" max-width=max-width caption="Figure 5" %}

The CLML is more strongly correlated with the generalization performance of the architectures than the LML. The LML suffers from higher first terms in the sum decomposition for more flexible models and is negatively correlated with test accuracy for smaller $$\lambda$$. The paper notes that the LML estimate using the LA's entropy is sensitive to the prior variance and the number of model parameters. This could also explain why larger models have worse LML.

The CLML is computed using predictions on the held-out data for parameter samples from the LA ("function space"), does not estimate the model entropy, and thus does not suffer from the same issues. 

## The Conditional Log Marginal Likelihood

While the paper covers a lot more in its breadth, the second half mainly focuses on the CLML and examines the question of how the CLML is different from simply using a validation loss. Again, this is not a question directly examined in the paper, but given the prevalence of validation sets, it might be worth asking what the suggested method provides beyond that.

### Ignoring Model Priors

First, from a Bayesian point of view, using the model evidence (LML) is only valid for model selection when the model prior is uniform **(i.e., all other things are considered equal):**

The Bayesian view on model selection given data $$\mathcal{D}$$ is:

$$
p(\mathcal{M}_i \mid \mathcal{D}) \propto p(\mathcal{D} \mid \mathcal{M}_i) \; p(\mathcal{M}_i).
$$

Hence, maximizing $$p(\mathcal{D} \mid \mathcal{M}_i)$$ is not sufficient depending on what model prior we use.

In particular, the **minimum description length (MDL)** approach states that we want to select the model that minimizes both the length of describing its parameters and the data <d-cite key="cover1999elements"></d-cite>. 
That is, MDL is about compressing the model and data:

$$
\min_i H[\mathcal{M_i}, \mathcal{D}] = \min_i H[\mathcal{D} \mid \mathcal{M_i}] + H[\mathcal{M_i}],
$$

where $$H$$ is Shannon’s information content here (using the Practical IT notation). We can also rewrite this as

$$
\begin{aligned}
\arg \min_i H[\mathcal{M_i}, \mathcal{D}] &= \arg \min_i H[\mathcal{M_i} \mid \mathcal{D}] + H[\mathcal{D}] \\&= \arg \min_i H[\mathcal{M_i} \mid \mathcal{D}],
\end{aligned}
$$

and we can drop $$H[\mathcal{D}]$$ as it is independent of the model.

By ignoring the model complexity when we only use the LML, we might potentially ignore an essential part of Occam’s razor and the MDL principle.
However, many other papers also ignore the model complexity and assume a uniform model prior. A uniform prior is valid, for example, when choosing specific hyperparameters. Moreover, estimating a model's description length is challenging and an open problem. The LML only makes sense for model selection when comparing models of equal complexity. 

### Comparison to Prior Art

We expand on the prior art mentioned in the paper to be able to place it better in the relevant literature.

#### CLML vs Cross-validation

As noted in the paper, [Fong and Holmes (2020)’s “On the marginal likelihood and cross-validation”](https://arxiv.org/abs/1905.08737)<d-cite key="fong2020marginal"></d-cite> connects LML and leave-p-out cross-validation:

{% include figure.html path="assets/img/2024-05-07-clml/otmlacv_sec3.1.png" %}

We obtain:

{% include figure.html path="assets/img/2024-05-07-clml/otmlacv_prop2.png" %}

This follows from writing out the terms and swapping the order of the sums.

The paper finally defines something similar to the CLML:

{% include figure.html path="assets/img/2024-05-07-clml/otmlacv_ccv.png" %}

#### CLML vs TSE-E

While Lyle et al. (2020) focus on the sum under the learning curve as a proxy for the LML, they only note preliminary empirical evidence for a connection to generalization for DNNs.
In the follow-up paper "Speedy Performance Estimation for Neural Architecture Search" by Ru et al. (2021)<d-cite key="ru2021speedy"></d-cite>, not cited by Lotfi et al (2022)<d-cite key="lotfi2022bayesian"></d-cite>, the authors focus on using similar ideas for model selection.

Ru et al. (2021) explain that:

> we hypothesise that an estimator of network training speed that assigns higher weights to later epochs may exhibit a better correlation with the true generalisation performance of the final trained network" and evaluate training speed estimators which discount the initial terms and sum over a left-truncated learning curve, amongst others. 

The particular estimator is referred to as TSE-E in the paper and shown to have a higher correlation with the generalization performance than the full-length TSE estimator, which is just a proxy for the LML from Lyle et al. (2020)<d-cite key="lyle2020bayesian"></d-cite>. Thus, the findings in this earlier paper are congruent with the CLML.

### CLML vs Validation Loss

A different way of examining what the CLML does is to note that the more data a model has trained with, the more its predictions for different samples will become independent. This follows from the model converging to a delta distribution and is, in particular, valid when using a Laplace approximation to the posterior, as is the case here.

In general, we have:

$$\begin{aligned}
&\log p(\mathcal{D}_{\ge m} \mid \mathcal{D}_{< m}, \mathcal{M}) \\&\quad = \sum_{i=m}^n p(\mathcal{D}_i \mid \mathcal{D}_{< m}, \mathcal{M})  - TC(D_m, \ldots, D_n \mid \mathcal{D}_{< m}, \mathcal{M}), 
\end{aligned}$$

where $$TC(D_m, \ldots, D_n \mid \mathcal{D}_{< m}, \mathcal{M})$$ is the total correlation of the samples in $$\mathcal{D}_{\ge m}$$ and is thus defined:

$$
\begin{aligned}
& TC(D_m, \ldots, D_n \mid \mathcal{D}_{< m}, \mathcal{M}) := \\
&\quad = \log p(D_m, \ldots, D_n \mid \mathcal{D}_{< m}, \mathcal{M}) - \sum_{i=n}^m \log p(D_i \mid \mathcal{D}_{< m}, \mathcal{M}).
\end{aligned}
$$

For $$m\to \infty$$, we have $$TC(D_m, \ldots, D_n \mid \mathcal{D}_{< m}) \to 0$$: The more the model converges, the more the LML is just the cross-entropy of the model trained on $$\mathcal{D}_{< m}$$, evaluated on the held-out data times the number of samples in $$\mathcal{D}_{\ge m}$$.

While this might not be true for DNNs overall---deep ensembles that capture multiple modes come to mind---it is true when using a Laplace approximation.

<aside class="box-warning l-body" markdown="1">
💡 (For large $$m$$) the CLML is just an estimator of the summed validation loss.
</aside>

To be precise, the paper always computes the **average CLML**: it divides the CLML by the number of elements in the $$\mathcal{D}_{\ge m}$$. As such, the (negative) validation loss (as negative cross-entropy estimate using the "validation set" $$\mathcal{D}_{\ge m}$$) and average CLML {% raw %}
$$\frac{\log p(\mathcal{D}_{\ge m} \mid \mathcal{D}_{\lt m}, \mathcal{M})} {|\mathcal{D}_{\ge m}|}$$ {% endraw %} are directly comparable.

In appendix D, the paper explains how many orderings the CLML is averaged over for permutation invariance for the neural architecture experiments on CIFAR-10 & CIFAR-100:

> When D is a large dataset such that $$\mathcal{D}_{< m}$$ and $$\mathcal{D}_{≥ m}$$ are both sufficiently large, a single permutation may suffice.

The paper notes that the DNN experiments use an 80%/20% split on CIFAR-10 using **only 20 LA samples**: 
A model is trained using 80% of the training data and then evaluated on a 20% held-out "validation set." The CLML of this model is compared to the (BMA) test accuracy or test (cross-entropy) loss of a model trained on 100% of the training data. The LML is finally estimated using the LA of a model trained on 100% of the training data. 

Given the findings in ["Marginal and Joint Cross-Entropies & Predictives for Online Bayesian Inference, Active Learning, and Active Sampling”](https://arxiv.org/abs/2205.08766)<d-cite key="kirsch2022marginal"></d-cite>, we can hypothesise that 20 LA samples are probably not sufficient to obtain a good CLML estimate (*which corresponds to estimating a joint prediction*). It is more likely that we only obtain a sample of the validation loss (*as sum of marginal predictions*) than a good sample of the CLML. 

<aside class="box-warning l-body" markdown="1">
❓ How does the CLML compare to the validation loss? Could we just compute the validation loss directly instead of the CLML? 
</aside>

Sadly, the paper does not compare the validation loss and their CLML estimate. The difference between the two quantities would tell us how well the 20 LA samples account for the total correlation.

<aside class="box-note l-body" markdown="1">
👁️‍🗨️ In the updated version of the paper, the authors compute both the CLML and the non-BMA validation loss. Overlaying the two seems to point towards the validation loss and the CLML being exactly equal for one model type (CNNs). The results are not entirely consistent between *v1* and *v2* of the paper. In particular, the CLML in *v2* seems to perform worse than the BMA validation loss in *v1*.
</aside>

### Laplace Approximation for LML vs Sampling for CLML

There might be an apple vs oranges comparison because the LML is computed using the LA while the CLML is computed via sampling:

$$\begin{aligned}
\log p(\mathcal D_{\ge m} \mid \mathcal D_{< m}, \mathcal{M} ) &\approx \log \sum_{k=1}^K \frac{1}{K}\, p(\mathcal{D}_{\ge m} \mid w_k, \mathcal M ) \\
&= \log \sum_{k=1}^K \frac{1}{K}\, \prod_{j=m}^n p(y_j \mid x_j, w_i, \mathcal M ),
\end{aligned}$$

where $$w_k \sim p(\omega_k \mid \mathcal{D}_{<m}, \mathcal M)$$ are parameter samples drawn from the LA for the model trained with a subset of the training data (80% of the training data).

<aside class="box-warning l-body" markdown="1">
❓ One could also compute the CLML via the LA: the CLML is just the LML for 100% of the training data when using the model with 80% of the training data as prior. It would be interesting to investigate this estimate, too.
</aside>

### Unexpected Anti-Correlation

Previously, we have highlighted the finding from Section 5 that:

> **correlation of the BMA test log-likelihood with the LML is positive for small datasets and negative for larger datasets, whereas the correlation with the CLML is consistently positive.**

Yet, the more data we have, the more the LML should correlate with the final performance simply because the model's LA will converge, as the following informal reasoning shows:

If we use $$H[Y \mid X, \mathcal{D}_{\le n}, \mathcal{M}]$$ to denote the average cross-entropy (loss) of the model class $$\mathcal M$$ when trained on $$\mathcal D_{\le n}$$ with $$n$$ samples on the test distribution and $$H[\mathcal{D}_{\le n} \mid \mathcal M]$$ to denote the (negative) LML, we expect:

$$
\frac{H[\mathcal{D}_{\le n} \mid \mathcal M]}{n} \overset{n\to\infty}{\to} H[Y \mid X, \mathcal D _{\infty}, \mathcal M].
$$

This is just saying the averaged (negative) LML converges to generalization loss of the model with “optimal” model parameters as we train on more and more data. In particular, the first few terms will matter less and less. 

Staying informal (but hopefully still correct), we see that for the correlation coefficient, dividing by $$n$$ cancels out:

$$
\begin{aligned}
&\rho_{\frac{H[\mathcal{D}_{\le n} \mid \mathcal M]}{n} ,\, H[Y \mid X, \mathcal D _{\le n}, \mathcal M]} = \\ &\quad =\frac{Cov[\frac{H[\mathcal{D}_{\le n} \mid \mathcal M]}{n} , H[Y \mid X, \mathcal D _{\le n}, \mathcal M]]}{\sqrt{Var[\frac{H[\mathcal{D}_{\le n} \mid \mathcal M]}{n}] \, Var[ H[Y \mid X, \mathcal D _{\le n}, \mathcal M]]}} \\
& \quad = \frac{Cov[H[\mathcal{D}_{\le n} \mid \mathcal M], H[Y \mid X, \mathcal D _{\le n}, \mathcal M]]}{\sqrt{Var[H[\mathcal{D}_{\le n} \mid \mathcal M]] \, Var[ H[Y \mid X, \mathcal D _{\le n}, \mathcal M]]}} = \\
&\quad = \rho_{H[\mathcal{D}_{\le n} \mid \mathcal M], \, H[Y \mid X, \mathcal D _{\le n}, \mathcal M]}  
\end{aligned}
$$

And hence:

$$
\rho_{H[\mathcal{D}_{\le n} \mid \mathcal M], \, H[Y \mid X, \mathcal D _{\le n}, \mathcal M]} = \rho_{\frac{H[\mathcal{D}_{\le n} \mid \mathcal M]}{n}, \, H[Y \mid X, \mathcal D _{\le n}, \mathcal M]} \overset{n\to\infty}{\to} 1
$$

Given this, LML and generalization loss should correlate positively with sufficient data. 

<aside class="box-warning l-body" markdown="1">
☝ The paper observes anti-correlation here, however, which raises questions about the inferences above, about the quality of the LML estimate, or the amount of data that is needed for the correlation to become positive again.
</aside>

<aside class="box-note l-body" markdown="1">
👁️‍🗨️ The authors reply to this, effectively agreeing that the LML indeed has that behavior as can be seen in the fourier model experiment---see (2) in their response below.
</aside>


### DNN Experiments: Validation Loss vs CLML

Lastly, the initially published DNN experiments in the paper did not compute the CLML but the validation loss. This has been fixed in *v2* of the paper.

<aside class="box-note l-body" markdown="1">
☝  Given the previous arguments, however, this is not a big issue. Indeed, the expectation is that CLML behaves like the validation loss anyway.
</aside>

<aside class="box-note l-body" markdown="1">
👁️‍🗨️ The authors have fixed this bug now and present initial experiment results for the affected figure 5b in the main paper. This posts compares the new results between *v1* and *v2* below and *finds that the **BMA validation loss** performs better than the **CLML**.*
</aside>

The [`logcml_` files in the repository](https://github.com/Sanaelotfi/Bayesian_model_comparison/tree/main/Laplace_experiments/cifar) contain the code to compute the CLML for partially trained models. However, instead of computing

$$
\begin{aligned}
\log p(\mathcal D_{\ge m} \mid \mathcal D_{< m}, \mathcal{M} ) \approx \log \sum_{k=1}^K \frac{1}{K}\, p(\mathcal{D}_{\ge m} \mid w_k, \mathcal M ) \\
= \log \sum_{k=1}^K \frac{1}{K}\, \prod_{j=m}^n p(y_j \mid x_j, w_k, \mathcal M ),
\end{aligned}
$$

the code computes:

$$
\begin{aligned}
&\frac{1}{|\mathcal{D}_{\ge m}|}\,\sum_{j=m}^n \log p(\mathcal D_{j} \mid \mathcal D_{< m}, \mathcal{M} ) \approx \\
&\quad =\frac{1}{|\mathcal{D}_{\ge m}|}\,\sum_{j=m}^n \log \sum_{k=1}^K \frac{1}{K}\, p(y_j \mid x_j, w_k, \mathcal M ),
\end{aligned}
$$

which is the validation cross-entropy loss of the BMA (of the model trained with 80% of the training data).

<details markdown="1"><summary><b>Detailed Code Review</b></summary>    
The high-level [code](https://github.com/Sanaelotfi/Bayesian_model_comparison/tree/c6f0da1d49374c0dda6ee743e5b02bcf3e158e96/Laplace_experiments/cifar/logcml_cifar10_resnets.py#L295) that computes the CLML is:

{% highlight python linenos %}
bma_accuracy, bma_probs, all_ys = get_bma_acc(
    net, la, trainloader_test, bma_nsamples, 
    hessian_structure, temp=best_temp
)
cmll = get_cmll(bma_probs, all_ys, eps=1e-4)
{% endhighlight %}

[`get_bma_acc`](https://github.com/Sanaelotfi/Bayesian_model_comparison/tree/c6f0da1d49374c0dda6ee743e5b02bcf3e158e96/Laplace_experiments/cifar/logcml_cifar10_resnets.py#L149) marginalizes over the LA samples before returning `bma_probs`: 

{% highlight python linenos %}
[...]
for sample_params in params:
    sample_probs = []
    all_ys = []
    with torch.no_grad():
        vector_to_parameters(sample_params, net.parameters())
        net.eval()
        for x, y in loader:
            logits = net(x.cuda()).detach().cpu()
            probs = torch.nn.functional.softmax(logits, dim=-1)
            sample_probs.append(probs.detach().cpu().numpy())
            all_ys.append(y.detach().cpu().numpy())
        sample_probs = np.concatenate(sample_probs, axis=0)
        all_ys = np.concatenate(all_ys, axis=0)
        all_probs.append(sample_probs)

all_probs = np.stack(all_probs)
bma_probs = np.mean(all_probs, 0)
bma_accuracy = (np.argmax(bma_probs, axis=-1) == all_ys).mean() * 100

return bma_accuracy, bma_probs, all_ys
{% endhighlight %}

The important line is #18: `bma_probs = np.mean(all_probs, 0)` which marginalizes over the predictions and returns the BMA prediction for each sample.

Finally, [`get_cmll`](https://github.com/Sanaelotfi/Bayesian_model_comparison/tree/c6f0da1d49374c0dda6ee743e5b02bcf3e158e96/Laplace_experiments/cifar/logcml_cifar10_resnets.py#L170) computes the validation loss for each sample independently (after applying a bit of label smoothing):
{% highlight python linenos %}
def get_cmll(bma_probs, all_ys, eps=1e-4):
    log_lik = 0      
    eps = 1e-4
    for i, label in enumerate(all_ys):
        probs_i = bma_probs[i]
        probs_i += eps
        probs_i[np.argmax(probs_i)] -= eps * len(probs_i)
        log_lik += np.log(probs_i[label]).item()
    cmll = log_lik/len(all_ys)
    
    return cmll
{% endhighlight %}
</details>

The DNN experiments in Section 5 and Section 6 of the paper (*v1*) thus did not estimate the CLML per-se but computed the BMA validation loss of a partially trained model (80%) and find that this correlates positively with the test accuracy and test log-likelihood of the fully trained model (at 100%).
This is not surprising because it is well-known that the validation loss of a model trained 80% of the data correlates positively with the test accuracy (and generalization loss).

## Author Response

The following response sadly seems to target the first draft mainly. However, it is also helpful for the final blog post and provides additional context.

<blockquote markdown="1">
Thanks for your interest in our paper and your comments. Here are our comments about the blog as it is currently framed:

(1) Thank you for pointing out a bug in the CLML computation for Figure 5b. We note that this bug is only relevant to a single panel of a single figure in the main text. We have re-run this experiment with the right CLML, and the results, attached here, are qualitatively the same. In summary, it was a very minor part of the paper, and even for that part it did not affect the take-away. We also attach the results of the correlation between the BMA test accuracy and the negative validation loss. You suggest in your post that the validation loss might correlate better with the BMA test accuracy than the CLML given that we use 20 samples for NAS. Our empirical results show the opposite conclusion. Additionally, we are not suggesting the CLML as a replacement to cross-validation but rather as a minor way to modify the LML for improvements in predicting generalization. Finally, we attach results for different sample sizes (20 samples vs. 100 samples) to address your comments on the sample size used to estimate the CLML. As we can see in the figure, the Spearman correlation factor is quite similar. 20 samples appears to provide a reasonable estimate of the CLML for these purposes, and is different from validation loss.

{% capture max-width %}
" style="max-width: 20em;
{% endcapture %}
{% include figure.html path="assets/img/2024-05-07-clml/rebuttal_1.png" max-width=max-width %}
{% include figure.html path="assets/img/2024-05-07-clml/rebuttal_2.png" max-width=max-width %}
{% include figure.html path="assets/img/2024-05-07-clml/rebuttal_3.png" max-width=max-width %}

(2) Your post currently opens by suggesting that there is something wrong with our experiments, likely either an LML approximation or a CLML issue, because we note that the LML correlates more poorly with generalization for larger datasets (where “large” is relative in the context of a specific experiment). A few points here: (i) this result is actually completely expected. The LML is in fact non-monotonic in how well it predicts generalization. For small datasets, the prior should be reasonably predictive of generalization. For intermediate datasets, the first terms in the LML decomposition have a negative effect on the correlation with generalization. For asymptotically large datasets, the first terms have a diminishing effect, and we get a consistent estimator; (ii) almost all of our experiments are exact, and we see this behaviour in the exact experiments for the Fourier model. For example, for the Fourier feature experiment in Fig 4(d), LML picks the better generalizing model for n < 50 and n > 296. For n in [50, 296] it picks the wrong model. For large neural network models, it is reasonable that the exact LML could pick the wrong model for CIFAR-sized datasets. (iii) any potential issues with the CLML are not relevant to these considerations, which are about the behaviour of the LML.

(3) Your post currently suggests that issues with approximate inference could be responsible for our take-aways, rather than issues with the LML in general. But as we note in (2), almost all of our experiments use the exact LML and CLML: the density model, Fourier features, Gaussian processes, and deep learning exps on DKL, and there was never any bug associated with CLML computation in these experiments. The takeaways for the Laplace experiments are consistent with the exact experiments, and also expected, as above. While it’s true that the CLML can be estimated more effectively than the LML for the Laplace experiments, this is actually an advantage of the CLML that we note in the paper. The LML results also stand on their own, as we discuss above.

(4) Your post places a lot of importance on Figure 5, as if it is the main result of the paper and our main “DNN” experiments. We stand by the results of Figure 5, but it is a relatively minor component of the paper. As we’ve mentioned most of our results are exact, including our DKL experiments, which are certainly the most substantial DNN experiments, with practically exciting results for transfer and few-shot learning. The DKL experiments are actually where we expect the CLML to be practically useful, and currently they seem to be overlooked in the post.

(5) The blog seems to question the learning curve experiments, but these experiments in Figure 4 are exact, with no Laplace approximation, and relatively straightforward.

(6) Your post seems to be negative about the CLML, presenting its similarity with cross-validation as a potential drawback, and implying the skepticism about the CLML should affect the interpretation of our take-aways. Two points here: (i) as above, the CLML is independent of most of our take-aways, which are about the properties of the LML; (ii) our goal with the CLML was not to introduce something starkly different from cross-validation, but to show how a very minor modification to the LML could improve alignment with generalization. Moreover, the DKL CLML results are quite promising as an efficient way to do gradient based estimation of a large number of hyperparameters.

(7) The blog opens as if it is leading up to some fatal flaw. But as above, (i) the LML considerations are independent of the CLML, (ii) most of the experiments are exact, (iii) the trends for the exact and approximate inference procedures are the same and are naturally understandable and explainable, such as the non-monotonic trend in how well the LML correlates with generalization, and (iv) the CLML bug only affected Figure 5, panel b, and when it’s corrected the qualitative take-away is the same as before.

We appreciate your interest and effort in reading the paper, and we think your questions will improve the clarity of the paper, which we have updated with an acknowledgement to you. Given the above considerations, we do think there would need to be substantial revisions to the blog post to accurately and fairly reflect the paper. We would appreciate being able to see the revisions before it’s posted.

Best wishes,\\
Sanae, Pavel, Greg, Micah, Andrew
</blockquote>

###  Ablation: CLML vs. BMA Validation Loss vs. (non-BMA) Validation Loss

Let us examine the new results: 

In the three panels below, two panels show test accuracy vs. validation loss; one shows test accuracy vs. CLML. The left-most panel is the BMA test accuracy vs. (negative) BMA validation loss, the middle panel is vs. the CLML, and the right-most panel is vs. the (negative) non-BMA validation loss. 

Note that the left-most panel is from *v1*, which was accidentally computing the BMA validation loss, and whose axis label is adapted here from *v1* for clarity. The two other plots are from *v2* after fixing the bug. See commits [here](https://github.com/Sanaelotfi/Bayesian_model_comparison/commit/a579aa292723dc20a6105ec8f4fff1045dd9a9fd) for fixing the CLML estimation and [here](https://github.com/Sanaelotfi/Bayesian_model_comparison/commit/3fa8ca2ecb314ee881f6c95a602ef58b9ccd3620) for computing the non-BMA validation loss. 

{% capture width %}
" style="width: 20em;
{% endcapture %}
<div class="row mt-3">
  <div class="col-sm mt-3 mt-md-0">
    {% include figure.html path="assets/img/2024-05-07-clml/bmsmlg_bma_validation_loss.svg" class="img-fluid" caption="BMA Neg Validation Loss" width=width %}
  </div>
  <div class="col-sm mt-3 mt-md-0">
    {% include figure.html path="assets/img/2024-05-07-clml/bmsmlg_clml.svg" class="img-fluid" caption="CLML" width=width %}
  </div>
</div>
<div class="row mt-3">
  <div class="col-sm mt-3 mt-md-0">
    {% include figure.html path="assets/img/2024-05-07-clml/bmsmlg_validation_loss.svg" class="img-fluid" caption="Validation Loss" width=width %}
  </div>  
{% capture width %}
" style="width: 5em;
{% endcapture %}
  <div class="col-sm mt-3 mt-md-0">
    {% include figure.html path="assets/img/2024-05-07-clml/bmsmlg_plot_legend.svg" class="img-fluid" caption="Leg" width=width %}
  </div>
</div>

At first glance, there might be an observer effect in the experiments for the validation loss. The BMA validation loss in *v1* performs better than the CLML in *v2*, while the non-BMA validation loss in *v2* underperforms the CLML in *v2*.
When asked about it, the authors pushed the respective code (see link above) and explained that the updated, right-most panel computes the **non-BMA** validation loss, i.e., without LA samples. 
It seems surprising that there is such a difference between the non-BMA validation loss and BMA validation loss: *the non-BMA validation loss is more than one nat worse on average than the BMA validation loss, based on visual inspection*. Note that the plots here and in the paper compute the average CLML and average validation loss and are thus directly comparable.

The authors said in their response that:

> You suggest in your post that the validation loss might correlate better with the BMA test accuracy than the CLML given that we use 20 samples for NAS. Our empirical results show the opposite conclusion.

This is only partially true. 
The BMA validation loss (which was accidentally computed in *v1* instead of the CLML) correlates very well with the BMA test accuracy.
This is not surprising given that this is the frequentist purpose of using validation sets. If validation sets were not correlating well with the test accuracy, we would not be using them in practice. 🤗 As such, this raises the question why the non-BMA validation loss correlates negatively with the BMA test accuracy for ResNets and overall in the *v2* results.
Thus, only the non-BMA validation loss supports the now opposite conclusion in *v2* of the paper and in the authors' response. 

Yet what is also surprising is how well the BMA validation loss does vs. the CLML:

<aside class="box-error l-body" markdown="1">
🔥 The BMA validation loss correlates even better than the CLML with BMA test accuracy: the Spearman's rank correlation is better for the BMA validation loss than for the CLML across all $$\lambda$$s.

Does this mean we should use the BMA validation loss rather than estimating the CLML for DNNs?
</aside>

### Ablation: LA Sample Size

Secondly, when we compare the reported values between BMA validation loss and CLML, we notice that the CLML is lower than the BMA validation loss by half a nat for $$\lambda=10^2$$ and generally for CNNs.

<aside class="box-note l-body" markdown="1">
👉 One would expect that the (average) CLML to be better (higher) than the negative BMA validation loss and not worse (lower) because the CLML ought to be able to compress the validation set better using the joint predictive distribution than the marginal predictive distribution for the validation loss.  


Intuitively, the CLML can take the validation data into account in a way that the validation loss cannot given the sequential conditioning of the former. 

Hence, the CLML should generally be lower-bounded by the (BMA) validation loss and upper-bounded by the (BMA) test loss up to sample variance, assuming there are not many outliers in the data.
The fact that the CLML is worse points toward the LA samples not being able to capture the implicit posterior in the joint predictive distribution. (This is a bit handwavy but is a reasonable interpretation, as we will see below when taking into account additional evidence.)
</aside>

However, it seems, even though the new experiments in *v2* are supposed to reproduce the ones from *v1*, and we can assume that the same model checkpoints were used for re-evaluation (as retraining is not necessary), both CLML and non-BMA validation loss are off by about half a nat for the CNNs. As such, the above consideration might hold but might not provide the answer here.

Instead, we overlay the non-BMA validation loss and the CLML plots, both from *v2*, with a "difference blend": it shows the absolute difference between the colors for overlapping data points (the circles 🔴 and triangles 🔺), leading to black where there is a match, negative (green-ish) color for CLML, and positive (sepia) color for validation losses. The background grids were used to match the plots, but we hid the ones from CLML afterward---as such, the strong overlay is because the values are so close.

{% capture width %}
" style="width: 25em;
{% endcapture %}
{% include figure.html path="assets/img/2024-05-07-clml/bmsmlg_difference_overlay_plot.svg" class="img-fluid" width=width %}


Surprisingly---or rather as predicted when the LA does not really do much---it turns out that the validation loss for the CNNs (🔴) mostly fully matches the estimated CLML with 20 LA samples following a visual inspection. To be more precise, either the models have already sufficiently converged, or the CLML estimate is not actually capturing the correlations between points and thus ends up being very similar to the validation loss.

<aside class="box-warning l-body" markdown="1">
As mentioned before, it could be insightful to actually compute the difference between CLML and BMA validation loss explicitly. Two things are not clear to me given the results in *v1* and *v2*:

1. Why is there a difference in nats between BMA validation loss in *v1* and the non-BMA validation loss and CLML in *v2*? Given that CLML matches the non-BMA validation loss for CNNs in *v2*, the BMA validation loss ought to be close to that, too.

2. Why is there different behavior for ResNets (🔺) in the BMA validation loss in *v1* and the CLML in *v2* compared to the non-BMA validation loss in *v2*? The low correlation coefficients of the non-BMA validation loss seem to come from that, as *v2* also explains in an added paragraph in appendix H.

</aside>

{% capture width %}
" style="width: 25em;
{% endcapture %}
{% include figure.html path="assets/img/2024-05-07-clml/rebuttal_3.png" class="img-fluid" width=width %}


This changes the interpretation of the sample ablation in the author's response. The ablation shows no difference between 20 and 100 LA samples, with 100 LA even samples having a slightly lower rank correlation. So it seems 5 times more LA samples are not sufficient to make a difference, or the Laplace posterior cannot capture the posterior as well as hoped. It would be interesting to examine this further. Kirsch et al (2022)<d-cite key="kirsch2022marginal"></d-cite> reported running toy experiments on MNIST with 10,000 MC Dropout samples without achieving good adaptation. Laplace approximation is not MC Dropout, and this is speculation, but it seems in agreement. Notwithstanding the compute cost and feasibility, could posterior samples using HMC or similar more principled methods provide better estimates? 

Tweets by [@RobertRosenba14](https://twitter.com/RobertRosenba14) and [@stanislavfort](https://twitter.com/stanislavfort) also explain why sampling might not be fruitful for estimating joint distributions and computing the CLML in high-dimensional spaces:

{% twitter https://twitter.com/stanislavfort/status/1529865444701577216 %}
{% twitter https://twitter.com/RobertRosenba14/status/1517465854157500419 %}

Given the above, it is fair to say that the estimate of the CLML is probably not as good as hoped, and further experiments might be needed to tease out when the CLML provides more value than the BMA validation loss. Note, however, that this question has not been explicitly examined in the paper. Instead, the paper only compares LML and CLML with distinct estimation methods for DNNs.

## Conclusion

It would be interesting to determine how the CLML estimated via LA compares with the LML estimated via LA. Similarly, comparing the CLML to the validation loss would be interesting.
(However, the expectation is that an estimate via sampling will not be different very different from the validation loss simply because sampling-based approaches do not seem to be working well for estimating joint predictive distributions.)

<aside class="box-error l-body" markdown="1">
🔥 Given the overall results of CLML vs. LML and the observation that the BMA validation loss seems to work very well for DNNs, even better than the CLML estimate, it might be worth ablating the performance of the validation loss for DKL and other methods as a separate research question. Then, we can see what we gain from the CLML exactly. 
*Maybe a validation set "is all you need"---after all, the validation loss is easier to estimate.* But then, we knew all along that validation sets work really well for DNNs, despite them being less principled.
</aside>

Further, the choice of the cutoff parameter $$m$$ in CLML and its impact on the results could be explored---while the authors mention that larger values of $$m$$ generally lead to better performance, it would be helpful to have more guidance on how to select $$m$$ in practice. Likewise (and more details on this follow below), a comparison of CLML with other model selection criteria beyond ML, such as cross-validation or information criteria like AIC and BIC.

While this post raises questions about the comparative advantages of CLML over the traditional validation loss, it is important to recognize that ongoing research may uncover scenarios where CLML's Bayesian foundations and theoretical underpinnings provide distinct benefits. Future studies could illuminate these contexts and further validate the utility of CLML in model selection and evaluation. 
Alternative model selection criteria such as AIC, BIC, or cross-validation also play a significant role in the field. Each method comes with its own set of assumptions and trade-offs, and the choice of the best criterion may depend on the specific application and data characteristics.
Thus, despite the critiques presented, which also focused in parts only on the experimental results for DNNs, the paper makes a valuable contribution to the field by rigorously examining the role of LML in model selection beyond DNNs and introducing the concept of CLML. Its thorough analysis and innovative approach provide a foundation for future research and potential advancements in Bayesian model evaluation.

<aside class="box-note l-body" markdown="1">
👉 Beyond providing a more principled view on using a validation set and a beautiful exposition of previous literature and various pitfalls and use-cases, this paper surfaces exciting research questions---especially in the small-data regime. Several follow-up experiments could provide further insights.
</aside>

Overall, the paper makes a significant contribution to the field of Bayesian model selection by questioning the reliance on the LML and proposing an alternative that may be better suited for deep learning. The empirical results are compelling, and the paper opens up avenues for further research on model selection criteria that are aligned with generalization performance.

