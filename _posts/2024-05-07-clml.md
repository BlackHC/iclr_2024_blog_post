---
layout: distill
title: >-
  On Bayesian Model Selection: The Marginal Likelihood, Cross-Validation, and Conditional Log Marginal Likelihood
description: >-
  Bayesian model selection has long relied on the marginal likelihood and related quantities, often motivated by the principle of Occam's razor. This blog post critically examines the conventional focus on the marginal likelihood and related quantities for Bayesian model selection as a direct consequence of Occam's razor. However, the suitability of these criteria depends on the specific context and goals of the modeling task. 
  We introduce the concepts of log marginal likelihood (LML), cross-validation, and the recently introduced conditional log marginal likelihood (CLML), highlighting their connections and differences through information-theoretic lenses. 
  Through thought experiments and empirical observations, we explore the behavior of these model selection criteria in different data regimes under model misspecification and prior-data conflict, finding that the conditional marginal cross-entropy, closely related to cross-validation, is often more reliable for optimizing generalization performance. We review relevant literature comparing the CLML and validation loss for deep neural networks, and using a toy Bayesian linear regression, we demonstrate that all the discussed quantities can fail to reliably predict generalization. 
  Our takeaways are that: there is no one-size-fits-all solution; the choice depends on the specific context and goals; and in the future we should take into account model complexity as well and not assume a uniform model prior.
  While the post is limited by the need for more rigorous theoretical justification, a broader range of models and datasets (and deeper engagement with philosophical implications), it questions the uniqueness of the (conditional) log marginal likelihood and encourages critical thinking about its foundations, aiming for a more nuanced understanding of Bayesian model selection.
date: 2024-05-07
future: true
htmlwidgets: true
tags:
- Bayesian Neural Network
- Generalization
- Log Marginal Likelihood
- Conditional Log Marginal Likelihood
- Information Theory
- Model Evaluation
- Model Selection

authors:
  - name: Andreas Kirsch
    url: "https://www.blackhc.net"
    affiliations:
      name: University of Oxford<sup>–2023</sup>

# must be the exact same name as your blogpost
bibliography: 2024-05-07-clml.bib  

# Add a table of contents to your post.
#   - make sure that TOC names match the actual section names
#     for hyperlinks within the post to work correctly. 
#   - please use this format rather than manually creating a markdown table of contents.
toc:
  - name: "Introduction"
    subsections:
      - name: "(Bayesian) Model Selection"
  - name: "Background: Information-Theoretic Notation and Concepts"  
    subsections:
      - name: "Expressing Occam's Razor in Information-Theoretic Terms"
  - name: "Hyperparameter Learning and Model Selection"
    subsections:  
      - name: "Model Parameters"
      - name: "Bayesian Model Averaging"
      - name: "Marginal Likelihood and Estimation"
  - name: "Datasets instead of Individual Data Points"
    subsections:
      - name: "Joint Marginal Information and Cross-Entropy"
      - name: "Marginal Information and Cross-Entropy"  
      - name: "Marginal Cross-Entropy vs Joint Cross-Entropy"
  - name: "Intermediate Comparison"
  - name: "Different Data Regimes"
    subsections:
      - name: "Model Misspecification"
      - name: "Infinite Data Limit"
      - name: "Prior-Data Conflict"
      - name: "Anti-Correlated Model Misspecification and Prior-Data Conflict"
  - name: "Approximating the (Cross-)Validation Loss"
  - name: "The Big Comparison"
  - name: "Literature Review"
    subsections:
      - name: "Fong and Holmes (2020): \"On the marginal likelihood and cross-validation\""
      - name: "Lyle et al. (2020) and Ru et al. (2021): Training speed and model selection"
      - name: "Lotfi et al. (2022/2023): \"Bayesian Model Selection, the Marginal Likelihood, and Generalization\""
  - name: "A Simple Toy Experiment"
    subsections:
      - name: "Experimental Setup"
      - name: "Results"
  - name: "A Narrow but Deep Dive into \"Bayesian Model Selection, the Marginal Likelihood, and Generalization\""
    subsections:  
      - name: "Use Cases and Pitfalls of the LML"
      - name: "The \"Conditional Marginal Likelihood\" in Lotfi et al. (2022/2023)"
      - name: "Estimating the CLML and LML via the Laplace Approximation"
      - name: "DNN Experiments: Validation Loss vs. CLML"
  - name: "Conclusion"
  - name: "Appendix"
    subsections:
      - name: "Detailed Code Review of the DNN Experiments in Lotfi et al. (2022/2023)"
      - name: "Author Response from 2022"
      - name: "Ablation: CLML vs. BMA Validation Loss vs. (non-BMA) Validation Loss"
      - name: "Ablation: LA Sample Size"
# Below is an example of injecting additional post-specific styles.
# This is used in the 'Layouts' section of this post.
# If you use this post as a template, delete this _styles block.
_styles: >
  .fake-img {
    background: #bbb;
    border: 1px solid rgba(0, 0, 0, 0.1);
    box-shadow: 0 0px 4px rgba(0, 0, 0, 0.1);
    margin-bottom: 12px;
  }
  .fake-img p {
    font-family: monospace;
    color: white;
    text-align: left;
    margin: 12px 0;
    text-align: center;
    font-size: 16px;
  }
  .box-note, .box-warning, .box-error, .box-important {
    padding: 15px 15px 15px 10px;
    margin: 20px 20px 20px 5px;
    border: 1px solid #eee;
    border-left-width: 5px;
    border-radius: 5px 3px 3px 5px;
  }
  d-article .box-note {
    background-color: #eee;
    border-left-color: #2980b9;
  }
  d-article .box-warning {
    background-color: #fdf5d4;
    border-left-color: #f1c40f;
  }
  d-article .box-error {
    background-color: #f4dddb;
    border-left-color: #c0392b;
  }
  d-article .box-important {
    background-color: #d4f4dd;
    border-left-color: #2bc039;
  }
  html[data-theme='dark'] d-article .box-note {
    background-color: #333333;
    border-left-color: #2980b9;
  }
  html[data-theme='dark'] d-article .box-warning {
    background-color: #3f3f00;
    border-left-color: #f1c40f;
  }
  html[data-theme='dark'] d-article .box-error {
    background-color: #300000;
    border-left-color: #c0392b;
  }
  html[data-theme='dark'] d-article .box-important {
    background-color: #003300;
    border-left-color: #2bc039;
  }
  html[data-theme='dark'] d-article aside {
    color: var(--global-text-color) !important;
  }
  html[data-theme='dark'] d-article blockquote {
    color: var(--global-text-color) !important;
  }
  html[data-theme='dark'] d-article summary {
    color: var(--global-text-color) !important;
  }
  d-article aside * {
    color: var(--global-text-color) !important;
  }
  d-article p {
    text-align: justify;
    text-justify: inter-word;
    -ms-hyphens: auto;
    -moz-hyphens: auto;
    -webkit-hyphens: auto;
    hyphens: auto;
  }
  d-article aside {
    border: 1px solid #aaa;
    border-radius: 4px;
    padding: .5em .5em 0;
    font-size: 90%;
  }
  d-article aside p:first-child {
      margin-top: 0;
  }
  d-article details {
    border: 1px solid #aaa;
    border-radius: 4px;
    padding: .5em .5em 0;
  }
  d-article summary {
    font-weight: bold;
    margin: -.5em -.5em 0;
    padding: .5em;
    display: list-item;
  }
  d-article details[open] {
    padding: .5em;
  }
  d-article details[open] summary {
    border-bottom: 1px solid #aaa;
    margin-bottom: .5em;
  }
  html[data-theme='dark'] d-article blockquote {
    border-left-color: #f1c40f;
  }
  @media (min-width: 1025px) {
    d-article d-contents {
      height: 0px;
    }
  }
---


{% raw %}
<div style="display: none;">
$$\require{mathtools}
\DeclareMathOperator{\opExpectation}{\mathbb{E}}
\newcommand{\E}[2]{\opExpectation_{#1} \left [ #2 \right ]}
\newcommand{\simpleE}[1]{\opExpectation_{#1}}
\newcommand{\MidSymbol}[1][]{\:#1\:}
\newcommand{\given}{\MidSymbol[\vert]}
\DeclareMathOperator{\opmus}{\mu^*}
\newcommand{\IMof}[1]{\opmus[#1]}
\DeclareMathOperator{\opInformationContent}{H}
\newcommand{\ICof}[1]{\opInformationContent[#1]}
\newcommand{\xICof}[1]{\opInformationContent(#1)}
\DeclareMathOperator{\opEntropy}{H}
\newcommand{\Hof}[1]{\opEntropy[#1]}
\newcommand{\xHof}[1]{\opEntropy(#1)}
\DeclareMathOperator{\opMI}{I}
\newcommand{\MIof}[1]{\opMI[#1]}
\DeclareMathOperator{\opTC}{TC}
\newcommand{\TCof}[1]{\opTC[#1]}
\newcommand{\CrossEntropy}[2]{\opEntropy(#1 \MidSymbol[\Vert] #2)}
\newcommand{\iCrossEntropy}[3]{\opEntropy_{#1 \Vert #2}[#3]}
\DeclareMathOperator{\opKale}{D_\mathrm{KL}}
\newcommand{\Kale}[2]{\opKale(#1 \MidSymbol[\Vert] #2)}
\newcommand{\iKale}[3]{\opKale_{,\, #1 \Vert #2}[#3]}
\DeclareMathOperator{\opJSD}{D_\mathrm{JSD}}
\newcommand{\JSD}[2]{\opJSD(#1 \MidSymbol[\Vert] #2)}
\DeclareMathOperator{\opp}{p}
\newcommand{\pof}[1]{\opp(#1)}
\newcommand{\hpof}[1]{\hat{\opp}(#1)}
\newcommand{\pcof}[2]{\opp_{#1}(#2)}
\newcommand{\hpcof}[2]{\hat\opp_{#1}(#2)}
\DeclareMathOperator{\opq}{q}
\newcommand{\qof}[1]{\opq(#1)}
\newcommand{\hqof}[1]{\hat{\opq}(#1)}
\newcommand{\qcof}[2]{\opq_{#1}(#2)}
\newcommand{\varHof}[2]{\opEntropy_{#1}[#2]}
\newcommand{\xvarHof}[2]{\opEntropy_{#1}(#2)}
\newcommand{\varMIof}[2]{\opMI_{#1}[#2]}
\newcommand{\w}{\boldsymbol{\theta}}
\newcommand{\W}{\boldsymbol{\Theta}}
\newcommand{\h}{\boldsymbol{\phi}}
\newcommand{\hopt}{\boldsymbol{\h^\star}}
\newcommand{\H}{\boldsymbol{\Phi}}
\DeclareMathOperator{\opf}{f}
\newcommand{\fof}[1]{\opf(#1)}
\newcommand{\xset}[3]{(\x_n^{#1})_{n=#2}^{#3}}
\newcommand{\xNset}{(\x_n)_{n=1}^N}
\newcommand{\XNtuple}{(\X_n)_{n=1}^N}
\newcommand{\xNtuple}{(\x_n)_{n=1}^N}
\newcommand{\XNset}{\{\X_n\}_{n=1}^N}
\newcommand{\xNset}{\{\x_n\}_{n=1}^N}
\newcommand{\XNsetk}{\{\X_n\}_{n=N-k+1}^N}
\newcommand{\XNkset}{\{\X_n\}_{n=1}^{N-k}}
\newcommand{\XNoset}{\{\X_n\}_{n=1}^{N-1}}
\newcommand{\y}{y}
\newcommand{\Y}{Y}
\newcommand{\L}{\boldsymbol{L}}
\newcommand{\x}{\boldsymbol{x}}
\newcommand{\X}{\boldsymbol{X}}
\newcommand{\oppdata}{\hat{\opp}_{\text{data}}}
\newcommand{\pdata}[1]{\hpcof{\text{data}}{#1}}
\newcommand{\normaldist}[1]{\mathcal{N}(#1)}
\newcommand{\wstddev}{\sigma_\w}
\newcommand{\noisestddev}{\sigma_\text{noise}}
\newcommand{\Dataset}{\mathcal{D}}
\newcommand{\Dtrain}{\Dataset_{\text{train}}}
\newcommand{\Dval}{\Dataset_{\text{val}}}
$$
</div>
{% endraw %}

## Introduction

<!-- Introduction:
The introduction should provide context for the importance of model selection in Bayesian machine learning and briefly introduce the key concepts of LML, cross-validation, and CLML. It should also outline the structure and main points of the blog post. -->

Model selection is a crucial aspect of machine learning, as it allows us to choose the most appropriate model for a given task. In the Bayesian setting, the marginal likelihood has been a popular tool for model selection and hyperparameter learning, often motivated by the principle of Occam's razor. However, the suitability of the marginal likelihood depends on the specific context and goals of the modeling task.

Recently, the paper "Bayesian Model Selection, the Marginal Likelihood, and Generalization," was accepted as Long Oral at ICML 2022, examines the importance and challenges of model selection in machine learning, focusing on the log marginal likelihood (LML) and proposing a variant, the conditional log marginal likelihood (CLML). This has renewed interesting in the role of the marginal likelihood and its alternatives for model selection. The authors argue that while LML is a useful tool for hypothesis testing, it may not be the best metric for predicting the generalization performance of trained models or learning hyperparameters. They introduce CLML as a potential improvement and demonstrate its effectiveness across various settings, including density models, Fourier features, Gaussian Processes, and deep neural networks.

In this blog post, inspired by above paper, we rederive insights that challenge the conventional focus on the marginal likelihood and related quantities for Bayesian model selection. We argue that the choice of model selection criterion should be guided by the context and the desired outcomes, rather than being seen as a direct consequence of Occam's razor. We highlight that many of recent proposed metrics for model selection, including CLML, are closely related to cross-validation, and have failure cases that can be explained by considering model misspecification and prior-data conflicts. Overall, the choice between them should be based on the specific requirements of the task at hand.

We begin by discussing the foundations of model selection, including the role of Occam's razor and its relationship to maximum likelihood estimation (MLE) and maximum a posteriori (MAP) estimation. We then introduce the concepts of log marginal likelihood (LML), cross-validation, and conditional log marginal likelihood (CLML), highlighting their connections and differences.

Through a series of thought experiments and empirical observations, we explore the behavior of these model selection criteria in various scenarios, such as model misspecification, prior-data conflict, and different data regimes. We find that the conditional marginal cross-entropy, which is closely related to cross-validation, is often a more reliable choice when the primary objective is to optimize generalization performance. On the other hand, the conditional joint marginal cross-entropy (permutation-invariant negative CLML) may be preferable when the focus is on sequential prediction and online learning. At the same time, the joint marginal information (negative LML) is rarely the right choice for model selection.

We also review relevant literature, including the work of Fong and Holmes (2020) on the connection between the LML and cross-validation, the training speed estimators by Lyle et al. (2020), and the experiments of Lotfi et al. (2022/2023), comparing the CLML and validation loss for deep neural networks (DNNs). These studies provide valuable insights into the strengths and limitations of different model selection criteria.

Throughout the post, we emphasize the importance of considering the context, available data, and desired outcomes when selecting the most appropriate metric for model selection and hyperparameter tuning. By questioning the uniqueness of the (conditional) joint marginal likelihood and encouraging critical thinking about the foundations of these quantities, we hope to foster a more nuanced understanding of Bayesian model selection.

## (Bayesian) Model Selection

In our daily lives, we're often faced with choices that require us to sift through competing explanations or decisions. Imagine you hear your doorbell ring. You might think it's the delivery you've been waiting for, a neighbor dropping by, or perhaps you didn't hear anything at all, and it was just your imagination. In deciding between these options, you're likely to lean towards the simplest explanation that aligns with your expectations—say, the long-awaited delivery. This inclination towards simplicity has a formal counterpart in scientific discovery and machine learning, known as [Occam’s razor](https://en.wikipedia.org/wiki/Occam%27s_razor):

<aside class="box-important l-body" markdown="1">
*Occam’s razor* is the principle that, all else being equal, the simplest explanation tends to be the right one.
</aside>

This concept is illustrated using [an example from chapter 28](https://www.inference.org.uk/itprnn/book.pdf#page=355) of David MacKay’s seminal book, ["Information Theory, Inference, and Learning Algorithms”](http://www.inference.org.uk/mackay/itila/book.html), where the essence of selecting between models based on their evidence is laid out succinctly.

{% capture caption %}
Excerpt from page 343 in David MacKay’s "Information Theory, Inference, and Learning Algorithms.”
{% endcapture %}
{% include figure.html path="assets/img/2024-05-07-clml/mackay_343.png" zoomable=True class="img-fluid" caption=caption alt="Occam’s razor --- How many boxes are in the picture (figure 28.1)? In particular, how many boxes are in the vicinity of the tree? If we looked with x-ray spectacles, would we see one or two boxes behind the trunk (figure 28.2)? (Or even more?) Occam’s razor is the principle that states a preference for simple theories. ‘Accept the simplest explanation that fits the data’. Thus according to Occam’s razor, we should deduce that there is only one box behind the tree. Is this an ad hoc rule of thumb? Or is there a convincing reason for believing there is most likely one box? Perhaps your intuition likes the argument ‘well, it would be a remarkable coincidence for the two boxes to be just the same height and colour as each other’. If we wish to make artificial intelligences that interpret data correctly, we must translate this intuitive feeling into a concrete theory." 

title="Occam’s razor --- How many boxes are in the picture (figure 28.1)? In particular, how many boxes are in the vicinity of the tree? If we looked with x-ray spectacles, would we see one or two boxes behind the trunk (figure 28.2)? (Or even more?) Occam’s razor is the principle that states a preference for simple theories. ‘Accept the simplest explanation that fits the data’. Thus according to Occam’s razor, we should deduce that there is only one box behind the tree. Is this an ad hoc rule of thumb? Or is there a convincing reason for believing there is most likely one box? Perhaps your intuition likes the argument ‘well, it would be a remarkable coincidence for the two boxes to be just the same height and colour as each other’. If we wish to make artificial intelligences that interpret data correctly, we must translate this intuitive feeling into a concrete theory."%}

But how can we express this using mathematics? 

In the next section, we will use information-theoretic concepts to formalize Occam's razor and connect it to the maximum likelihood estimation (MLE) and maximum-a-posteriori (MAP) estimation approaches. This formalization highlights that Occam's razor, as a general principle favoring simplicity, can motivate various techniques, including less Bayesian ones. Thus using Occam's razor as the justification for Bayesian model selection may not be as strong an argument as it initially appears.

On the other hand, one might equally argue that Occam's razor, when properly applied in a Bayesian framework, encapsulates a more nuanced notion of complexity. In this view, the Bayesian formulation of Occam's razor favors models that balance goodness-of-fit with model complexity, as measured by the model's ability to compress the data. This perspective aligns with the minimum description length (MDL) principle, which states that the best model is the one that minimizes the total description length of the model and the data given the model.

**From Philosophical Principle to Mathematical Statement**

Let's first connect Occam's razor to **Maximum Likelihood Estimation (MLE)** before diving deeper into the background and (Bayesian) model selection.

In information theory, the information content of an event $$x$$ is defined as $$-\log_2 \pof{x}$$---this is also called *Shannon's information content*---where $$\pof{x}$$ is the probability of that event occurring according to a given model. We use the base $$2$$ for logarithms and measure information in *bits (binary digits)*. For the rest of the post, we will drop the basis of the logarithm. 
The information content measures the optimal encoding length in bits for the event $$x$$ under the model specified by its probability distribution $$\pof{\cdot}$$. In the context of probabilistic modeling, variables that cannot be directly observed are called *latent variables*. Occam's razor suggests that we should prefer simpler explanations for latent variables, given the *observed data*.

Consider a model with a latent variable $$z$$ and observed data $$x$$. The model specifies a probability distribution $$\pof{z \given x}$$. According to Occam's razor, we prefer simpler explanations, which correspond to smaller values of $$-\log \pof{z \given x}$$. Using Bayes' theorem, we can rewrite this as:

$$\text{minimize } -\log \pof{z \given x} = -\log \pof{x \given z} - \log \pof{z} + \log \pof{x}.$$

Given that $$\pof{x}$$ is independent of $$z$$, we can omit it from our objective. Additionally, if we posit a uniform (or non-informative prior) for $$z$$, implying that all potential values of $$z$$ are equally probable before observing $$x$$, then $$\pof{z}$$ becomes constant and can also be dropped from our objective. This simplifies our preference to:

$$\text{minimize } -\log \pof{x \given z}.$$

Equivalently, we can maximize $$\pof{x \given z}$$, which is the *likelihood* of the observed data $$x$$ given the latent variable $$z$$. 

<div class="l-gutter" markdown="1" style="height: 0px">
<aside>
As MacKay notes, \(\pof{x \given z}\) as a function in \(z\) is called a likelihood, while \(\pof{x \given z}\) as a function in \(x\) is called a probability.
</aside>
</div>

When making a decision and selecting a single value for $$z$$, this leads to the maximum likelihood estimation (MLE) approach.

In summary, the connection between Occam's razor and MLE relies on the following assumptions:

1. Shannon's information content is how we measure complexity.
2. The prior distribution for the latent variables is uniform (or uninformative).
3. Simpler explanations, as measured by the information content, are preferred (Occam's razor).

Under these assumptions, the preference for simpler explanations leads to the MLE approach, where more likely values of the latent variable given the observed data are preferred.

Optimizing the MLE is a common approach and the most straightforward when we have a parametric model because we can directly optimize the likelihood function. Still, this is not easy for deep learning models because they have a large number of parameters and the loss function is non-convex.

### Maximum Likelihood and Maximum-a-Posteriori Estimation

However, the assumption of a uniform or non-informative prior for the latent variables is not always valid or desirable. In many cases, we have prior knowledge about the latent variables that can be incorporated into the model. This leads to the **Maximum-A-Posteriori (MAP) Estimation** as an alternative to MLE.

In MAP estimation, $$\pof{z}$$ is not constant, so we cannot drop it---we can still drop $$\pof{x}$$, however---and maximize the joint distribution $$\pof{z, x}$$, or equivalently:

$$\text{minimize } -\log \pof{x, z}=-\log \pof{x \given z} - \log \pof{z}.$$

Before we go further, let us delve into some important background notation.

## Background: Information-Theoretic Notation and Concepts

<!-- TK ask if I should cite the other post TK -->

Information theory deals with the communication of information<d-footnote>See the excellent <a href="https://colah.github.io/posts/2015-09-Visual-Information/">"Visual Information Theory"</a> by Chris Olah for a visual introduction to information theory.</d-footnote>. In this post, we use a unified information-theoretic notation to express various quantities related to probability distributions and their relationships<d-footnote>It largely follows "<a href="https://arxiv.org/abs/2106.12062">A Practical & Unified Notation for Information-Theoretic Quantities in ML</a>".</d-footnote>. Here are some key concepts we will use:

The **information content** of an event $$x$$ is denoted as $$\Hof{x}$$ and is defined as $$-\log_2 \pof{x}$$. It represents the minimum amount of information needed to describe the occurrence of $$x$$ given an underlying probability distribution.
In machine learning, this information content is often used as a minimization objective, represented as the negative log-likelihood or cross-entropy when averaged over a dataset.

The **entropy** $$\Hof{X}$$ of a random variable $$X$$ is the expectation of its information content:

$$
\Hof{X} \triangleq \E{\pof{x}}{\Hof{x}} = \E{\pof{x}}{-\log \pof{x}}.
$$

The entropy measures the average amount of information needed to describe the random variable $$X$$. It provides a measure of uncertainty or randomness associated with $$X$$. We can similarly define the entropy of a conditional distribution $$\Hof{X \given Y}$$ and the joint entropy $$\Hof{X, Y}$$.

The **mutual information** $$\MIof{X;Y}$$ between two random variables $$X$$ and $$Y$$ is a measure of the amount of information that one random variable contains about the other. It is defined as:

$$
\begin{aligned}
\MIof{X;Y} & \triangleq \Hof{X} - \Hof{X \given Y} \\
&= \Hof{Y} - \Hof{Y \given X} \\
&= \Hof{X} + \Hof{Y} - \Hof{X, Y}.
\end{aligned}
$$

We will also use the **Kullback-Leibler divergence** $$\Kale{\pof{X}}{\qof{X}}$$ and the **cross-entropy** $$\CrossEntropy{\pof{X}}{\qof{X}}$$:

$$
\begin{aligned}
\CrossEntropy{\pof{X}}{\qof{X}} & = \E{\pof{x}}{-\log \qof{x}}\\
\Kale{\pof{X}}{\qof{X}} & = \CrossEntropy{\pof{X}}{\qof{X}} - \Hof{X}
\end{aligned}
$$

The cross-entropy quantifies the average number of bits needed to encode samples drawn from the true distribution $$\pof{X}$$ using a different distribution $$\qof{X}$$. The Kullback-Leibler divergence is a measure of the difference between two probability distributions and captures the additional bits needed to encode samples from $$\pof{X}$$ compared to encoding them using the true distribution $$\qof{X}$$.

<aside class="box-note l-body" markdown="1">
Because the Kullback-Leibler divergence and cross-entropy both are bulky to write, we will use the shorthand $$\iKale{\opp}{\opq}{X}$$ and $$\iCrossEntropy{\opp}{\opq}{X}$$.
</aside>

### Expressing Occam's Razor in Information-Theoretic Terms

Taking this notation into account, we can express Occam's razor as:

$$\text{prefer small } \Hof{z \given x},$$

where $$Z$$ is the latent variable and $$X$$ is the observed data. Note that $$x$$ and $$z$$ are individual realizations of the random variables $$X$$ and $$Z$$, respectively.

The MLE and MAP objectives are accordingly:

$$\text{minimize } \Hof{x \given z} \text{ for MLE and } \Hof{x, z} \text{ for MAP.}$$

This measures the number of bits we need to encode the observed data given the latent variable for MLE and the number of bits to encode both the observed data and the latent variable for MAP. This relates Occam's razor to the minimum description length principle<d-footnote>See the <a href="https://en.wikipedia.org/wiki/Minimum_description_length">Wikipedia article on Minimum Description Length</a> for more details.</d-footnote>.

## Hyperparameter Learning and Model Selection

We often need to determine the best hyperparameters for a model or select a model (architecture) to use from several discrete options. The overall goal is to find the hyperparameters or model that generalizes best to new, unseen data.

We can view both cases as needing to infera random variable $$\H$$, that either represents the model choice as categorical distribution, or the hyperparameters as a continuous distribution. Thus, $$\H$$ is "just" (another) latent variable. 

In the following, we will keep using $$\x$$ for data points. Often, we have a side channel and then we use $$\y$$ for the predictions and $$\x$$ for the side channel, but we will not need this distinction here, so we'll stick to $$\x$$. 

The same arguments as previously also apply here and we can write the objective as:

$$\text{minimize } \Hof{\x \given \H}.$$

### Model Parameters

In addition to the hyperparameters $$\H$$, we usually have model parameters $$\W$$ for a given $$\h$$ with a distribution $$\pof{\w \given \h}$$ that we need to learn. These parameters are the learnable components of the model, such as the weights and biases in a neural network. For given $$\w$$ and $$\h$$, we can easily compute the likelihood $$\pof{\x \given \w, \h}$$, which represents the probability of observing the data $$\x$$ given the specific values of the parameters and hyperparameters. However, to make predictions or compute the marginal likelihood, we need to consider the uncertainty in the parameter values by integrating over all possible $$\w$$.

### Bayesian Model Averaging

This leads to the **Bayesian Model Averaging (BMA)**, which integrates---that is, marginalizes---over the model parameters $$\W$$ when making predictions to account for the uncertainty in the model parameters. It is useful when dealing with complex models and high-dimensional parameter spaces in a low-data regime, as it provides a more robust and comprehensive approach to making predictions. This contrasts with the MLE or MAP as defined above which uses a single parameter value $$\w$$ to make predictions. The probability for a new data point $$\x'$$ using BMA is given by:

$$\pof{\x' \given \x, \h} = \int \pof{\x' \given \x, \w, \h} \pof{\w \given \x, \h} , d\w,$$

where $$\pof{\w \given \x, \h}$$ is the posterior distribution of the parameters given the data, and $$\pof{\x' \given \x, \w, \h}$$ is the probability of the new data point given the parameters, hyperparameters, and training data. 

<aside class="box-error l-body" markdown="1">
For this post, we refer to a **model** for some hyperparameters $$\h$$ as the probability distribution $$\pof{\x, \w \given \h}$$, which includes both the parameters $$\w$$ and the data $$\x$$. 

When we refer to a model's predictions, we refer to the marginal probability:

$$\pof{\x \given \h} = \E{\pof{\w \given \h}}{\pof{\x \given \w, \h}}.$$

Depending on the context, we might talk about the prior distributions or posterior distributions after conditioning on additional data, but we will try to be very clear about that.
</aside>

While BMA offers benefits, it is challenging to compute, particularly when dealing with high-dimensional parameter spaces commonly encountered in deep learning models. To make BMA computationally tractable, various approximation methods, such as Markov Chain Monte Carlo (MCMC) and Variational Inference, are have been suggested.

### Marginal Likelihood and Estimation

Now, let's discuss the marginal likelihood and its relation to BMA. The marginal likelihood, denoted as $$\pof{\x \given \h}$$, is the likelihood of the observed data given the hyperparameters, marginalized over all possible parameter values $$\W$$. It is also known as the **model evidence**. To compute the marginal likelihood, we also need to integrate over all possible $$\w$$:

$$\pof{\x \given \h} = \int \pof{\x \given \w, \h} \pof{\w \given \h} \, d\w,$$

where $$\pof{\x \given \w, \h}$$ is the likelihood of the data given the parameters and hyperparameters, and $$\pof{\w \given \h}$$ is the prior distribution of the parameters given the hyperparameters.

Comparing the BMA to the marginal likelihood, we see that the two match in the case of individual data points. In the case of multiple data points, that is conditioning on datasets, the marginal likelihood is much more complex because we use "BMA" to refer to only making predictions for a single new data point, while the marginal likelihood can be considered for many points at the same time. Otherwise, the two are equivalent. Let's discuss the case of multiple data points in detail next to see why computing the marginal likelihood on datasets is even more challenging.

<aside class="box-note l-body" markdown="1">
Going forward, we will focus on the marginalized quantities and condition on different hyperparameters $$\h$$. Obviously, the separation between hyperparameters $$\H$$ and parameters $$\W$$ is somewhat artificial.
</aside>

## Datasets instead of Individual Data Points

So far, we have described everything as if we only had a single data point $$x$$. However, in practice, we often have a dataset $$\xNtuple = (\x_1, \x_2, \ldots, \x_N)$$.

### Joint Marginal Information and Cross-Entropy

The easiest way to extend the previous definitions is to simply substitute $$\xNset$$ for $$\x$$ and assume we can compute a likelihood for the entire dataset using its joint predictive distribution and maximize that:

$$\pof{\xNtuple \given \h} = \int \pof{\x_1, \x_2, \ldots, \x_N \given \w, \h} \, \pof{\w \given \h} \, d\w.$$

Or equivalently, we can minimize $$\Hof{\xNtuple \given \h}.$$

<aside class="box-note l-body" markdown="1">
For multiple data points, $$\Hof{\xNset \given \h}$$ is often referred to as the negative *marginal log likelihood* or (negative) *log marginal likelihood (LML)*. It is a key quantity in Bayesian model selection<d-footnote>Personally, I prefer log marginal likelihood as that matches the order of the terms, but that might just be the German in me.</d-footnote>.

As the negative log likelihood is just Shannon's information content, we will also use the term **joint marginal information** to be unambiguous.
</aside>

If our model fulfills exchangeability, that is the order of the $$\x_n$$ does not matter, we could equivalently take an expectation over all permutations of the data, and obtain the **joint marginal cross-entropy**:

$$
\CrossEntropy{\pdata{\X_1, ...,\X_n}}{\pof{\X_1, ... \X_n \given \h}},
$$

where $$\pdata{\cdot}$$ is an empirical data distribution that allows us to draw samples *without replacement*---in which case both are indeed equivalent as you can easily check.

With exchangeability, we can simply write $$\iCrossEntropy{\oppdata}{\pof{\cdot \given \h}}{\XNset}$$ instead of using the tuple notation $$\iCrossEntropy{\oppdata}{\pof{\cdot \given \h}}{\XNtuple}$$ as the order of the data points does not matter.

Vice-versa if a model does not fulfill exchangeability, we can induce exchangeability by averaging over all permutations of the data points via ensembling. For example, deep learning models using stochastic gradient descent are generally *not* exchangeable---the order and make-up of the batches does matter---but we can effectively make them exchangeable by training multiple models and averaging their predictions: in the limit of infinite models, the resulting ensemble will be exchangeable<d-footnote>It might not better though: papers on training curricula have shown that batch order can be important!</d-footnote>. 

<!-- TK reference the deepmind paper TK -->

The joint marginal cross-entropy turns a potentially non-exchangeable joint information into an exchangeable one by taking an expectation.

### Marginal Information and Cross-Entropy

Before we go on to understand the joint cross-entropy expression better, we should consider the space of alternatives as this is not the only way to extend the previous definitions. 

For example, we could take the average of the likelihoods for individual data points:

$$ \frac{1}{N} \sum_{n=1}^N \pof{\x_n \given \h}. $$

If we assume an underlying data distribution $$\pdata{x}$$, we can also express this as trying to estimate:

$$ \E{\pdata{\x}}{\pof{\x \given \h}} = \int \pof{\x \given \h} \, \pdata{\x} \, d\x. $$

This gives us a score for data likelihood on average.

From the perspective of Occam's razor, simply taking the average likelihood is not the most principled approach. Instead, we can leverage information theory, which has been our tool of choice so far. Recall that we prefer small values of the **marginal information** $$\Hof{\x \given \h}$$. If we take the expectation over the data distribution, we obtain the *individual* marginal cross-entropy:

$$\CrossEntropy{\pdata{\X}}{\pof{\X \given \h}} = \E{\pdata{\x}}{-\log \pof{\x \given \h}}.$$

This cross-entropy measures the average number of bits needed to encode the data using the model's probability distribution. Since it does not involve a joint distribution, we refer to it simply as the **marginal cross-entropy**.

Obviously, the marginal cross-entropy and the average likelihood are not the same. Using the convexity of the negative logarithm and Jensen's inequality, we can show that the marginal cross-entropy is always larger than the negative logarithm of the average likelihood:

$$
\begin{aligned}
\CrossEntropy{\pdata{\X}}{\pof{\X \given \h}} &= \E{\pdata{\x}}{-\log \pof{\x \given \h}} \\
&\geq -\log \E{\pdata{\x}}{\pof{\x \given \h}} \\
&\approx -\log \frac{1}{N} \sum_{n=1}^N \pof{\x_n \given \h}.
\end{aligned}
$$

<aside class="box-note l-body" markdown="1">
The cross-entropy $$\CrossEntropy{\pdata{\X}}{\pof{\X \given \h}}$$ is a common loss function in machine learning, often referred to as the **negative log-likelihood (NLL)** or **negative log-likelihood loss**. Minimizing the cross-entropy is equivalent to minimizing the Kullback-Leibler divergence between the model's distribution and the true data distribution. It is usually clear from context whether the NLL refers to marginalized likelihoods or likelihoods for individual parameter values $$\w$$, depending on we optimize MLE and MAP estimates or follow a more fully Bayesian approach.
</aside>

The NLL is often used to evaluate a model's performance *after* training, typically on a held-out *validation set*. This is equivalent to computing the cross-entropy between the empirical distribution of the validation set and the model's predictive distribution, conditioned on the parameters learned from the training data:

$$\CrossEntropy{\hpcof{\text{val}}{\X'}}{\pof{\X' \given \xNtuple, \h}}$$

It is crucial to distinguish this from the cross-entropy computed on the prior distribution of the model parameters before seeing any data, which is not as useful for evaluating a trained model's performance:

$$\CrossEntropy{\hpcof{\text{val}}{\X'}}{\pof{\X' \given \xNtuple, \h}}$$

Only the NLL on a validation set *conditioned on the training data* provides an estimate of the model's generalization ability after training. The same holds for the quantities marginalized over the model parameters.

### Marginal Cross-Entropy vs Joint Cross-Entropy

Occam's razor does not clearly specify which aggregate metric on $$\Hof{\x \given \h}$$ we should prefer. Instead of the mean, we could use the median or another quantile of the information content as a summary statistic to assess the model's performance on the dataset. This might be more robust, as it is less sensitive to outliers.

Crucially, the marginal cross-entropy and related summary statistics measure the model's performance using the "prior" parameter distribution, not the posterior conditioned on data. However, the joint distribution captures something else, which can be seen more clearly using the chain rule:

$$\Hof{\xNset \given \h} = \sum_{k=1}^N \Hof{\x_n \given \x_1, \ldots, \x_{k-1}, \h}$$

Each term is a **conditional marginal information** on the previous data points. Similarly, when we take an expectation over the data distribution, we obtain a chain of **conditional marginal cross-entropies**:

$$
\begin{aligned}
& \iCrossEntropy{\oppdata}{\pof{\cdot \given \h}}{\XNtuple} = \\
&\quad = \iCrossEntropy{\oppdata}{\pof{\cdot \given \h}}{\X_1 \given \h} + \iCrossEntropy{\oppdata}{\pof{\cdot \given \h}}{\X_2 \given \X_1}  \\ 
&\quad \quad + \ldots + \iCrossEntropy{\oppdata}{\pof{\cdot \given \h}}{X_N \given \X_1, \X_2, \ldots, \X_{N-1}} \\
&\quad = \sum_{n=1}^N \iCrossEntropy{\oppdata}{\pof{\cdot \given \h}}{\X_n \given \X_{n-1}, \ldots, \X_1}.
\end{aligned}
$$

Each term in the sum is a conditional marginal cross-entropy conditioned on the previous data points, which is different from the marginal cross-entropy (recognized in the first term).

The following visualization summarizes the relationship between the conditional and joint marginal cross-entropies and information. The chain rule tells us that the area under the curve of the conditional quantities equals the joint quantity.

{% comment %} 
include figure.html path="assets/img/2024-05-07-clml/area_under_curve.png" 
class="l-screen-inset img-fluid rounded z-depth-1" 
{% endcomment %}

<figure markdown="1" class="l-page rounded z-depth-1"> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/2024/assets/img/2024-05-07-clml/area_under_curve_1.00-480.webp"> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/2024/assets/img/2024-05-07-clml/area_under_curve_1.00-800.webp"> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/2024/assets/img/2024-05-07-clml/area_under_curve_1.00-1400.webp"> <img class="img-fluid" src="/2024/assets/img/2024-05-07-clml/area_under_curve_1.00.png" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> 
<figcaption class="caption" markdown="1">
*The relationship between conditional and joint marginal cross-entropies and information.*
**Left**: Conditional marginal cross-entropy (blue) for a multi-class classification problem. The area under the curve (orange) represents the joint marginal cross-entropy. As the dataset size increases, the conditional marginal cross-entropy decreases and converges to the best achievable loss for the given model hypothesis $$\h$$.
**Right**: Conditional marginal information (green). The area under the curve (red) represents the joint information. The conditional marginal information is a noisy estimate of the conditional marginal cross-entropy, as it is computed on individual data points.
</figcaption>
</figure>

<aside class="box-note l-body" markdown="1">
The joint cross-entropy measures the model's performance during progressive training, capturing its *online performance*. The conditional marginal cross-entropy measures the model's performance without updating parameters, providing a more static view.
</aside>

## Intermediate Comparison

This brings us back to the earlier question of what we prefer and want to use for model selection. Let's consider:

1. Obviously, the marginal cross-entropy as in the first term is quite likely useless for model selection with deep learning models, as it is not conditioned on any data and thus does not capture the model's performance after training.

2. If we care about the model's "generalization" performance *after training on $$N-1$$ data points* without further adaptation, the marginal cross-entropy on the last data point is the most relevant quantity:

    $$\CrossEntropy{\pdata{\X_N \given \X_{N-1}, ..., \X_1}}{\pof{\X_N \given \X_{N-1}, \ldots, \X_1, \h}}$$
  
    It measures the model's performance on the last data point after having seen all previous data points, similar to a "leave-one-out" metric. Indeed, it is equivalent to [leave-one-out cross-validation](https://www.wikiwand.com/en/Cross-validation_(statistics)#Leave-one-out_cross-validation) when we have an empirical data distribution consisting of $$N$$ data points and we sample without replacement.
    
3. More generally, it is equivalent to cross-validation when we hold out more than one data point for evaluation from the empirical data distribution:

    $$ \CrossEntropy{\pdata{\X' \given \X_{N-k}, ..., \X_{1}}}{\pof{\X' \given \X_{N-k}, ..., \X_{1}, \h}}.$$

    This is the same expression as in **(2.)** but we assume there are more samples to draw from in the empirical data distribution $$\pdata{\x'}$$. We call this term the conditional marginal cross-entropy and keep in mind its connection to cross-validation.

4. On the other hand, if we care about the model's performance as an online learner, or in the case of LLMs, as an in-context learner, the joint marginal cross-entropy becomes a more relevant metric. It measures the model's ability to adapt and make accurate predictions as it sequentially processes new data points, conditioned on the information it has seen so far.

   In the context of online learning, the model receives data points one at a time and updates its predictions based on the cumulative knowledge gained from previous data points. The joint marginal cross-entropy captures how well the model incorporates this sequential information to make accurate predictions for future data points.

   Similarly, for in-context learning of LLMs, the model is provided with a prompt or context consisting of a sequence of data points, and it is expected to generate accurate completions or predictions based on this context. The joint marginal cross-entropy measures the model's ability to effectively utilize the provided context to make accurate predictions for the next data point in the sequence.

5. However, we would not want to use the unconditional joint marginal cross-entropy, but rather condition on some initial data to be closer to the actual use case of the model, which will have been (pre-)trained already. As such, we are interested in estimating a **conditional joint marginal cross-entropy**:

    $$ \CrossEntropy{\pdata{\XNsetk \given \XNkset}}{\pof{\XNsetk \given \XNkset, \h}}. $$

   By conditioning on the previously seen data points, this metric assesses the model's capacity to learn and adapt its predictions based on the evolving context. It provides a more fine-grained evaluation of the model's sequential prediction performance, taking into account the specific order and dependencies within the data.

   Moreover, the conditional joint marginal cross-entropy can be used to compare different models or hyperparameter settings in terms of their online learning or in-context learning capabilities. By evaluating this metric on held-out data sequences, we can determine which model or setting is better suited for tasks that require sequential adaptation and context-dependent predictions.

6. All these quantities are equally valid as far as Occam's razor is concerned---after all, we have already connected Occam's razor to MLE and MAP, which are often not even considered as properly Bayesian. 

7. We have also not discussed how we can efficiently estimate these quantities, especially in the case of deep learning models. More importantly, we have already considered that the joint marginal information (marginal likelihood), BMA, and the joint marginal cross-entropy (as an expectation over the marginal likelihood) are not easy to estimate.

While we will look at the paper in detail in the second half of this blog post, this discussion already brought us to one of the main points of the paper: 

<aside class="box-important l-body" markdown="1">
The marginal likelihood and its approximations are not the only tools we have for model selection and hyperparameter learning and often not even the tools we should use. 
We always need to consider the context in which we want to use a model and data, and then choose the most relevant metric for our use-case. 
</aside>

This is a very important point, and it is also a point that has not been considered sufficiently in the literature on model selection and hyperparameter learning, where the model evidence and marginal likelihood have been presented as the ultimate criterion for model selection and hyperparameter learning. In practice, we rarely update a model on additional data during inference---this is changing with the advent of LLMs and strong in-context learners, but it is still not the norm.

<!-- TK add a schema of all the quantities using "reasonable" terminology and include the conventional terminology in parentheses TK -->

But why has the marginal likelihood been the preferred choice for model selection so far then?

## Different Data Regimes

<!-- (TK need to use consistent heading styles! TK) -->

To sketch a possible answer for this, a fruitful question is: when do the conditional marginal cross-entropy and joint marginal cross-entropy actually lead to different outcomes for model selection and hypothesis testing?
<!-- TK do I talk about hypothesis testing before? TK Obviously, there are different scenarios: one is hypothesis testing using discrete model choices, another is hyperparameter tuning. TK AND WHAT ELSE TK -->

For the discrete case, we can reduce the question to one about ranking: if we have two possible hyperparameter choices $$\h_1$$ and $$\h_2$$, when do we get the same ranking $$\h_1 \succ \h_2$$ for the conditional marginal cross-entropy and the joint marginal cross-entropy? 

### Model Misspecification

First, let's look at the case when we have a lot of data available. Here, model misspecification, which is a common cause of concern, is the most important factor.

As the renowned statistician George Box famously stated:

<blockquote markdown="1">
  All models are wrong, but some are useful.
<footer markdown="1">
  --- <cite>George Box, Science and Statistics (1976)</cite>
</footer>
</blockquote>

When working with real-world data, we must always assume that our models are misspecified to some degree. Models simplify complex systems and cannot capture every nuance of the data-generating process. Consequently, the goal of model selection is not to find the "true" model but rather to identify the most useful model that balances simplicity, interpretability, and predictive performance.

<aside class="box-note l-body" markdown="1">
**Model misspecification** occurs when the assumed model class does not contain the true data-generating process. In other words, no parameter values within the model class can perfectly capture the underlying reality that produced the observed data. 
Even with infinite data, a misspecified model will not converge to the true data-generating distribution, leading to biased parameter estimates, incorrect inferences, and suboptimal predictions. Model misspecification is common in real-world applications, as our models are often simplifications of complex phenomena.
</aside>

Without model misspecification, we would always end up at a maximum likelihood estimate (MLE) that matches the data-generating model in the infinite data limit. The [Bernstein-von Mises' theorem](https://www.wikiwand.com/en/Bernstein%E2%80%93von_Mises_theorem) tells us that posteriors converge to the MLE in the limit. However, in practice, we are always dealing with misspecified models, and the MLE will not converge to the true data-generating model.

<aside class="box-note l-body" markdown="1">
When dealing with misspecified models $\H$, it becomes crucial to identify which model performs best. Since the models are misspecified, their best achievable performance in the infinite data limit will differ. Here, model selection aims to find the model class $\hopt$ that can achieve the highest performance given ample data. By selecting the best misspecified model, we can optimize our predictive capabilities within the limitations of our model classes.
</aside>

<!-- TK find box1976science TK -->

### Infinite Data Limit

But let's go back to our question when the different quantities lead to similar rankings.

While a conditional joint marginal cross-entropy as a sum of conditional marginal cross-entropies is obviously larger than each individual one, if we divide the joint marginal cross-entropy by the number of samples in the conditional joint distribution, we obtain the **rate**<d-footnote>In this context, "rate" refers to the average amount of cross-entropy or information per (training) sample, drawing a parallels to the concept of entropy rate in Shannon's information theory. This usage is distinct from other common uses of "rate" in machine learning, such as learning rate or convergence rate.</d-footnote> of the conditional joint marginal cross-entropies as its average, which can be more easily related:

$$
\begin{aligned}m
& \frac{1}{N-k} \CrossEntropy{\pdata{\XNsetk \given \XNkset}}{\pof{\XNsetk \given \XNkset, \h}} \\
&\quad = \sum_{n=N-k+1}^N \frac{1}{N-k} \CrossEntropy{\pdata{\X_n \given \X_{n-1}, ..., \X_1}}{\pof{\X_n \given \X_{n-1}, ..., \X_1, \h}}.
\end{aligned}
$$

Here, Bernstein-von Mises' theorem concretely tells us that the posterior distribution of the model parameters converges to a normal distribution around the MLE as the number of data points goes to infinity<d-footnote>There are likely fewer caveats to this statement than the naive interpretation of the theorem implies because we are usually not interested in converging towards some unique and identifiable parameters but rather in the predictions matching the data-generating process.</d-footnote>. This means that the later terms in the chain rule decomposition of the joint cross-entropy will converge to the same value in the infinite sample limit as the data we condition one becomes infinite. If we take the limit, we can obviously ignore the first terms in the chain rule decomposition of the joint cross-entropy, and we will get the same average value for the terms of the joint cross-entropy (one per sample in the joint) and the conditional cross-entropy. This also matches a similar result on entropy rates in "Elements of Information Theory" by Cover & Thomas<d-cite key="cover1999elements"></d-cite>. 

<!-- TK shall I mention that result to draw a nice connection to Thomas & Cover? TK -->

Overall, we have (without formal proof):

$$
\begin{aligned}
&\lim_{N \to \infty} \frac{1}{N} \CrossEntropy{\pdata{\XNset}}{\pof{\XNset \given \h}} = \\
&\quad = \lim_{N \to \infty} \frac{1}{N} \sum_{n=1}^N \CrossEntropy{\pdata{\X_n \given \X_{n-1}, ..., \X_1}}{\pof{\X_n \given \X_{n-1}, ..., \X_1, \h}} \\
&\quad = \lim_{N \to \infty} \CrossEntropy{\pdata{\X' \given \XNset}}{\pof{\X' \given \XNset, \h}}.
\end{aligned}
$$

Given sufficient data (in the infinite sample limit), we see that either of these quantities will lead to the same ranking of different hyperparameters/model hypotheses. In reverse, we can thus expect to see meaningful differences only in low-data regimes, where the model is not yet fully adapted to the data. 

Finally, in the infinite data limit, for the conditional marginal cross-entropy, we don't need to take an expectation on the data we condition on (as the model parameters will still have converged):

$$
\begin{aligned}
&\lim_{N \to \infty} \CrossEntropy{\pdata{\XNsetk \given \XNkset}}{\pof{\XNsetk \given \XNkset, \h}} \\
&\quad = \lim_{N \to \infty} \CrossEntropy{\pdata{\XNset}}{\pof{\XNset \given \xNset \h}},
\end{aligned}
$$

for any $$\xNset \sim \pdata{\xNset}$$ as $$n \to \infty$$. But more importantly, this also holds for the joint marginal information, whose rate in the limit is as the same as the rate of the joint marginal cross-entropy above (and thus also joint cross-entropy):

$$
\begin{aligned}
&\lim_{N \to \infty} \frac{1}{N} \Hof{\xNset \given \h} = \\
&\quad = \lim_{N \to \infty} \CrossEntropy{\pdata{\X' \given \XNset}}{\pof{\X' \given \XNset, \h}}.
\end{aligned}
$$

This is noteworthy because we have previously mentioned that the connection between cross-validation and leave-one-out validation and the conditional marginal cross-entropy: this also connects the marginal likelihood in the limit to it.

<!-- TK With all these results, they might seem obvious, but then maybe they are not, given the amount of confusion around the important and meaning of the marginal likelihood. TK -->

Thus:

<aside class="box-important l-body" markdown="1">
In the infinite data limit, the rate of the (conditional) log marginal likelihood (or equivalently the (conditional) joint marginal information), the rate of the (conditional) joint marginal cross-entropy, and the conditional marginal cross-entropy converge to the same value when averaged over the data distribution. This means that given sufficient data, all three metrics---log marginal likelihood, joint marginal cross-entropy, and conditional marginal cross-entropy---will end up producing the same ranking of different model hypotheses or hyperparameter choices.
</aside>

The catch is that "sufficient data" might be a very large amount of data, especially for highly expressive models like neural networks.

Hence, we only expect these quantities to be meaningfully different in the low-data regime. So let's focus on the low-data regime now. In particular, with the caveat above, what is the low-data regime for different $$\h$$?

### Prior-Data Conflict

Even if we were to converge to the same generalization loss in the infinite data limit, different choices of hyperparameters, inducing different priors, might affect the convergence speed and the quality of the model's predictions in the low-data regime. 

<aside class="box-note l-body" markdown="1">
When a prior distribution prevents the model from quickly converging to the MLE because it does not place sufficient probability density (or mass) on the MLE in the infinite data limit, this is called a **prior-data conflict**. When we perform hyperparameter tuning, we are effectively trying to resolve this conflict by finding a prior that is more aligned with the data distribution.
</aside>

Let's concretize this a bit: if we assumed that for different $$\H$$, we will converge to the same generalization performance in the infinite data limit, we would still expect different generalization "speeds" in the low-data regime. This is because the prior distribution of the model parameters will affect the convergence speed and the quality of the model's predictions in this regime.

<figure markdown="1" class="l-page rounded z-depth-1"> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/2024/assets/img/2024-05-07-clml/prior_conflict_and_model_misspecification_0.67-480.webp"> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/2024/assets/img/2024-05-07-clml/prior_conflict_and_model_misspecification_0.67-800.webp"> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/2024/assets/img/2024-05-07-clml/prior_conflict_and_model_misspecification_0.67-1400.webp"> <img class="img-fluid" src="/2024/assets/img/2024-05-07-clml/prior_conflict_and_model_misspecification_0.67.svg" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> 
<figcaption class="caption" markdown="1">
*The relationship between the conditional marginal cross-entropy and dataset size under different modeling scenarios.*
**Left plot --- Model misspecification**: The conditional marginal cross-entropy is plotted for three different model hypotheses ($$\h_1$$, $$\h_2$$, $$\h_3$$) in a multi-class classification problem with 10 classes. Due to the model class not containing the true data-generating process, here each hypothesis converges to a different best achievable loss in the infinite data limit, representing the minimum achievable misspecification error.
**Right plot --- Prior-data conflict**: The conditional marginal cross-entropy is plotted for three different model priors ($$\h_1$$, $$\h_2$$, $$\h_3$$) in the same setup. All priors converge to the same achievable loss in the infinite data limit, but their convergence speeds differ due to varying degrees of alignment of the prior distribution with the data distribution. A prior that places more probability mass near the maximum likelihood estimate (MLE) will lead to faster convergence.
*Real-world models often face a combination of prior-data conflict and model misspecification, of course.*
</figcaption>
</figure>

<!-- TK change the blue line in model misspecification to also have a higher loss! increase the dataset size so that there is a longer flat region and add that to the caption and can we make the infty symbol bold or a different color? TK -->

In the low-data regime, when all models are assumed to converge to the same validation loss in the infinite data limit, we will likely prefer the model that converges the "fastest", that is, with the least amount of training data. This preference arises because a model with a prior that aligns well with the data distribution will be able to learn efficiently and generalize better with limited data. In this scenario, the area under the conditional marginal cross-entropy or information curve (or validation loss curve)---which corresponds to the joint marginal cross-entropy, the negative log marginal likelihood, or the joint marginal information---indicates which model we will prefer, all other things being equal. The model with the lowest joint marginal information (or highest log marginal likelihood) will be the one that fits the available data best while also having a prior that enables efficient learning and generalization.

### Anti-Correlated Model Misspecification and Prior-Data Conflict

Finally, what happens when there are both model misspecification and a prior-data conflict in the low-data regime? If both are correlated, ranking will be preserved, but if they are anti-correlated, the ranking might change.

Let's visualize this: the curves will intersect at some point, and the model with the best achievable loss in the infinite data limit might not be the best choice in the low-data regime depending on how much data we can train on and depending on the amount of data available this might also change.

<figure markdown="1" class="l-body rounded z-depth-1"> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/2024/assets/img/2024-05-07-clml/anticorrelated_prior_conflict_and_model_misspecification_1.30-480.webp"> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/2024/assets/img/2024-05-07-clml/anticorrelated_prior_conflict_and_model_misspecification_1.30-800.webp"> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/2024/assets/img/2024-05-07-clml/anticorrelated_prior_conflict_and_model_misspecification_1.30-1400.webp"> <img class="img-fluid" src="/2024/assets/img/2024-05-07-clml/anticorrelated_prior_conflict_and_model_misspecification_1.30.svg" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> 
<figcaption class="caption" markdown="1">
*The conditional marginal cross-entropy is plotted for three different model hypotheses ($$\phi_0$$, $$\phi_1$$, $$\phi_2$$) as a function of dataset size. The models exhibit both prior-data conflict and model misspecification.* 
In the small data regime, $$\phi_2$$ has the lowest loss due to its prior aligning well with the data distribution, allowing for faster initial learning. However, as more data becomes available, the models' asymptotic performance quickly plateaus.
$$\phi_1$$ converges to the lowest achievable loss in the infinite data limit, indicating it suffers the least from model misspecification. In contrast, $$\phi_1$$ and $$\phi_0$$ converge to higher loss values due to greater misspecification.
Notably, the models' performance ranking changes as the dataset grows, with $$\phi_2$$ being initially favored but ultimately having the worst infinite-data loss. Each model ranks best for the conditional joint marginal cross-entropy for some chosen range.
This illustrates how the interplay between prior-data conflict and model misspecification can lead to different model selection decisions depending on the amount of available data and the metric used to measure performance.*
</figcaption>
</figure>

Here, the joint marginal cross-entropy and the joint marginal information (log marginal likelihood) might not lead to the sane decision because the area under the curve at the start might be larger than what the best model is able to save later, and thus it might change the ranking of the models compared to the conditional marginal cross-entropy (cross-validation loss) at the end of training as a proxy for model's generalization performance.

Instead, the conditional joint marginal cross-entropy and information can shine here by conditioning "away" the beginning of the curve and thus giving us a better estimate of the conditional marginal cross-entropy (or expected information) at the point of interest.

To formalize this, we can just use the chain rule to split the joint marginal cross-entropy in two:

$$
\begin{aligned}
&\underbrace{\CrossEntropy{\pdata{\XNset}}{\pof{\XNset \given \h}}}_{\text{Joint Marginal Cross-Entropy}} = \\
&\quad = \CrossEntropy{\pdata{\XNsetk}}{\pof{\XNsetk \given \h}} \\
&\quad \quad + \underbrace{\CrossEntropy{\pdata{\XNset \given \XNsetk}}{\pdata{\XNset \given \XNsetk}}}_{\text{Conditional Joint Marginal Cross-Entropy}},
\end{aligned}
$$
<!-- \pof{\XNset \given \XNsetk, \h}}}_{\text{CJMCE} \overset{N \to \infty}{\to} \CrossEntropy{\pdata{\X' \given \XNset}}{\pof{\X' \given \XNset, \h}} -->

and note that the per-sample averages of both terms converge to the same value in the infinite data limit---the conditional marginal cross-entropy (cross-validation loss), as we have discussed previously---but obviously the second term will converge faster because it does not include the constant $$\CrossEntropy{\pdata{\XNsetk}}{\pof{\XNsetk \given \h}}$$. 

Somewhat trivially, we can also see both terms as approximating the conditional marginal cross-entropy (cross-validation loss) for a fixed $$N$$ in the low-data regime and the per-sample average of the second term will be a better approximation.

<!-- TK maybe use per-sample average instead of rate TK -->

Thus, all in all, the consistency of the ranking will depend on the size of $$\CrossEntropy{\pdata{\XNsetk}}{\pof{\XNsetk \given \h}}$$ for different $$\h$$ and how it compares to the conditional joint marginal cross-entropy $$\CrossEntropy{\pdata{\XNset \given \XNsetk}}{\pdata{\XNset \given \XNsetk}}$$.

## Approximating the (Cross-)Validation Loss

The question on your mind is then: why bother with the marginal likelihood at all? And likewise, why bother with the conditional joint marginal cross-entropy and information? We could just always use cross-validation (i.e. the conditional marginal cross-entropy) or simply a validation loss and be done with it.

That is true, but the question is: can we approximate the validation loss sooner than after training a model fully? Or can we do this in a way that is more efficient than performing inference on each element of a validation set?

We could try to extraploate the training loss to predict the validation loss. This might be underexplored in this context, but scaling laws have been found to work well for predicting model performance.

Alternatively, when we train a model on a dataset for only a single epoch, which is still surprisingly common for large language models TK citation? TK---and we do not perform any sort of active data sampling---the average training loss for each batch is a good approximation of the validation loss. With a cross-entropy loss, this is equivalent to an estimate of the conditional marginal cross-entropy.

Obviously, the result will be biased as each batch is trained on a different subset of the data, so we might want to use only the last few batches or an exponential average. This is equivalent to per-sample average of the conditional joint marginal cross-entropy, which is the average of the conditional marginal cross-entropies for the last data points. Compared to using the training loss of the last batch, this has a smoothing effect and is less sensitive to outliers.

In the multiple epoch case, we revisit the same data points multiple times, and thus cannot use the training loss as an estimate of the validation loss. Here, cross-validation can come to the rescue, and we can train on the held-out data in the last epoch while also using it to compute a validation loss while also obtaining an ensemble of fully trained models without wasting any of the data.

## The Big Comparison 

Thus far, we have explored various metrics for model selection and hyperparameter learning in the Bayesian context, focusing on the marginal likelihood, joint marginal cross-entropy, and conditional marginal cross-entropy. Our discussion has led to several important insights:

1. **Infinite data limit**: In the infinite data limit, the rate of the log marginal likelihood (or equivalently, the joint marginal information), the joint marginal cross-entropy, and the conditional marginal cross-entropy converge to the same value when averaged over the data distribution. This means that given sufficient data, all three metrics—log marginal likelihood, joint marginal cross-entropy, and conditional marginal cross-entropy—will end up producing the same ranking of different model hypotheses or hyperparameter choices.

2. **Connection to cross-validation**: The conditional marginal cross-entropy is equivalent to the expected cross-validation loss. Cross-validation is the gold standard for model selection in machine learning practice, where we estimate a model's generalization performance by evaluating it on held-out validation data after training on the rest.

3. **Sufficient data requirement**: The catch is that "sufficient data" might be a very large amount of data, especially for highly expressive models like neural networks. Therefore, the convergence of these metrics in the infinite data limit may not be practically relevant in many real-world scenarios.

4. **Low-data regimes**: In low-data regimes, the metrics can differ meaningfully. The conditional marginal cross-entropy (or cross-validation loss) is often the more reliable choice for model selection targeting generalization performance, as it directly measures the model's ability to predict unseen data after being trained on the available data.

5. **Sequential prediction and compression**: On the other hand, the joint marginal cross-entropy, which corresponds to the negative log marginal likelihood, may be preferable if we care about a model's overall sequential prediction performance or compression ability on the training data itself. It measures how well the model fits the entire training dataset jointly, without splitting into train and validation sets.

   Furthermore, the conditional joint marginal information and cross-entropy are particularly relevant for measuring the performance of online learners and in-context learning of large language models (LLMs). These metrics capture the model's ability to adapt and make accurate predictions based on the sequential information and evolving context after training on available data.
   
6. **Model misspecification and prior-data conflict**: In real-world scenarios, models often face a combination of model misspecification (where the true data-generating process is not contained within the model class) and prior-data conflict (where the prior distribution does not align well with the data distribution). The interplay between these factors can lead to different rankings of models depending on the amount of available data and the specific metric used for evaluation.

Wile the marginal likelihood has been a popular tool for model selection and hyperparameter learning in the Bayesian community, it is not the only option, and its suitability depends on the specific context and goals. The conditional marginal cross-entropy, which is closely related to cross-validation, is often a more reliable choice when the primary objective is to optimize generalization performance. However, the conditional joint marginal cross-entropy (or conditional log marginal likelihood) may be preferable when the focus is on sequential prediction after training or the measurement of in-context learning abilities.

As machine learning practitioners and researchers, it is crucial to carefully consider the context, available data, and desired outcomes when selecting the most appropriate metric for model selection and hyperparameter tuning.

<!-- TK What was the novelty of the previous exposition? TK -->

Now after having thought about the all this in detail and mostly from first principle, let's discuss the literature and how it supports or augments these considerations.

## Literature Review

Having discussed everything, we will look at several papers that have influenced this discussion on model selection and hyperparameter tuning in the Bayesian context or have provided valuable insights into the marginal likelihood and its connections to other metrics in more detail.

### Fong and Holmes (2020): "On the marginal likelihood and cross-validation"

Fong and Holmes (2020)<d-cite key="fong2020marginal"></d-cite> explore the connection between the log marginal likelihood (joint marginal information) and cumulative leave-p-out cross-validation. They show that under exchangeability, the joint marginal information can be rewritten as a cumulative sum of leave-p-out cross-validation terms.

To start, the authors define the *leave-p-out cross-validation score* as:

$$S_{CV}(\xNset;p) = \frac{1}{\binom{N}{p}}  \sum_{V \in \binom{[N]}{p}} \frac{1}{p} \sum_{i=1}^p \Hof{\x^{V}_i \given \{\x^{\bar{V}_k}\}_{k=1}^{N-p}}$$

where $$\binom{[N]}{p}$$ denotes the set of all $$p$$-length subsets of $$\{1,...,N\}$$---the indices of the validation set---$$\x^V_i$$ denotes the $$i$$-th validation data point, and $$\x^{\bar{V}}_k$$ denotes the $$k$$-th training data point. The leave-p-out cross-validation score measures the model's performance using $$p$$ validation data points given the remaining data points for training. This is equivalent to the respective conditional marginal cross-entropy.

The *cumulative leave-P-out cross-validation score* is defined as:

$$S_{CCV}(\xNset; P) = \sum_{p=1}^P S_{CV}(\xNset; p)$$

This score focuses on the last $$P$$ stages of the learning curve equally and is the same as the conditional joint marginal cross-entropy. For $$P=N$$, the cumulative leave-N-out cross-validation score and the joint marginal information are the same:

$$S_{CCV}(\xNset; N) = \Hof{\xNset}$$

Comparing to $$P<N$$, Fong and Holmes highlight the potential sensitivity of the marginal likelihood ($$P=N$$) to the choice of prior. Hence, they argue for using cumulative cross-validation following a preparatory training phase with $$P<N$$ (e.g., $$10\%$$ or $$50\%$$). They demonstrate the benefits of using $$S_{CCV}$$ over the full marginal likelihood for model selection, especially in situations with vague priors or model misspecification.

The paper also discusses the coherence of the log posterior predictive probability as a scoring rule in cross-validation and explores connections to prequential analysis and intrinsic Bayes factors.

Fong and Holmes provide strong support for the ideas explored in this blog post, particularly the connections between the marginal likelihood, cross-validation, and the importance of focusing on the later stages of the learning curve for model selection. Their work establishes the equivalence between the cumulative leave-p-out cross-validation score and the conditional joint marginal information, aligning with our discussion of the conditional joint marginal cross-entropy as a more reliable metric for model selection compared to the full marginal likelihood.

### Lyle et al. (2020) and Ru et al. (2021): Training speed and model selection

In "A Bayesian Perspective on Training Speed and Model Selection" establish, Lyle et al. (2020)<d-cite key="lyle2020bayesian"></d-cite> establish a connection between training speed and the marginal likelihood in linear models. They propose using the sum of mini-batch training losses as a proxy for the log marginal likelihood to predict the generalization behavior of deep neural networks. This sum, referred to in later works as the *training speed estimator* (TSE), corresponds to the area under the learning curve. Here for 1-sample batches, the TSE is defined as:

$$\text{TSE}(\xNset) = \sum_{n=1}^N \Hof{\x_n \given \w_n},$$

where $$\Hof{\x_n \given \w_n}$$ is the cross-entropy loss at training step $$n$$ with model parameters $$\w_n$$.

The authors provide an iterative algorithm for linear models to estimate a lower bound on the LML over multiple epochs of training. This allows capturing the model's performance as it sees more data points over the course of training, rather than being limited to a single epoch. They also discuss extending their estimator to the infinite-width limit of neural networks.

Building upon Lyle et al. (2020), Ru et al. (2021)<d-cite key="ru2021speedy"></d-cite> focus on using TSE for model selection in neural architecture search in "Speedy Performance Estimation for Neural Architecture Search". They propose two variants of TSE: *TSE-E*, which focuses on the last few epochs, and *TSE-EMA*, which uses an exponential moving average to assign higher weights to later epochs:

$$
\begin{aligned}
\text{TSE-E}(\xNset) &= \sum_{n=N-E+1}^N \Hof{\x_n \given \w_n}, \\
\text{TSE-EMA}(\xNset) &= \sum_{n=1}^N \alpha^{N-n} \Hof{\x_n \given \w_n},
\end{aligned}
$$

where $$\alpha \in (0, 1)$$ is a hyperparameter controlling the decay rate.

The authors hypothesize that assigning higher weights to later epochs may lead to better correlation with the true generalization performance of the final trained network, as the early epochs may be unstable and less informative.

They demonstrate empirically that TSE-E and TSE-EMA can reliably estimate the generalization performance of neural architectures with a small training budget and remain effective for a large range of training epochs. TSE outperforms other efficient estimators, such as early stopping and learning curve extrapolation, in terms of rank correlation with the true test performance.

The TSE estimator proposed by Ru et al. (2021) aligns closely with the ideas discussed in this blog post, as they prioritize the model's performance in the later stages of learning. The empirical results presented by Ru et al. (2021) and Lyle et al. (2020) provide supporting evidence for the importance of going beyond the marginal likelihood.

### Lotfi et al. (2022/2023): "Bayesian Model Selection, the Marginal Likelihood, and Generalization"

Lotfi et al. (2022/2023)<d-cite key="lotfi2022bayesian"></d-cite> provide a comprehensive re-evaluation of the marginal likelihood as a metric for predicting the generalization performance of trained models and learning hyperparameters. They argue that while the marginal likelihood is well-suited for prior hypothesis testing, it is only peripherally related to generalization after training. The authors identify several practical and philosophical issues in using the marginal likelihood for selecting between trained models, such as its sensitivity to the choice of prior, potential to lead to both underfitting and overfitting, and negative correlation with generalization performance in some cases.

To address these limitations, Lotfi et al. propose the conditional marginal likelihood (CLML) as a partial remedy. The CLML is computed by conditioning on a subset of the training data, which helps to mitigate the influence of the prior and focus on the model's performance under this posterior, and is less sensitive to the number of parameters in the model. 
The authors demonstrate that the CLML is better correlated with generalization than the marginal likelihood and provides promising performance for deep kernel hyperparameter learning and neural architecture search. 

The CLML shares significant similarities with the cumulative leave-p-out cross-validation score proposed by Fong and Holmes (2020)<d-cite key="fong2020marginal"></d-cite>. Both approaches essentially propose the same metric, which focuses on the model's performance in the later stages of learning and provides a more reliable indication of generalization compared to the full marginal likelihood.
Lotfi et al. also critically compare to the work of Lyle et al. (2020)<d-cite key="lyle2020bayesian"></d-cite> (but not Ru et al. (2021)<d-cite key="ru2021speedy"></d-cite>).

Lotfi et al. conduct an extensive empirical evaluation of the CLML across various settings, comparing it to the marginal likelihood and other baselines under different conditions, such as varying dataset sizes, model complexities, and hyperparameter settings. They demonstrate that the CLML consistently outperforms the marginal likelihood in terms of selecting the hyperparameters that lead to better generalization performance. The authors also acknowledge some limitations of their work, such as the need for further theoretical analysis of the CLML's properties and the potential challenges in estimating the CLML for more complex models.

The key novelty of Lotfi et al.'s work lies in their comprehensive analysis of the limitations of the marginal likelihood for model selection and hyperparameter learning, as well as their proposal of the CLML as a practical alternative that addresses these limitations.

## A Simple Toy Experiment

To illustrate the concepts discussed in this post, we conduct a simple toy experiment using a Bayesian linear regression model. The goal is to demonstrate how the various information metrics behave under different prior settings and dataset sizes and that there is none of the metrics is reliable, but the joint marginal information might definitely not be the one to use when you care about static performance after training on data.

### Experimental Setup

We generate a synthetic dataset with 64 features and 500 training and validation samples each. The true coefficients were drawn from a normal distribution with a mean of 2, and the target is the dot product between the features and the true coefficients.

For the model we learn, we use a Bayesian linear regression model with an isotropic Gaussian prior on the weights (hyperparameter $$\wstddev$$) and independent Gaussian noise (hyperparameter $$\noisestddev$$). The model is thus misspecified when $$\noisestddev > 0$$. We consider three different prior settings:

- Model 1 ($$\h_1$$): $$\wstddev=0.1$$, $$\noisestddev=0.8$$,
- Model 2 ($$\h_2$$): $$\wstddev=100$$, $$\noisestddev=1.0$$, and
- Model 3 ($$\h_3$$): $$\wstddev=1$$, $$\noisestddev=1.2$$.

Thus, all three models are misspecified and different degrees of prior-data conflict.

We train the model on subsets of the training data of varying sizes, ranging from 1 to the full training set size. We perform 5 trials each, where we shuffle the training data. For each subset size, we computed several metrics:

- Joint Marginal Information (JMI);
- Conditional Joint Marginal Information (CJMI) with half the data used for conditioning;
- Marginal Cross-Entropy (MCE) on the training set;
- Marginal Cross-Entropy (MCE) on the validation set;
- Training Speed (Approximate); and
- Joint Marginal Information Rate (JMI Rate).

The JMI is equivalent to the negative log marginal likelihood, the CJMI to the negative conditional likelihood, and the MCE corresponds to the cross-entropy loss. The Training Speed approximates an iterative algorithm by following the full data gradient, and the JMI rate is the JMI divided by the dataset size, which converges to the MCE in the infinite limit of large dataset sizes.

### Results

The results of the experiment are summarized in the following plots:

{% comment %} 
include figure.html path="assets/img/2024-05-07-clml/binary_regression_information_metrics.png" 
class="l-screen-inset img-fluid rounded z-depth-1" 
{% endcomment %}

<figure markdown="1" class="l-page rounded z-depth-1"> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/2024/assets/img/2024-05-07-clml/binary_regression_information_metrics-480.webp"> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/2024/assets/img/2024-05-07-clml/binary_regression_information_metrics-800.webp"> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/2024/assets/img/2024-05-07-clml/binary_regression_information_metrics-1400.webp"> <img class="img-fluid" src="/2024/assets/img/2024-05-07-clml/binary_regression_information_metrics.png" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> 
<figcaption class="caption" markdown="1">
*Information metrics for the three Bayesian linear regression models as a function of dataset size.* The joint marginal information does not indicate the best performing model. The conditional joint marginal information (conditioned on half the dataset size, predicting on the other half) only finds the best model after 4/5 of the data are observed. *Metrics are reported in bits (log base 2), five trials each.*
</figcaption>
</figure>

The plots show the behavior of the information metrics as the dataset size increases for the three different prior settings. Some key observations:

- The MCE metrics decrease as the dataset size increases, indicating improved model performance.
- The JMI increases with more data, as it is equivalent to the area under the curve of the MCE on the training set. (As we take the average over multiple trials, its mean is actually an estimate of the joint marginal cross-entropy.)
- The JMI rate, which is the JMI divided by the dataset size, decreases and very slowly towards the same value as the MCE. This agrees with the previous discussion on the infinite data limit.
- The training losses also decrease, while their sum, equal the TSE, increases with the dataset size.
- The CJMI with half the data used for conditioning shows a similar trend to the JMI but with lower values, as it focuses on the model's performance on the held-back data. As we take an average over multiple trials, it is actually an estimate of the conditional joint marginal cross-entropy.

To further analyze the model selection behavior, we computed the CJMI for different conditioning set sizes and selected the model with the lowest CJMI for each combination of dataset size and conditioning set size. The results are visualized in the following plot:

{% comment %} 
include figure.html path="assets/img/2024-05-07-clml/binary_regression_conditional_joint_marginal_information_decision_boundary.png" 
class="l-screen-inset img-fluid rounded z-depth-1" 
{% endcomment %}

<figure markdown="1" class="l-body rounded z-depth-1"> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/2024/assets/img/2024-05-07-clml/binary_regression_conditional_joint_marginal_information_decision_boundary-480.webp"> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/2024/assets/img/2024-05-07-clml/binary_regression_conditional_joint_marginal_information_decision_boundary-800.webp"> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/2024/assets/img/2024-05-07-clml/binary_regression_conditional_joint_marginal_information_decision_boundary-1400.webp"> <img class="img-fluid" src="/2024/assets/img/2024-05-07-clml/binary_regression_conditional_joint_marginal_information_decision_boundary.png" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> 
<figcaption class="caption" markdown="1">
*Decision boundary for the best model amongst three ($$\phi_1$$, $$\phi_2$$, $$\phi_3$$) with the lowest conditional joint marginal cross-entropy/information, as a function of dataset size and held-back size.* The three models $$\phi_1$$, $$\phi_2$$, and $$\phi_3$$ correspond to different prior variances and noise levels. The white diagonal line shows where the conditional joint marginal information is computed using half the dataset size. In the region below this line, $$\phi_1$$ (blue) has the lowest conditional joint marginal information, while $$\phi_2$$ (orange) and $$\phi_3$$ (green) are preferred for different dataset and held-back sizes.
</figcaption>
</figure>

The plot shows which model is selected based on the lowest CJMI for different dataset sizes (x-axis) and conditioning set sizes (y-axis). The white line represents the case where half the data is used for conditioning (CJMI half in the previous plot). We observe that the model selection decision changes depending on the amount of available data and the size of the conditioning set/held-back data.

<!-- ## Take-Aways

TK Merge this in earlier:

Recent works by Ian Osband et al., starting with [The Neural Testbed: Evaluating Joint Predictions](https://arxiv.org/abs/2110.04629)<d-cite key="osband2022neural"></d-cite> can help build intuitions for joint predictions.
Similarly, a *gentler* introduction, comparing marginal and joint predictions, can also be found in the arXiv note [Marginal and Joint Cross-Entropies & Predictives for Online Bayesian Inference, Active Learning, and Active Sampling](https://arxiv.org/abs/2205.08766)<d-cite key="kirsch2022marginal"></d-cite>. 

TK -->

## A Narrow but Deep Dive into "Bayesian Model Selection, the Marginal Likelihood, and Generalization"

Now that we have both introduced the necessary concepts and discussed the literature, let's take a closer look at the paper by Lotfi et al. (2022/2023)<d-cite key="lotfi2022bayesian"></d-cite> and examine it critically.

### Use Cases and Pitfalls of the LML

After a very readable introduction to the log marginal likelihood (LML), Lotfi et al. (2022/2023) present both the case for the LML as well as potential pitfalls when using it. Specifically, they highlight the following use cases for the LML---*quoted and paraphrased from the paper*:

1. **Hypothesis testing:** The LML provides an elegant mechanism to select between fixed prior hypotheses, even if each hypothesis is entirely consistent with observations. It automatically favors the most constrained hypothesis that fits the data, encoding a notion of Occam's razor. The paper gives the example of the LML favoring general relativity over alternative explanations for Mercury's orbit.

2. **Hyperparameter learning:** The LML is often successfully used in practice to learn hyperparameters of the prior, finding the hyperparameters $$\h$$ that maximize $$\pof{\mathcal{D} \given \h}$$, where $$\mathcal{D}$$ is a dataset. The paper highlights Gaussian processes as a compelling example, where the LML chooses kernel hyperparameters that make the distribution over functions likely to generate the training data, rather than simply maximizing data fit. The LML can learn many kernel parameters and be used where cross-validation would be intractable.

3. **Constraint learning:** Unlike typical learning objectives like maximum likelihood, the LML is incentivized to select for constraints. It provides a consistent estimator for constraints, automatically selecting the most constrained solution that fits the data and collapsing to the true constraint value as the number of observations grows. Examples include the LML consistently estimating the true dimensionality in Bayesian PCA and automatically learning symmetries like rotation invariance.

At the same time, the paper argues that the LML has several pitfalls for model selection and generalization---*again quoted and paraphrased from the paper*:

{:start="4"}
4. **It is not aligned with generalization:** The LML answers "what is the probability a prior model generated the training data?" rather than "how likely is the posterior to have generated withheld points?". A prior that initially explains the data well can still lead to a posterior that generalizes poorly.

5. **It is misaligned in model selection:** The LML evaluates priors (fixed hypothesis classes), while model selection should evaluate posteriors. Maximizing LML is not generally equivalent to selecting the model with the best generalizing posterior.

6. **It can overfit:** If the model search space includes priors concentrated around overfit maximum likelihood solutions, the LML can favor these "simple" models that achieve high LML but provide poor generalization. An example is overfitting the LML in Gaussian process regression by parameterizing the mean with a neural network.

7. **It has an underfitting bias in hyperparameter selection:** Supporting a good solution could involve supporting many solutions unlikely to generate the data. The LML won't favor hyperparameters that make the best generalizing parameters likely if they also make many poorly fitting parameters likely. 

Let's relate some of the points to the previous discussions.

For hypothesis testing and hyperparameter learning (**1.** & **2.**), assuming that both hypotheses are similarly consistent with the data, the LML will favor the simpler hypothesis---and simpler means that it converges faster, which implies a smaller area under the learning curve. This is in line with our discussion on the prior-data conflict for otherwise similarly misspecified models. 

At the same time, the paper also states about the case of Mercury's orbit that:

> We emphasize here we are comparing fixed *prior* hypotheses. We are not interested in how parameters of general relativity update based on orbital data, and then deciding whether the updated general relativity is the correct description of orbital trajectories.

This could be misconstrued at computing the marginal cross-entropy for the data under the prior, which is not what the LML is doing: it computes a joint marginal cross-entropy after all.

The two questions in (**4.**) point to the joint marginal cross-entropy and the conditional marginal cross-entropy, respectively. The former is the area under the learning curve, and the latter is the area under the learning curve after a certain point.

However, disagreeing with (**5.**), neither LML and CLML are aligned with *static* evaluation of models, but with continued learning.

Both (**6.**) and (**7.**) are related to the prior-data conflict and model misspecification, respectively, and the case when they are anti-correlated.

Overall, we have already seen that all the quantities can fail in the low-data regime, and they are less interesting in the infinite data regime because there the level of model (mis-)specification dominates all other factors.

<!-- TK should I include the figures here again? TK -->

### The "Conditional Marginal Likelihood" in Lotfi et al. (2022/2023)

The paper goes on to introduce the conditional marginal likelihood (CLML) as a remedy for the pitfalls of the LML, which matches the earlier definition as the conditional joint marginal information. As a reminder, it is:

$$
\Hof{\xset{}{N-P+1}{N} \given \xset{}{1}{N-P}, \h}.
$$

Even assuming exchangeability, unlike the LML as joint information of all data, the CLML is dependent on the order the data as it is split into data to condition/train on and validation data to evaluate the joint information on. This order is usually arbitrary, and thus a permutation-invariant version of the CLML is proposed by averaging over different permutations, which is equivalent to the joint marginal cross-entropy. Even sampling multiple permutations quickly becomes expensive, and the paper uses a single permutation instead as noted in its appendix. To use a posterior that is sufficiently converged, the paper uses $$P=20\% \, N$$ for the CLML, so equivalent to holding back $$20%$$ of the data as validation set.

<aside class="box-note l-body" markdown="1">
⚠️ The CLML matches the conditional joint marginal cross-entropy in its permutation-invarianet form, and thus exactly matches $$S_{CCV}(\xNset; P)$$ from Fong and Holmes (2020)<d-cite key="fong2020marginal"></d-cite>.
</aside>

### Estimating the CLML and LML via the Laplace Approximation

Computing the LML via sampling is generally intractable for deep neural networks. It is only straightforward for very simple models like Bayesian linear regression, for which we can write down closed-form analytical solutions. Estimating the LML for deep neural networks in a way that is useful for model selection is challenging and an active area of research.

In particular, estimating it from a prior usually highly uninformative distribution leads to high-variance estimates when performing Monte Carlo sampling. Let's spend a moment on this. The LML as marginalization over the prior distribution is written as:

$$
\Hof{\xNset \given \h} = -\log \int \pof{\xNset \given \w} \, \pof{\w \given \h} \, \mathrm{d}\w.
$$

This integral is incredible badly behaved for uninformative priors because most $$\w$$ will perform very badly on the data, leading to a high-variance estimate. Even though, Monte Carlo sampling can often be used for high-dimensional estimates, it will fail here because it is incredibly unlikely to sample a good $$\w$$ from the prior by chance. To see how incredibly unlikely this is, the following tweets by [@RobertRosenba14](https://twitter.com/RobertRosenba14) and [@stanislavfort](https://twitter.com/stanislavfort) provide great illustrations:

{% twitter https://twitter.com/stanislavfort/status/1529865444701577216 %}
{% twitter https://twitter.com/RobertRosenba14/status/1517465854157500419 %}

While sampling from the prior to estimate the LML is intractable, we can fare better when sampling from a posterior for computing a CLML, which is the approach taken by the paper for the CLML. The posterior is more concentrated around "good" $$\w$$, and the paper uses a Laplace approximation to approximate it:

<aside class="box-note l-body" markdown="1">
💡 A **Laplace approximation (LA)** estimates the posterior distribution by fitting a Gaussian on the second-order Taylor expansion of the MLE or MAP estimate <d-cite key="murphy2012machine"></d-cite>.
 
The LA can be motivated by Bernstein-von Mises' thereom as well, as it tells us that our posterior will concentrate around the maximum likelihood estimate in the infinite data limit (with the previously mentioned caveats).
</aside>

As the paper notes, the LA has drawbacks: it only captures uncertainty around a single mode, leading to underestimation of the model uncertainty when we use the approximation before a model has sufficiently converged. The paper provides a beautiful example of this issue:

{% capture max-width %}
" style="max-width: 35em;
{% endcapture %}
{% include figure.html path="assets/img/2024-05-07-clml/bmsmlg_fig3.png" max-width=max-width %}

This is of particular importance for overparameterized models like DNNs, which have multiple diverse modes, as we know from deep ensembles ([Wilson, Izmailov (2021, blog post)](https://cims.nyu.edu/~andrewgw/deepensembles/);  [Wilson, Izmailov (2020)](https://arxiv.org/abs/2002.08791)<d-cite key="wilson2020bayesian"></d-cite>). The LA, however, only centers around a single one. 

At the same time, a similar argument as the previous one can be made against a Laplace approximation when computing the CLML when the data it evaluates on has a meaningful effect on the model: the CLML as conditional joint marginal information is computing sequential marginal information terms as we condition on more and more data when we use the chain rule to decompose it. Using the same argument, it is unlikely that we will randomly sample meaningful $$\w$$ from the intermediate posteriori that will perform better on much of the additional data we condition on.

### DNN Experiments: Validation Loss vs. CLML

So far we have already drawn parallels between the CLML and cross-validation, and the validation loss as a single-sample estimate of the latter. The DNN experiments in Lotfi et al. (2022/2023) compare the CLML to the validation loss for DNNs on CIFAR-10 and CIFAR-100 datasets and inadvertently provide empirical evidence for the challenges of computing the CLML and beg the question whether these approximations might not be meaningfully different from a validation loss. 

The paper shows that the CLML is better correlated with the generalization performance of the model than the LML, but the validation loss is still also better correlated with the generalization performance than the CLML. This might not have been the intention of the paper throughout its different revisions. The initially published DNN experiments in the paper did not compute the CLML but actually the validation loss, which was then fixed in the second arXiv revision of the paper<d-footnote>This bug was found by yours truly, see the appendix of this post.</d-footnote>. Taking into account the previous discussions, this was not a big issue for the paper. Indeed, the expectation was that the CLML is very similar to cross-validation. Importantly, when comparing the CLML using MC sampling with the validation loss computed using MC sampling for the BMA, the validation loss is better correlated with the generalization performance than the CLML as we examine in the appendix of this post.

## Conclusion

In conclusion, this blog post has challenged the conventional focus on the marginal likelihood and related quantites for Bayesian model selection as direct consequence of Occam's razor and highlighted the importance of considering context and goals when choosing a model selection criterion. By also motivating MLE and MAP using Occam's razor and questioning the uniqueness of the (conditional) joint marginal likelihood, we hope to encourages critical thinking about the foundations of these quantities.

However, it is important to acknowledge the limitations of our arguments and experiments. A more rigorous theoretical justification, a broader range of models and datasets, and a deeper engagement with philosophical implications are needed to strengthen the insights. As most of the presented methods ignore model complexity and assume a uniform model prior $$\pof{\h}$$, we have done the same and not discussed it in the detail necessary, even though from the perspective of model description lengths (MDL), it would be crucial to take into account.

Despite these limitations, our exploration of the connections between information-theoretic concepts and their behavior in different data regimes along the lines of model misspecification and prior-data conflict provides a necessary starting point for understanding recent metrics that have been proposed.

The toy experiment demonstrates that all discussed quantities can fail to reliably predict generalization under model misspecification and prior-data conflict even for a basic setting using Bayesian linear regression, emphasizing the need for caution when making claims about the superiority of any particular metric.

Ultimately, the key takeaway is that there is no one-size-fits-all solution, and the choice of model selection criterion should be guided by a careful consideration of the specific context and goals at hand.

---

**Acknowledgements:** We would like to thank the authors of the paper for their valuable contributions to the field and for inspiring this blog post. We also appreciate the insightful discussions and feedback from the community that have helped shape this post.

---

## Appendix

### Detailed Code Review of the DNN Experiments in Lotfi et al. (2022/2023)   
The [`logcml_` files in the repository](https://github.com/Sanaelotfi/Bayesian_model_comparison/tree/main/Laplace_experiments/cifar) contain the code to compute the CLML for partially trained models. However, instead of computing

$$
\begin{aligned}
\log p(\mathcal D_{\ge m} \mid \mathcal D_{< m}, \mathcal{M} ) \approx \log \sum_{k=1}^K \frac{1}{K}\, p(\mathcal{D}_{\ge m} \mid w_k, \mathcal M ) \\
= \log \sum_{k=1}^K \frac{1}{K}\, \prod_{j=m}^n p(y_j \mid x_j, w_k, \mathcal M ),
\end{aligned}
$$

the code computes:

$$
\begin{aligned}
&\frac{1}{|\mathcal{D}_{\ge m}|}\,\sum_{j=m}^n \log p(\mathcal D_{j} \mid \mathcal D_{< m}, \mathcal{M} ) \approx \\
&\quad =\frac{1}{|\mathcal{D}_{\ge m}|}\,\sum_{j=m}^n \log \sum_{k=1}^K \frac{1}{K}\, p(y_j \mid x_j, w_k, \mathcal M ),
\end{aligned}
$$

which is the validation cross-entropy loss of the BMA (of the model trained with 80% of the training data).

The high-level [code](https://github.com/Sanaelotfi/Bayesian_model_comparison/tree/c6f0da1d49374c0dda6ee743e5b02bcf3e158e96/Laplace_experiments/cifar/logcml_cifar10_resnets.py#L295) that computes the CLML is:

{% highlight python linenos %}
bma_accuracy, bma_probs, all_ys = get_bma_acc(
    net, la, trainloader_test, bma_nsamples, 
    hessian_structure, temp=best_temp
)
cmll = get_cmll(bma_probs, all_ys, eps=1e-4)
{% endhighlight %}

[`get_bma_acc`](https://github.com/Sanaelotfi/Bayesian_model_comparison/tree/c6f0da1d49374c0dda6ee743e5b02bcf3e158e96/Laplace_experiments/cifar/logcml_cifar10_resnets.py#L149) marginalizes over the LA samples before returning `bma_probs`: 

{% highlight python linenos %}
[...]
for sample_params in params:
    sample_probs = []
    all_ys = []
    with torch.no_grad():
        vector_to_parameters(sample_params, net.parameters())
        net.eval()
        for x, y in loader:
            logits = net(x.cuda()).detach().cpu()
            probs = torch.nn.functional.softmax(logits, dim=-1)
            sample_probs.append(probs.detach().cpu().numpy())
            all_ys.append(y.detach().cpu().numpy())
        sample_probs = np.concatenate(sample_probs, axis=0)
        all_ys = np.concatenate(all_ys, axis=0)
        all_probs.append(sample_probs)

all_probs = np.stack(all_probs)
bma_probs = np.mean(all_probs, 0)
bma_accuracy = (np.argmax(bma_probs, axis=-1) == all_ys).mean() * 100

return bma_accuracy, bma_probs, all_ys
{% endhighlight %}

The important line is #18: `bma_probs = np.mean(all_probs, 0)` which marginalizes over the predictions and returns the BMA prediction for each sample.

Finally, [`get_cmll`](https://github.com/Sanaelotfi/Bayesian_model_comparison/tree/c6f0da1d49374c0dda6ee743e5b02bcf3e158e96/Laplace_experiments/cifar/logcml_cifar10_resnets.py#L170) computes the validation loss for each sample independently (after applying a bit of label smoothing):
{% highlight python linenos %}
def get_cmll(bma_probs, all_ys, eps=1e-4):
    log_lik = 0      
    eps = 1e-4
    for i, label in enumerate(all_ys):
        probs_i = bma_probs[i]
        probs_i += eps
        probs_i[np.argmax(probs_i)] -= eps * len(probs_i)
        log_lik += np.log(probs_i[label]).item()
    cmll = log_lik/len(all_ys)
    
    return cmll
{% endhighlight %}

The DNN experiments in Section 5 and Section 6 of the first arXiv revision of the paper (*v1*) thus did not estimate the CLML per-se but computed the BMA validation loss of a partially trained model (80%) and find that this correlates positively with the test accuracy and test log-likelihood of the fully trained model (at 100%). This is not surprising because it is well-known that the validation loss of a model trained 80% of the data correlates positively with the test accuracy (and generalization loss).

### Author Response from 2022

The following response sadly seems to target the first draft mainly. However, it is also helpful for the final blog post and provides additional context.

<blockquote markdown="1">
Thanks for your interest in our paper and your comments. Here are our comments about the blog as it is currently framed:

(1) Thank you for pointing out a bug in the CLML computation for Figure 5b. We note that this bug is only relevant to a single panel of a single figure in the main text. We have re-run this experiment with the right CLML, and the results, attached here, are qualitatively the same. In summary, it was a very minor part of the paper, and even for that part it did not affect the take-away. We also attach the results of the correlation between the BMA test accuracy and the negative validation loss. You suggest in your post that the validation loss might correlate better with the BMA test accuracy than the CLML given that we use 20 samples for NAS. Our empirical results show the opposite conclusion. Additionally, we are not suggesting the CLML as a replacement to cross-validation but rather as a minor way to modify the LML for improvements in predicting generalization. Finally, we attach results for different sample sizes (20 samples vs. 100 samples) to address your comments on the sample size used to estimate the CLML. As we can see in the figure, the Spearman correlation factor is quite similar. 20 samples appears to provide a reasonable estimate of the CLML for these purposes, and is different from validation loss.

{% capture max-width %}
" style="max-width: 20em;
{% endcapture %}
{% include figure.html path="assets/img/2024-05-07-clml/rebuttal_1.png" max-width=max-width %}
{% include figure.html path="assets/img/2024-05-07-clml/rebuttal_2.png" max-width=max-width %}
{% include figure.html path="assets/img/2024-05-07-clml/rebuttal_3.png" max-width=max-width %}

(2) Your post currently opens by suggesting that there is something wrong with our experiments, likely either an LML approximation or a CLML issue, because we note that the LML correlates more poorly with generalization for larger datasets (where “large” is relative in the context of a specific experiment). A few points here: (i) this result is actually completely expected. The LML is in fact non-monotonic in how well it predicts generalization. For small datasets, the prior should be reasonably predictive of generalization. For intermediate datasets, the first terms in the LML decomposition have a negative effect on the correlation with generalization. For asymptotically large datasets, the first terms have a diminishing effect, and we get a consistent estimator; (ii) almost all of our experiments are exact, and we see this behaviour in the exact experiments for the Fourier model. For example, for the Fourier feature experiment in Fig 4(d), LML picks the better generalizing model for n < 50 and n > 296. For n in [50, 296] it picks the wrong model. For large neural network models, it is reasonable that the exact LML could pick the wrong model for CIFAR-sized datasets. (iii) any potential issues with the CLML are not relevant to these considerations, which are about the behaviour of the LML.

(3) Your post currently suggests that issues with approximate inference could be responsible for our take-aways, rather than issues with the LML in general. But as we note in (2), almost all of our experiments use the exact LML and CLML: the density model, Fourier features, Gaussian processes, and deep learning exps on DKL, and there was never any bug associated with CLML computation in these experiments. The takeaways for the Laplace experiments are consistent with the exact experiments, and also expected, as above. While it’s true that the CLML can be estimated more effectively than the LML for the Laplace experiments, this is actually an advantage of the CLML that we note in the paper. The LML results also stand on their own, as we discuss above.

(4) Your post places a lot of importance on Figure 5, as if it is the main result of the paper and our main “DNN” experiments. We stand by the results of Figure 5, but it is a relatively minor component of the paper. As we’ve mentioned most of our results are exact, including our DKL experiments, which are certainly the most substantial DNN experiments, with practically exciting results for transfer and few-shot learning. The DKL experiments are actually where we expect the CLML to be practically useful, and currently they seem to be overlooked in the post.

(5) The blog seems to question the learning curve experiments, but these experiments in Figure 4 are exact, with no Laplace approximation, and relatively straightforward.

(6) Your post seems to be negative about the CLML, presenting its similarity with cross-validation as a potential drawback, and implying the skepticism about the CLML should affect the interpretation of our take-aways. Two points here: (i) as above, the CLML is independent of most of our take-aways, which are about the properties of the LML; (ii) our goal with the CLML was not to introduce something starkly different from cross-validation, but to show how a very minor modification to the LML could improve alignment with generalization. Moreover, the DKL CLML results are quite promising as an efficient way to do gradient based estimation of a large number of hyperparameters.

(7) The blog opens as if it is leading up to some fatal flaw. But as above, (i) the LML considerations are independent of the CLML, (ii) most of the experiments are exact, (iii) the trends for the exact and approximate inference procedures are the same and are naturally understandable and explainable, such as the non-monotonic trend in how well the LML correlates with generalization, and (iv) the CLML bug only affected Figure 5, panel b, and when it’s corrected the qualitative take-away is the same as before.

We appreciate your interest and effort in reading the paper, and we think your questions will improve the clarity of the paper, which we have updated with an acknowledgement to you. Given the above considerations, we do think there would need to be substantial revisions to the blog post to accurately and fairly reflect the paper. We would appreciate being able to see the revisions before it’s posted.

Best wishes,\\
Sanae, Pavel, Greg, Micah, Andrew
</blockquote>

###  Ablation: CLML vs. BMA Validation Loss vs. (non-BMA) Validation Loss

Let us examine the new results: 

In the three panels below, two panels show test accuracy vs. validation loss; one shows test accuracy vs. CLML. The left-most panel is the BMA test accuracy vs. (negative) BMA validation loss, the middle panel is vs. the CLML, and the right-most panel is vs. the (negative) non-BMA validation loss. 

Note that the left-most panel is from *v1*, which was accidentally computing the BMA validation loss, and whose axis label is adapted here from *v1* for clarity. The two other plots are from *v2* after fixing the bug. See commits [here](https://github.com/Sanaelotfi/Bayesian_model_comparison/commit/a579aa292723dc20a6105ec8f4fff1045dd9a9fd) for fixing the CLML estimation and [here](https://github.com/Sanaelotfi/Bayesian_model_comparison/commit/3fa8ca2ecb314ee881f6c95a602ef58b9ccd3620) for computing the non-BMA validation loss. 

{% capture width %}
" style="width: 20em;
{% endcapture %}
<div class="row mt-3">
  <div class="col-sm mt-3 mt-md-0">
    {% include figure.html path="assets/img/2024-05-07-clml/bmsmlg_bma_validation_loss.svg" class="img-fluid" caption="BMA Neg Validation Loss" width=width %}
  </div>
  <div class="col-sm mt-3 mt-md-0">
    {% include figure.html path="assets/img/2024-05-07-clml/bmsmlg_clml.svg" class="img-fluid" caption="CLML" width=width %}
  </div>
</div>
<div class="row mt-3">
  <div class="col-sm mt-3 mt-md-0">
    {% include figure.html path="assets/img/2024-05-07-clml/bmsmlg_validation_loss.svg" class="img-fluid" caption="Validation Loss" width=width %}
  </div>  
{% capture width %}
" style="width: 5em;
{% endcapture %}
  <div class="col-sm mt-3 mt-md-0">
    {% include figure.html path="assets/img/2024-05-07-clml/bmsmlg_plot_legend.svg" class="img-fluid" caption="Leg" width=width %}
  </div>
</div>

At first glance, there might be an observer effect in the experiments for the validation loss. The BMA validation loss in *v1* performs better than the CLML in *v2*, while the non-BMA validation loss in *v2* underperforms the CLML in *v2*.
When asked about it, the authors pushed the respective code (see link above) and explained that the updated, right-most panel computes the **non-BMA** validation loss, i.e., without LA samples. 
It seems surprising that there is such a difference between the non-BMA validation loss and BMA validation loss: *the non-BMA validation loss is more than one nat worse on average than the BMA validation loss, based on visual inspection*. Note that the plots here and in the paper compute the average CLML and average validation loss and are thus directly comparable.

The authors said in their response that:

> You suggest in your post that the validation loss might correlate better with the BMA test accuracy than the CLML given that we use 20 samples for NAS. Our empirical results show the opposite conclusion.

This is only partially true. 
The BMA validation loss (which was accidentally computed in *v1* instead of the CLML) correlates very well with the BMA test accuracy.
This is not surprising given that this is the frequentist purpose of using validation sets. If validation sets were not correlating well with the test accuracy, we would not be using them in practice. 🤗 As such, this raises the question why the non-BMA validation loss correlates negatively with the BMA test accuracy for ResNets and overall in the *v2* results.
Thus, only the non-BMA validation loss supports the now opposite conclusion in *v2* of the paper and in the authors' response. 

Yet what is also surprising is how well the BMA validation loss does vs. the CLML:

<aside class="box-error l-body" markdown="1">
🔥 The BMA validation loss correlates even better than the CLML with BMA test accuracy: the Spearman's rank correlation is better for the BMA validation loss than for the CLML across all $$\lambda$$s.

Does this mean we should use the BMA validation loss rather than estimating the CLML for DNNs?
</aside>

### Ablation: LA Sample Size

Secondly, when we compare the reported values between BMA validation loss and CLML, we notice that the CLML is lower than the BMA validation loss by half a nat for $$\lambda=10^2$$ and generally for CNNs.

<aside class="box-note l-body" markdown="1">
👉 One would expect that the (average) CLML to be better (higher) than the negative BMA validation loss and not worse (lower) because the CLML ought to be able to compress the validation set better using the joint predictive distribution than the marginal predictive distribution for the validation loss.  


Intuitively, the CLML can take the validation data into account in a way that the validation loss cannot given the sequential conditioning of the former. 

Hence, the CLML should generally be lower-bounded by the (BMA) validation loss and upper-bounded by the (BMA) test loss up to sample variance, assuming there are not many outliers in the data.
The fact that the CLML is worse points toward the LA samples not being able to capture the implicit posterior in the joint predictive distribution. (This is a bit handwavy but is a reasonable interpretation, as we will see below when taking into account additional evidence.)
</aside>

However, it seems, even though the new experiments in *v2* are supposed to reproduce the ones from *v1*, and we can assume that the same model checkpoints were used for re-evaluation (as retraining is not necessary), both CLML and non-BMA validation loss are off by about half a nat for the CNNs. As such, the above consideration might hold but might not provide the answer here.

Instead, we overlay the non-BMA validation loss and the CLML plots, both from *v2*, with a "difference blend": it shows the absolute difference between the colors for overlapping data points (the circles 🔴 and triangles 🔺), leading to black where there is a match, negative (green-ish) color for CLML, and positive (sepia) color for validation losses. The background grids were used to match the plots, but we hid the ones from CLML afterward---as such, the strong overlay is because the values are so close.

{% capture width %}
" style="width: 25em;
{% endcapture %}
{% include figure.html path="assets/img/2024-05-07-clml/bmsmlg_difference_overlay_plot.svg" class="img-fluid" width=width %}


Surprisingly---or rather as predicted when the LA does not really do much---it turns out that the validation loss for the CNNs (🔴) mostly fully matches the estimated CLML with 20 LA samples following a visual inspection. To be more precise, either the models have already sufficiently converged, or the CLML estimate is not actually capturing the correlations between points and thus ends up being very similar to the validation loss.

<aside class="box-warning l-body" markdown="1">
As mentioned before, it could be insightful to actually compute the difference between CLML and BMA validation loss explicitly. Two things are not clear to me given the results in *v1* and *v2*:

1. Why is there a difference in nats between BMA validation loss in *v1* and the non-BMA validation loss and CLML in *v2*? Given that CLML matches the non-BMA validation loss for CNNs in *v2*, the BMA validation loss ought to be close to that, too.

2. Why is there different behavior for ResNets (🔺) in the BMA validation loss in *v1* and the CLML in *v2* compared to the non-BMA validation loss in *v2*? The low correlation coefficients of the non-BMA validation loss seem to come from that, as *v2* also explains in an added paragraph in appendix H.

</aside>

{% capture width %}
" style="width: 25em;
{% endcapture %}
{% include figure.html path="assets/img/2024-05-07-clml/rebuttal_3.png" class="img-fluid" width=width %}

This changes the interpretation of the sample ablation in the author's response. The ablation shows no difference between 20 and 100 LA samples, with 100 LA even samples having a slightly lower rank correlation. So it seems 5 times more LA samples are not sufficient to make a difference, or the Laplace posterior cannot capture the posterior as well as hoped. It would be interesting to examine this further. Kirsch et al (2022)<d-cite key="kirsch2022marginal"></d-cite> reported running toy experiments on MNIST with 10,000 MC Dropout samples without achieving good adaptation. Laplace approximation is not MC Dropout, and this is speculation, but it seems in agreement. Notwithstanding the compute cost and feasibility, could posterior samples using HMC or similar more principled methods provide better estimates? 

All in all, given the above, it is fair to say that the estimate of the CLML is probably not as good as hoped, and further experiments might be needed to tease out when the CLML provides more value than the (BMA) validation loss. Note, however, that this question has not been explicitly examined in the paper. Instead, for DNNs, the paper only compares LML and CLML with distinct estimation methods.